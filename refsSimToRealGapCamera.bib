
@inproceedings{li_closed-form_2018,
	address = {Munich, Germany},
	title = {A {Closed}-form {Solution} to {Photorealistic} {Image} {Stylization}},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Li, Yijun and Liu, Ming-Yu and Li, Xueting and Yang, Ming-Hsuan and Kautz, Jan},
	year = {2018},
	keywords = {Photorealistic stylization},
	pages = {453--468},
	file = {Li et al. - 2018 - A Closed-form Solution to Photorealistic Image Sty.pdf:C\:\\Users\\wb619\\Zotero\\storage\\2HRQCTB4\\Li et al. - 2018 - A Closed-form Solution to Photorealistic Image Sty.pdf:application/pdf},
}

@inproceedings{yoo_photorealistic_2019,
	address = {Seoul, South Korea},
	title = {Photorealistic {Style} {Transfer} via {Wavelet} {Transforms}},
	doi = {10.1109/ICCV.2019.00913},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yoo, Jaejun and Uh, Youngjung and Chun, Sanghyuk and Kang, Byeongkyu and Ha, Jung-Woo},
	year = {2019},
	keywords = {Photorealistic stylization},
	pages = {9035--9044},
	file = {Yoo et al. - 2019 - Photorealistic Style Transfer via Wavelet Transfor.pdf:C\:\\Users\\wb619\\Zotero\\storage\\TCTMPZ6N\\Yoo et al. - 2019 - Photorealistic Style Transfer via Wavelet Transfor.pdf:application/pdf},
}

@article{dundar_domain_2018,
	title = {Domain {Stylization}: {A} {Strong}, {Simple} {Baseline} for {Synthetic} to {Real} {Image} {Domain} {Adaptation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1807.09384},
	doi = {10.48550/ARXIV.1807.09384},
	journal = {arXiv},
	author = {Dundar, Aysegul and Liu, Ming-Yu and Wang, Ting-Chun and Zedlewski, John and Kautz, Jan},
	year = {2018},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {Dundar et al. - 2018 - Domain Stylization A Strong, Simple Baseline for .pdf:C\:\\Users\\wb619\\Zotero\\storage\\XKRT2DLE\\Dundar et al. - 2018 - Domain Stylization A Strong, Simple Baseline for .pdf:application/pdf},
}

@inproceedings{binkowski_demystifying_2018,
	address = {Vancouver, Canada},
	title = {Demystifying {MMD} {GANs}},
	url = {https://openreview.net/forum?id=r1lUOzWCW},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Bińkowski, Mikołaj and Sutherland, Dougal J. and Arbel, Michael and Gretton, Arthur},
	year = {2018},
	file = {Bińkowski et al. - 2018 - Demystifying MMD GANs.pdf:C\:\\Users\\wb619\\Zotero\\storage\\J8XPIK39\\Bińkowski et al. - 2018 - Demystifying MMD GANs.pdf:application/pdf},
}

@inproceedings{heusel_gans_2017,
	address = {Long Beach, USA},
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	file = {Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:C\:\\Users\\wb619\\Zotero\\storage\\48ZZT2LQ\\Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf},
}

@inproceedings{salimans_improved_2016,
	address = {Barcelona, Spain},
	title = {Improved {Techniques} for {Training} {GANs}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi and Chen, Xi},
	editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
	year = {2016},
	file = {Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:C\:\\Users\\wb619\\Zotero\\storage\\YXWNUKUG\\Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf},
}

@article{richter_enhancing_2023,
	title = {Enhancing {Photorealism} {Enhancement}},
	volume = {45},
	doi = {10.1109/TPAMI.2022.3166687},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Richter, Stephan R. and Alhaija, Hassan Abu and Koltun, Vladlen},
	year = {2023},
	pages = {1700--1715},
	file = {Richter et al. - 2023 - Enhancing Photorealism Enhancement.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CPSQXIE8\\Richter et al. - 2023 - Enhancing Photorealism Enhancement.pdf:application/pdf},
}

@inproceedings{kar_meta-sim_2019,
	address = {Seoul, South Korea},
	title = {Meta-{Sim}: {Learning} to {Generate} {Synthetic} {Datasets}},
	doi = {10.1109/ICCV.2019.00465},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Kar, Amlan and Prakash, Aayush and Liu, Ming-Yu and Cameracci, Eric and Yuan, Justin and Rusiniak, Matt and Acuna, David and Torralba, Antonio and Fidler, Sanja},
	year = {2019},
	pages = {4550--4559},
	file = {Kar et al. - 2019 - Meta-Sim Learning to Generate Synthetic Datasets.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6Q5RM3DI\\Kar et al. - 2019 - Meta-Sim Learning to Generate Synthetic Datasets.pdf:application/pdf},
}

@inproceedings{feng_bayesian_2022,
	address = {Kyoto, Japan},
	title = {Bayesian {Active} {Learning} for {Sim}-to-{Real} {Robotic} {Perception}},
	doi = {10.1109/IROS47612.2022.9982175},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Feng, Jianxiang and Lee, Jongseok and Durner, Maximilian and Triebel, Rudolph},
	year = {2022},
	pages = {10820--10827},
	file = {Feng et al. - 2022 - Bayesian Active Learning for Sim-to-Real Robotic P.pdf:C\:\\Users\\wb619\\Zotero\\storage\\UFX68TZ8\\Feng et al. - 2022 - Bayesian Active Learning for Sim-to-Real Robotic P.pdf:application/pdf},
}

@inproceedings{zhu_unpaired_2017,
	address = {Venice, Italy},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	doi = {10.1109/ICCV.2017.244},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	year = {2017},
	pages = {2242--2251},
	file = {Zhu et al. - 2017 - Unpaired Image-to-Image Translation Using Cycle-Co.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8AL5HDRL\\Zhu et al. - 2017 - Unpaired Image-to-Image Translation Using Cycle-Co.pdf:application/pdf},
}

@inproceedings{rao_rl-cyclegan_2020,
	address = {Seattle, USA},
	title = {{RL}-{CycleGAN}: {Reinforcement} {Learning} {Aware} {Simulation}-to-{Real}},
	doi = {10.1109/CVPR42600.2020.01117},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Rao, Kanishka and Harris, Chris and Irpan, Alex and Levine, Sergey and Ibarz, Julian and Khansari, Mohi},
	year = {2020},
	keywords = {Improve - GAN},
	file = {Rao et al. - 2020 - RL-CycleGAN Reinforcement Learning Aware Simulati.pdf:C\:\\Users\\wb619\\Zotero\\storage\\7NVDP2WT\\Rao et al. - 2020 - RL-CycleGAN Reinforcement Learning Aware Simulati.pdf:application/pdf},
}

@article{sung_realistic_2020,
	title = {Realistic sonar image simulation using deep learning for underwater object detection},
	volume = {18},
	doi = {10.1007/s12555-019-0691-3},
	number = {3},
	journal = {International Journal of Control, Automation and Systems},
	author = {Sung, Minsung and Kim, Jason and Lee, Meungsuk and Kim, Byeongjin and Kim, Taesik and Kim, Juhwan and Yu, Son-Cheol},
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Improve - GAN},
	pages = {523--534},
	file = {Sung et al. - 2020 - Realistic sonar image simulation using deep learni.pdf:C\:\\Users\\wb619\\Zotero\\storage\\58PYB9D5\\Sung et al. - 2020 - Realistic sonar image simulation using deep learni.pdf:application/pdf},
}

@inproceedings{jeong_self-supervised_2020,
	address = {Paris, France},
	title = {Self-{Supervised} {Sim}-to-{Real} {Adaptation} for {Visual} {Robotic} {Manipulation}},
	doi = {10.1109/ICRA40945.2020.9197326},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Jeong, Rae and Aytar, Yusuf and Khosid, David and Zhou, Yuxiang and Kay, Jackie and Lampe, Thomas and Bousmalis, Konstantinos and Nori, Francesco},
	year = {2020},
	keywords = {Algorithm’s domain adaptation},
	pages = {2718--2724},
	file = {Jeong et al. - 2020 - Self-Supervised Sim-to-Real Adaptation for Visual .pdf:C\:\\Users\\wb619\\Zotero\\storage\\CB7WZEPZ\\Jeong et al. - 2020 - Self-Supervised Sim-to-Real Adaptation for Visual .pdf:application/pdf},
}

@article{dong_shipgan_2023,
	title = {{ShipGAN}: {Generative} {Adversarial} {Network} based simulation-to-real image translation for ships},
	volume = {131},
	issn = {0141-1187},
	url = {https://www.sciencedirect.com/science/article/pii/S0141118722003856},
	doi = {https://doi.org/10.1016/j.apor.2022.103456},
	abstract = {Recent advances in robotics and autonomous systems (RAS) have significantly improved the autonomy level of unmanned surface vehicles (USVs) and made them capable of undertaking demanding tasks in various environments. During the operation of USVs, apart from normal situations, it is those unexpected scenes, such as busy waterways or navigation in dust/nighttime, impose most dangers to USVs as these scenes are rarely seen during training. Such a rare occurrence also makes the manual collection and recording of these scenes into dataset difficult, expensive and inefficient, with the majority of existing public available datasets not able to fully cover them. One of many plausible solutions is to purposely generate these data using computer vision techniques with the assistance from high-fidelity simulations that can create various desirable motions/scenarios. However, the stylistic difference between the simulation images and the natural images would cause a domain shift problem. Hence, there is a need for designing a method that can transfer the data distribution and styles of the simulation images into the realistic domain. This paper proposes and evaluates a novel solution to fill this gap using a Generative Adversarial Network (GAN) based model, ShipGAN, to translate the simulation images into realistic images. Experiments were carried out to investigate the feasibility of generating realistic images using GAN-based image translation models. The synthetic realistic images from the simulation images were demonstrated to be reliable by the object detection and image segmentation algorithms trained with natural images.},
	journal = {Applied Ocean Research},
	author = {Dong, Yuxuan and Wu, Peng and Wang, Sen and Liu, Yuanchang},
	year = {2023},
	keywords = {Autonomous navigation, Generative networks, Sim-to-real translation, Unmanned surface vessels, Improve - GAN},
	pages = {103456},
	file = {Dong et al. - 2023 - ShipGAN Generative Adversarial Network based simu.pdf:C\:\\Users\\wb619\\Zotero\\storage\\K9VYJY62\\Dong et al. - 2023 - ShipGAN Generative Adversarial Network based simu.pdf:application/pdf},
}

@inproceedings{zhang_unreasonable_2018,
	address = {Salt Lake City, UT, USA},
	title = {The {Unreasonable} {Effectiveness} of {Deep} {Features} as a {Perceptual} {Metric}},
	doi = {10.1109/CVPR.2018.00068},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	year = {2018},
	pages = {586--595},
	file = {Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as.pdf:C\:\\Users\\wb619\\Zotero\\storage\\SQZN25TG\\Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as.pdf:application/pdf},
}

@article{wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	doi = {10.1109/TIP.2003.819861},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	year = {2004},
	pages = {600--612},
	file = {Wang et al. - 2004 - Image quality assessment from error visibility to.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5K6ERK82\\Wang et al. - 2004 - Image quality assessment from error visibility to.pdf:application/pdf},
}

@inproceedings{ani_quantifying_2021,
	address = {Milan, Italy},
	title = {Quantifying the {Use} of {Domain} {Randomization}},
	doi = {10.1109/ICPR48806.2021.9412118},
	booktitle = {International {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Ani, Mohammad and Basevi, Hector and Leonardis, Aleš},
	year = {2021},
	pages = {6128--6135},
	file = {Ani et al. - 2021 - Quantifying the Use of Domain Randomization.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6HEP79SF\\Ani et al. - 2021 - Quantifying the Use of Domain Randomization.pdf:application/pdf},
}

@inproceedings{hu_sim--real_2022,
	address = {Aachen, Germany},
	title = {Sim-to-{Real} {Domain} {Adaptation} for {Lane} {Detection} and {Classification} in {Autonomous} {Driving}},
	doi = {10.1109/IV51971.2022.9827450},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Hu, Chuqing and Hudson, Sinclair and Ethier, Martin and Al-Sharman, Mohammad and Rayside, Derek and Melek, William},
	year = {2022},
	keywords = {Algorithm’s domain adaptation},
	pages = {457--463},
	file = {Hu et al. - 2022 - Sim-to-Real Domain Adaptation for Lane Detection a.pdf:C\:\\Users\\wb619\\Zotero\\storage\\28JBXVMT\\Hu et al. - 2022 - Sim-to-Real Domain Adaptation for Lane Detection a.pdf:application/pdf},
}

@inproceedings{guizilini_geometric_2021,
	address = {Montreal, QC, Canada},
	title = {Geometric {Unsupervised} {Domain} {Adaptation} for {Semantic} {Segmentation}},
	doi = {10.1109/ICCV48922.2021.00842},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Guizilini, Vitor and Li, Jie and Ambruş, Rareş and Gaidon, Adrien},
	year = {2021},
	keywords = {Algorithm’s domain adaptation},
	pages = {8517--8527},
	file = {Guizilini et al. - 2021 - Geometric Unsupervised Domain Adaptation for Seman.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MG9DNUTF\\Guizilini et al. - 2021 - Geometric Unsupervised Domain Adaptation for Seman.pdf:application/pdf},
}

@inproceedings{shyam_infra_2022,
	address = {Aachen, Germany},
	title = {Infra {Sim}-to-{Real}: {An} efficient baseline and dataset for {Infrastructure} based {Online} {Object} {Detection} and {Tracking} using {Domain} {Adaptation}},
	doi = {10.1109/IV51971.2022.9827395},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	publisher = {IEEE},
	author = {Shyam, Pranjay and Mishra, Sumit and Yoon, Kuk-Jin and Kim, Kyung-Soo},
	year = {2022},
	keywords = {Algorithm’s domain adaptation},
	pages = {1393--1399},
	file = {Shyam et al. - 2022 - Infra Sim-to-Real An efficient baseline and datas.pdf:C\:\\Users\\wb619\\Zotero\\storage\\E6IDNN76\\Shyam et al. - 2022 - Infra Sim-to-Real An efficient baseline and datas.pdf:application/pdf},
}

@inproceedings{james_sim--real_2019,
	title = {Sim-{To}-{Real} via {Sim}-{To}-{Sim}: {Data}-{Efficient} {Robotic} {Grasping} via {Randomized}-{To}-{Canonical} {Adaptation} {Networks}},
	doi = {10.1109/CVPR.2019.01291},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
	year = {2019},
	pages = {12619--12629},
	file = {James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.pdf:C\:\\Users\\wb619\\Zotero\\storage\\3MWRRLKP\\James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.pdf:application/pdf},
}

@inproceedings{prakash_structured_2019,
	address = {Montreal, QC, Canada},
	title = {Structured {Domain} {Randomization}: {Bridging} the {Reality} {Gap} by {Context}-{Aware} {Synthetic} {Data}},
	doi = {10.1109/ICRA.2019.8794443},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Prakash, Aayush and Boochoon, Shaad and Brophy, Mark and Acuna, David and Cameracci, Eric and State, Gavriel and Shapira, Omer and Birchfield, Stan},
	year = {2019},
	keywords = {Domain randomization},
	pages = {7249--7255},
	file = {Prakash et al. - 2019 - Structured Domain Randomization Bridging the Real.pdf:C\:\\Users\\wb619\\Zotero\\storage\\FIVEEZLU\\Prakash et al. - 2019 - Structured Domain Randomization Bridging the Real.pdf:application/pdf},
}

@inproceedings{movshovitz-attias_how_2016,
	address = {Cham},
	title = {How {Useful} {Is} {Photo}-{Realistic} {Rendering} for {Visual} {Learning}?},
	isbn = {978-3-319-49409-8},
	doi = {10.1007/978-3-319-49409-8_18},
	abstract = {Data seems cheap to get, and in many ways it is, but the process of creating a high quality labeled dataset from a mass of data is time-consuming and expensive.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV}) {Workshops}},
	publisher = {Springer International Publishing},
	author = {Movshovitz-Attias, Yair and Kanade, Takeo and Sheikh, Yaser},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	pages = {202--217},
	file = {Movshovitz-Attias et al. - 2016 - How Useful Is Photo-Realistic Rendering for Visual.pdf:C\:\\Users\\wb619\\Zotero\\storage\\XLQKRPNL\\Movshovitz-Attias et al. - 2016 - How Useful Is Photo-Realistic Rendering for Visual.pdf:application/pdf},
}

@inproceedings{carlson_modeling_2018,
	title = {Modeling {Camera} {Effects} to {Improve} {Visual} {Learning} from {Synthetic} {Data}},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV}) {Workshops}},
	author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
	month = sep,
	year = {2018},
	file = {Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:C\:\\Users\\wb619\\Zotero\\storage\\ZCA74XHE\\Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:application/pdf},
}

@article{carlson_sensor_2019,
	title = {Sensor {Transfer}: {Learning} {Optimal} {Sensor} {Effect} {Image} {Augmentation} for {Sim}-to-{Real} {Domain} {Adaptation}},
	volume = {4},
	doi = {10.1109/LRA.2019.2896470},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
	year = {2019},
	pages = {2431--2438},
	file = {Carlson et al. - 2019 - Sensor Transfer Learning Optimal Sensor Effect Im.pdf:C\:\\Users\\wb619\\Zotero\\storage\\SI5QAAKF\\Carlson et al. - 2019 - Sensor Transfer Learning Optimal Sensor Effect Im.pdf:application/pdf},
}

@article{schellenberg_photoacoustic_2022,
	title = {Photoacoustic image synthesis with generative adversarial networks},
	volume = {28},
	issn = {2213-5979},
	url = {https://www.sciencedirect.com/science/article/pii/S2213597922000672},
	doi = {10.1016/j.pacs.2022.100402},
	abstract = {Photoacoustic tomography (PAT) has the potential to recover morphological and functional tissue properties with high spatial resolution. However, previous attempts to solve the optical inverse problem with supervised machine learning were hampered by the absence of labeled reference data. While this bottleneck has been tackled by simulating training data, the domain gap between real and simulated images remains an unsolved challenge. We propose a novel approach to PAT image synthesis that involves subdividing the challenge of generating plausible simulations into two disjoint problems: (1) Probabilistic generation of realistic tissue morphology, and (2) pixel-wise assignment of corresponding optical and acoustic properties. The former is achieved with Generative Adversarial Networks (GANs) trained on semantically annotated medical imaging data. According to a validation study on a downstream task our approach yields more realistic synthetic images than the traditional model-based approach and could therefore become a fundamental step for deep learning-based quantitative PAT (qPAT).},
	journal = {Photoacoustics},
	author = {Schellenberg, Melanie and Gröhl, Janek and Dreher, Kris K. and Nölke, Jan-Hinrich and Holzwarth, Niklas and Tizabi, Minu D. and Seitel, Alexander and Maier-Hein, Lena},
	year = {2022},
	keywords = {Deep learning, Generative adversarial networks, Optoacoustic imaging, Optoacoustic tomography, Photoacoustic imaging, Photoacoustic tomography, Synthetic data},
	pages = {100402},
	file = {Schellenberg et al. - 2022 - Photoacoustic image synthesis with generative adve.pdf:C\:\\Users\\wb619\\Zotero\\storage\\AQQQNF9N\\Schellenberg et al. - 2022 - Photoacoustic image synthesis with generative adve.pdf:application/pdf},
}

@inproceedings{alghonaim_benchmarking_2021,
	title = {Benchmarking {Domain} {Randomisation} for {Visual} {Sim}-to-{Real} {Transfer}},
	doi = {10.1109/ICRA48506.2021.9561134},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Alghonaim, Raghad and Johns, Edward},
	year = {2021},
	pages = {12802--12808},
	file = {Alghonaim and Johns - 2021 - Benchmarking Domain Randomisation for Visual Sim-t.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CC2VFCXA\\Alghonaim and Johns - 2021 - Benchmarking Domain Randomisation for Visual Sim-t.pdf:application/pdf},
}

@inproceedings{zakharov_photo-realistic_2022,
	address = {Cham},
	title = {Photo-realistic {Neural} {Domain} {Randomization}},
	isbn = {978-3-031-19806-9},
	abstract = {Synthetic data is a scalable alternative to manual supervision, but it requires overcoming the sim-to-real domain gap. This discrepancy between virtual and real worlds is addressed by two seemingly opposed approaches: improving the realism of simulation or foregoing realism entirely via domain randomization. In this paper, we show that the recent progress in neural rendering enables a new unified approach we call Photo-realistic Neural Domain Randomization (PNDR). We propose to learn a composition of neural networks that acts as a physics-based ray tracer generating high-quality renderings from scene geometry alone. Our approach is modular, composed of different neural networks for materials, lighting, and rendering, thus enabling randomization of different key image generation components in a differentiable pipeline. Once trained, our method can be combined with other methods and used to generate photo-realistic image augmentations online and significantly more efficiently than via traditional ray-tracing. We demonstrate the usefulness of PNDR through two downstream tasks: 6D object detection and monocular depth estimation. Our experiments show that training with PNDR enables generalization to novel scenes and significantly outperforms the state of the art in terms of real-world transfer.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer Nature Switzerland},
	author = {Zakharov, Sergey and Ambruș, Rareș and Guizilini, Vitor and Kehl, Wadim and Gaidon, Adrien},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {310--327},
	file = {Zakharov et al. - 2022 - Photo-realistic Neural Domain Randomization.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8F2SYL4R\\Zakharov et al. - 2022 - Photo-realistic Neural Domain Randomization.pdf:application/pdf},
}

@inproceedings{bousmalis_using_2018,
	title = {Using {Simulation} and {Domain} {Adaptation} to {Improve} {Efficiency} of {Deep} {Robotic} {Grasping}},
	doi = {10.1109/ICRA.2018.8460875},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
	year = {2018},
	keywords = {Domain adaptation},
	pages = {4243--4250},
	file = {Bousmalis et al. - 2018 - Using Simulation and Domain Adaptation to Improve .pdf:C\:\\Users\\wb619\\Zotero\\storage\\J7H55PWN\\Bousmalis et al. - 2018 - Using Simulation and Domain Adaptation to Improve .pdf:application/pdf},
}

@inproceedings{bousmalis_unsupervised_2017,
	title = {Unsupervised pixel-level domain adaptation with generative adversarial networks},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
	month = jul,
	year = {2017},
	keywords = {Improve - GAN},
	file = {Bousmalis et al. - 2017 - Unsupervised pixel-level domain adaptation with ge.pdf:C\:\\Users\\wb619\\Zotero\\storage\\RKPH8SAB\\Bousmalis et al. - 2017 - Unsupervised pixel-level domain adaptation with ge.pdf:application/pdf},
}

@article{andrychowicz_learning_2020,
	title = {Learning dexterous in-hand manipulation},
	volume = {39},
	url = {https://doi.org/10.1177/0278364919887447},
	doi = {10.1177/0278364919887447},
	abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.},
	number = {1},
	journal = {International Journal of Robotics Research},
	author = {Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Józefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
	year = {2020},
	note = {\_eprint: https://doi.org/10.1177/0278364919887447},
	keywords = {Domain randomization},
	pages = {3--20},
	file = {Andrychowicz et al. - 2020 - Learning dexterous in-hand manipulation.pdf:C\:\\Users\\wb619\\Zotero\\storage\\DZBYQU63\\Andrychowicz et al. - 2020 - Learning dexterous in-hand manipulation.pdf:application/pdf},
}

@inproceedings{tobin_domain_2017,
	title = {Domain randomization for transferring deep neural networks from simulation to the real world},
	doi = {10.1109/IROS.2017.8202133},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	year = {2017},
	keywords = {Domain randomization},
	pages = {23--30},
	file = {Tobin et al. - 2017 - Domain randomization for transferring deep neural .pdf:C\:\\Users\\wb619\\Zotero\\storage\\8W3CFVJP\\Tobin et al. - 2017 - Domain randomization for transferring deep neural .pdf:application/pdf},
}

@article{zhang_close_2023,
	title = {Close the {Optical} {Sensing} {Domain} {Gap} by {Physics}-{Grounded} {Active} {Stereo} {Sensor} {Simulation}},
	doi = {10.1109/TRO.2023.3235591},
	journal = {IEEE Transactions on Robotics},
	author = {Zhang, Xiaoshuai and Chen, Rui and Li, Ang and Xiang, Fanbo and Qin, Yuzhe and Gu, Jiayuan and Ling, Zhan and Liu, Minghua and Zeng, Peiyu and Han, Songfang and Huang, Zhiao and Mu, Tongzhou and Xu, Jing and Su, Hao},
	year = {2023},
	keywords = {FOS: Computer and information sciences, Robotics (cs.RO)},
	pages = {1--19},
	file = {Zhang et al. - 2023 - Close the Optical Sensing Domain Gap by Physics-Gr.pdf:C\:\\Users\\wb619\\Zotero\\storage\\DXU7BDXT\\Zhang et al. - 2023 - Close the Optical Sensing Domain Gap by Physics-Gr.pdf:application/pdf},
}

@article{liu_neural_2020,
	title = {Neural {Network} {Generalization}: {The} {Impact} of {Camera} {Parameters}},
	volume = {8},
	doi = {10.1109/ACCESS.2020.2965089},
	journal = {IEEE Access},
	author = {Liu, Zhenyi and Lian, Trisha and Farrell, Joyce and Wandell, Brian A.},
	year = {2020},
	pages = {10443--10454},
	file = {Liu et al. - 2020 - Neural Network Generalization The Impact of Camer.pdf:C\:\\Users\\wb619\\Zotero\\storage\\IWRMTYV6\\Liu et al. - 2020 - Neural Network Generalization The Impact of Camer.pdf:application/pdf},
}

@article{goossens_ray-transfer_2022,
	title = {Ray-transfer functions for camera simulation of {3D} scenes with hidden lens design},
	volume = {30},
	url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-30-13-24031},
	doi = {10.1364/OE.457496},
	abstract = {Combining image sensor simulation tools with physically based ray tracing enables the design and evaluation (soft prototyping) of novel imaging systems. These methods can also synthesize physically accurate, labeled images for machine learning applications. One practical limitation of soft prototyping has been simulating the optics precisely: lens manufacturers generally prefer to keep lens design confidential. We present a pragmatic solution to this problem using a black box lens model in Zemax; such models provide necessary optical information while preserving the lens designer's intellectual property. First, we describe and provide software to construct a polynomial ray transfer function that characterizes how rays entering the lens at any position and angle subsequently exit the lens. We implement the ray-transfer calculation as a camera model in PBRT and confirm that the PBRT ray-transfer calculations match the Zemax lens calculations for edge spread functions and relative illumination.},
	number = {13},
	journal = {Optics Express},
	author = {Goossens, Thomas and Lyu, Zheng and Ko, Jamyuen and Wan, Gordon C. and Farrell, Joyce and Wandell, Brian},
	year = {2022},
	note = {Publisher: Optica Publishing Group},
	keywords = {Chromatic aberrations, Image sensors, Imaging systems, Lens design, Position sensing equipment, Ray tracing, Color metric},
	pages = {24031--24047},
	file = {Goossens et al. - 2022 - Ray-transfer functions for camera simulation of 3D.pdf:C\:\\Users\\wb619\\Zotero\\storage\\VFJBSIJL\\Goossens et al. - 2022 - Ray-transfer functions for camera simulation of 3D.pdf:application/pdf},
}

@inproceedings{stein_genesis-rt_2018,
	address = {Brisbane, Australia},
	title = {{GeneSIS}-{RT}: {Generating} {Synthetic} {Images} for {Training} {Secondary} {Real}-{World} {Tasks}},
	doi = {10.1109/ICRA.2018.8462971},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Stein, Gregory J. and Roy, Nicholas},
	year = {2018},
	keywords = {Improve - GAN},
	pages = {7151--7158},
	file = {Stein and Roy - 2018 - GeneSIS-RT Generating Synthetic Images for Traini.pdf:C\:\\Users\\wb619\\Zotero\\storage\\VFX2ZUED\\Stein and Roy - 2018 - GeneSIS-RT Generating Synthetic Images for Traini.pdf:application/pdf},
}

@inproceedings{huang_multimodal_2018,
	address = {Munich, Germany},
	title = {Multimodal {Unsupervised} {Image}-to-image {Translation}},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
	month = sep,
	year = {2018},
	file = {Huang et al. - 2018 - Multimodal Unsupervised Image-to-image Translation.pdf:C\:\\Users\\wb619\\Zotero\\storage\\R3DXBHBB\\Huang et al. - 2018 - Multimodal Unsupervised Image-to-image Translation.pdf:application/pdf},
}

@inproceedings{hoffman_cycada_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{CyCADA}: {Cycle}-{Consistent} {Adversarial} {Domain} {Adaptation}},
	volume = {80},
	abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	keywords = {Improve - GAN},
	pages = {1989--1998},
	file = {Hoffman et al. - 2018 - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf:C\:\\Users\\wb619\\Zotero\\storage\\BDFP2M9M\\Hoffman et al. - 2018 - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf:application/pdf},
}

@inproceedings{liu_unsupervised_2017,
	address = {Long Beach, CA, USA},
	title = {Unsupervised {Image}-to-{Image} {Translation} {Networks}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	month = dec,
	year = {2017},
	file = {Liu et al. - 2017 - Unsupervised Image-to-Image Translation Networks.pdf:C\:\\Users\\wb619\\Zotero\\storage\\2LPCRCTM\\Liu et al. - 2017 - Unsupervised Image-to-Image Translation Networks.pdf:application/pdf},
}

@article{hapke_bidirectional_2008,
	title = {Bidirectional reflectance spectroscopy: 6. {Effects} of porosity},
	volume = {195},
	issn = {0019-1035},
	url = {https://www.sciencedirect.com/science/article/pii/S0019103508000419},
	doi = {10.1016/j.icarus.2008.01.003},
	abstract = {It is well known that the bidirectional reflectance of a particulate medium such as a planetary regolith depends on the porosity, in contrast to predictions of models based on the equation of radiative transfer as usually formulated. It is shown that this failure to predict porosity dependence arises from an incorrect treatment of the light that passes between the particles. In this paper a more physically correct treatment that takes account of the necessity of preventing particles from interpenetrating is used together with the two-stream approximation to solve the radiative transfer equation and derive improved expressions for the bidirectional and directional-hemispherical reflectances. It is found that increasing the filling factor (decreasing the porosity) increases the reflectance of low and medium albedo powders, but decreases it for ones with very high albedos. The model agrees qualitatively with measured data.},
	number = {2},
	journal = {Icarus},
	author = {Hapke, Bruce},
	year = {2008},
	keywords = {Radiative transfer, Regoliths, Spectrophotometry},
	pages = {918--926},
	file = {Hapke - 2008 - Bidirectional reflectance spectroscopy 6. Effects.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MQ488AW6\\Hapke - 2008 - Bidirectional reflectance spectroscopy 6. Effects.pdf:application/pdf},
}

@inproceedings{burley_physically-based_2012,
	title = {Physically-based shading at {Disney}},
	booktitle = {{SIGGRAPH}},
	author = {Burley, Brent and Studios, Walt Disney Animation},
	year = {2012},
	file = {Burley and Studios - 2012 - Physically-based shading at Disney.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5GRPST8G\\Burley and Studios - 2012 - Physically-based shading at Disney.pdf:application/pdf},
}

@inproceedings{munkberg_extracting_2022,
	address = {New Orleans, USA},
	title = {Extracting {Triangular} {3D} {Models}, {Materials}, and {Lighting} from {Images}},
	doi = {10.1109/CVPR52688.2022.00810},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Munkberg, Jacob and Chen, Wenzheng and Hasselgren, Jon and Evans, Alex and Shen, Tianchang and Müller, Thomas and Gao, Jun and Fidler, Sanja},
	year = {2022},
	keywords = {Neural renderer},
	pages = {8270--8280},
	file = {Munkberg et al. - 2022 - Extracting Triangular 3D Models, Materials, and Li.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CL396VUP\\Munkberg et al. - 2022 - Extracting Triangular 3D Models, Materials, and Li.pdf:application/pdf;NVDiffRec_supple.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LN4NRVUN\\NVDiffRec_supple.pdf:application/pdf},
}

@inproceedings{zhang_physg_2021,
	address = {Nashville, USA},
	title = {{PhySG}: {Inverse} {Rendering} with {Spherical} {Gaussians} for {Physics}-based {Material} {Editing} and {Relighting}},
	doi = {10.1109/CVPR46437.2021.00541},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Kai and Luan, Fujun and Wang, Qianqian and Bala, Kavita and Snavely, Noah},
	year = {2021},
	keywords = {Neural renderer},
	pages = {5449--5458},
	file = {Zhang et al. - 2021 - PhySG Inverse Rendering with Spherical Gaussians .pdf:C\:\\Users\\wb619\\Zotero\\storage\\KBMN7GL7\\Zhang et al. - 2021 - PhySG Inverse Rendering with Spherical Gaussians .pdf:application/pdf},
}

@inproceedings{mildenhall_nerf_2020,
	address = {Online},
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	isbn = {978-3-030-58451-1},
	url = {https://doi.org/10.1007/978-3-030-58452-8_24},
	doi = {10.1007/978-3-030-58452-8_24},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,\&nbsp;y,\&nbsp;z) and viewing direction ) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer-Verlag},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	year = {2020},
	note = {event-place: Glasgow, United Kingdom},
	keywords = {3D deep learning, Image-based rendering, Scene representation, View synthesis, Volume rendering, Neural renderer},
	pages = {405--421},
	file = {Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:C\:\\Users\\wb619\\Zotero\\storage\\TIFYGBGD\\Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf},
}

@article{muller_instant_2022,
	title = {Instant {Neural} {Graphics} {Primitives} with a {Multiresolution} {Hash} {Encoding}},
	volume = {41},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {encodings, function approximation, GPUs, hashing, image synthesis, neural networks, parallel computation},
	pages = {1--15},
	file = {InstantNGP.pdf:C\:\\Users\\wb619\\Zotero\\storage\\BE9969KK\\InstantNGP.pdf:application/pdf;Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:C\:\\Users\\wb619\\Zotero\\storage\\2I4EWIC7\\Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:application/pdf},
}

@inproceedings{boss_nerd_2021,
	address = {Montreal, Canada},
	title = {{NeRD}: {Neural} {Reflectance} {Decomposition} from {Image} {Collections}},
	doi = {10.1109/ICCV48922.2021.01245},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Boss, Mark and Braun, Raphael and Jampani, Varun and Barron, Jonathan T. and Liu, Ce and Lensch, Hendrik P.A.},
	year = {2021},
	keywords = {Neural renderer},
	pages = {12664--12674},
	file = {Boss et al. - 2021 - NeRD Neural Reflectance Decomposition from Image .pdf:C\:\\Users\\wb619\\Zotero\\storage\\X7L2WYWT\\Boss et al. - 2021 - NeRD Neural Reflectance Decomposition from Image .pdf:application/pdf},
}

@article{zhang_nerfactor_2021,
	title = {{NeRFactor}: {Neural} {Factorization} of {Shape} and {Reflectance} under an {Unknown} {Illumination}},
	volume = {40},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3478513.3480496},
	doi = {10.1145/3478513.3480496},
	abstract = {We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.},
	number = {6},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Xiuming and Srinivasan, Pratul P. and Deng, Boyang and Debevec, Paul and Freeman, William T. and Barron, Jonathan T.},
	month = dec,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {appearance factorization, inverse rendering, lighting estimation, material editing, reflectance estimation, relighting, shape estimation, view synthesis, Neural renderer},
	pages = {1--18},
	file = {Zhang et al. - 2021 - NeRFactor Neural Factorization of Shape and Refle.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8HVLWKRR\\Zhang et al. - 2021 - NeRFactor Neural Factorization of Shape and Refle.pdf:application/pdf},
}

@inproceedings{carlson_modeling_2018-1,
	title = {Modeling {Camera} {Effects} to {Improve} {Visual} {Learning} from {Synthetic} {Data}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1803.07721},
	doi = {10.48550/ARXIV.1803.07721},
	publisher = {arXiv},
	author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
	year = {2018},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5V3SRTLT\\Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:application/pdf},
}

@inproceedings{barron_mip-nerf_2021,
	address = {Montreal, Canada},
	title = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
	doi = {10.1109/ICCV48922.2021.00580},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	year = {2021},
	pages = {5835--5844},
	file = {Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf:C\:\\Users\\wb619\\Zotero\\storage\\QKWX5RDP\\Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf:application/pdf},
}

@inproceedings{wang_neus_2021,
	title = {{NeuS}: {Learning} {Neural} {Implicit} {Surfaces} by {Volume} {Rendering} for {Multi}-view {Reconstruction}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/e41e164f7485ec4a28741a2d0ea41c74-Paper.pdf},
	booktitle = {Neural {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {27171--27183},
	file = {Wang et al. - 2021 - NeuS Learning Neural Implicit Surfaces by Volume .pdf:C\:\\Users\\wb619\\Zotero\\storage\\JIGT56RR\\Wang et al. - 2021 - NeuS Learning Neural Implicit Surfaces by Volume .pdf:application/pdf},
}

@article{denninger_blenderproc_2019,
	title = {{BlenderProc}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1911.01911},
	doi = {10.48550/arXiv.1911.01911},
	journal = {arXiv},
	author = {Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},
	year = {2019},
	doi = {10.48550/ARXIV.1911.01911},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Robotics (cs.RO), Graphics (cs.GR)},
	file = {Denninger et al. - 2019 - BlenderProc.pdf:C\:\\Users\\wb619\\Zotero\\storage\\GJGFEM9V\\Denninger et al. - 2019 - BlenderProc.pdf:application/pdf},
}

@inproceedings{hodan_photorealistic_2019,
	address = {Taipei, Taiwan},
	title = {Photorealistic {Image} {Synthesis} for {Object} {Instance} {Detection}},
	doi = {10.1109/ICIP.2019.8803821},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Hodaň, Tomáš and Vineet, Vibhav and Gal, Ran and Shalev, Emanuel and Hanzelka, Jon and Connell, Treb and Urbina, Pedro and Sinha, Sudipta N. and Guenter, Brian},
	month = sep,
	year = {2019},
	pages = {66--70},
	file = {Hodaň et al. - 2019 - Photorealistic Image Synthesis for Object Instance.pdf:C\:\\Users\\wb619\\Zotero\\storage\\C3JQF52Z\\Hodaň et al. - 2019 - Photorealistic Image Synthesis for Object Instance.pdf:application/pdf},
}

@inproceedings{chen_dib-r_2021,
	title = {{DIB}-{R}++: {Learning} to {Predict} {Lighting} and {Material} with a {Hybrid} {Differentiable} {Renderer}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/c0f971d8cd24364f2029fcb9ac7b71f5-Paper.pdf},
	booktitle = {Neural {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Wenzheng and Litalien, Joey and Gao, Jun and Wang, Zian and Fuji Tsang, Clement and Khamis, Sameh and Litany, Or and Fidler, Sanja},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {22834--22848},
}

@article{li_learning_2018,
	title = {Learning to {Reconstruct} {Shape} and {Spatially}-{Varying} {Reflectance} from a {Single} {Image}},
	volume = {37},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3272127.3275055},
	doi = {10.1145/3272127.3275055},
	abstract = {Reconstructing shape and reflectance properties from images is a highly under-constrained problem, and has previously been addressed by using specialized hardware to capture calibrated data or by assuming known (or highly constrained) shape or reflectance. In contrast, we demonstrate that we can recover non-Lambertian, spatially-varying BRDFs and complex geometry belonging to any arbitrary shape class, from a single RGB image captured under a combination of unknown environment illumination and flash lighting. We achieve this by training a deep neural network to regress shape and reflectance from the image. Our network is able to address this problem because of three novel contributions: first, we build a large-scale dataset of procedurally generated shapes and real-world complex SVBRDFs that approximate real world appearance well. Second, single image inverse rendering requires reasoning at multiple scales, and we propose a cascade network structure that allows this in a tractable manner. Finally, we incorporate an in-network rendering layer that aids the reconstruction task by handling global illumination effects that are important for real-world scenes. Together, these contributions allow us to tackle the entire inverse rendering problem in a holistic manner and produce state-of-the-art results on both synthetic and real data.},
	number = {6},
	journal = {ACM Transactions on Graphics},
	author = {Li, Zhengqin and Xu, Zexiang and Ramamoorthi, Ravi and Sunkavalli, Kalyan and Chandraker, Manmohan},
	month = dec,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {cascade network, deep learning, flash light, global illumination, rendering layer, single image, SVBRDF},
	pages = {1--11},
	file = {Li et al. - 2018 - Learning to Reconstruct Shape and Spatially-Varyin.pdf:C\:\\Users\\wb619\\Zotero\\storage\\ZFPRB9A2\\Li et al. - 2018 - Learning to Reconstruct Shape and Spatially-Varyin.pdf:application/pdf},
}

@article{guarnera_brdf_2016,
	title = {{BRDF} {Representation} and {Acquisition}},
	volume = {35},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12867},
	doi = {10.1111/cgf.12867},
	abstract = {Abstract Photorealistic rendering of real world environments is important in a range of different areas; including Visual Special effects, Interior/Exterior Modelling, Architectural Modelling, Cultural Heritage, Computer Games and Automotive Design. Currently, rendering systems are able to produce photorealistic simulations of the appearance of many real-world materials. In the real world, viewer perception of objects depends on the lighting and object/material/surface characteristics, the way a surface interacts with the light and on how the light is reflected, scattered, absorbed by the surface and the impact these characteristics have on material appearance. In order to re-produce this, it is necessary to understand how materials interact with light. Thus the representation and acquisition of material models has become such an active research area. This survey of the state-of-the-art of BRDF Representation and Acquisition presents an overview of BRDF (Bidirectional Reflectance Distribution Function) models used to represent surface/material reflection characteristics, and describes current acquisition methods for the capture and rendering of photorealistic materials.},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Guarnera, D. and Guarnera, G.C. and Ghosh, A. and Denk, C. and Glencross, M.},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12867},
	keywords = {and texture, Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation, I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Colour, I.6.8 Computer Graphics: Types of simulation—Monte Carlo, shading, shadowing},
	pages = {625--650},
	file = {Guarnera et al. - 2016 - BRDF Representation and Acquisition.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4QSWRQBX\\Guarnera et al. - 2016 - BRDF Representation and Acquisition.pdf:application/pdf},
}

@article{schlick_inexpensive_1994,
	title = {An {Inexpensive} {BRDF} {Model} for {Physically}-based {Rendering}},
	volume = {13},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.1330233},
	doi = {10.1111/1467-8659.1330233},
	abstract = {Abstract: A new BRDF model is presented which can be viewed as an kind of intermediary model between empirism and theory. Main results of physics are observed (energy conservation, reciprocity rule, microfacet theory) and numerous phenomena involved in light reflection are accounted for, in a physically plausible way (incoherent and coherent reflection, spectrum modifications, anisotropy, self-shadowing, multiple surface and subsurface reflection, differences between homogeneous and heterogeneous materials). The model has been especially intended for computer graphics applications and therefore includes two main features: simplicity (a small number of intuitively understandable parameters controls the model) and efficiency (the formulation provides adequation to Monte-Carlo rendering techniques and/or hardware implementations).},
	number = {3},
	journal = {Computer Graphics Forum},
	author = {Schlick, Christophe},
	year = {1994},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-8659.1330233},
	keywords = {Bidirectional Reflectance Distribution Function, Optimization, Physically-Based Rendering},
	pages = {233--246},
	file = {Schlick - 1994 - An Inexpensive BRDF Model for Physically-based Ren.pdf:C\:\\Users\\wb619\\Zotero\\storage\\W5655TK5\\Schlick - 1994 - An Inexpensive BRDF Model for Physically-based Ren.pdf:application/pdf},
}

@article{lee_real-time_2010,
	title = {Real-{Time} {Lens} {Blur} {Effects} and {Focus} {Control}},
	volume = {29},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1778765.1778802},
	doi = {10.1145/1778765.1778802},
	abstract = {We present a novel rendering system for defocus blur and lens effects. It supports physically-based rendering and outperforms previous approaches by involving a novel GPU-based tracing method. Our solution achieves more precision than competing real-time solutions and our results are mostly indistinguishable from offline rendering. Our method is also more general and can integrate advanced simulations, such as simple geometric lens models enabling various lens aberration effects. These latter is crucial for realism, but are often employed in artistic contexts, too. We show that available artistic lenses can be simulated by our method. In this spirit, our work introduces an intuitive control over depth-of-field effects. The physical basis is crucial as a starting point to enable new artistic renderings based on a generalized focal surface to emphasize particular elements in the scene while retaining a realistic look. Our real-time solution provides realistic, as well as plausible expressive results.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Lee, Sungkil and Eisemann, Elmar and Seidel, Hans-Peter},
	year = {2010},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	file = {Lee et al. - 2010 - Real-Time Lens Blur Effects and Focus Control.pdf:C\:\\Users\\wb619\\Zotero\\storage\\3Z6PMGWL\\Lee et al. - 2010 - Real-Time Lens Blur Effects and Focus Control.pdf:application/pdf},
}

@inproceedings{kang_automatic_2007,
	address = {Minneapolis, USA},
	title = {Automatic {Removal} of {Chromatic} {Aberration} from a {Single} {Image}},
	doi = {10.1109/CVPR.2007.383214},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kang, Sing Bing},
	year = {2007},
	pages = {1--8},
	file = {Kang - 2007 - Automatic Removal of Chromatic Aberration from a S.pdf:C\:\\Users\\wb619\\Zotero\\storage\\YD7GIMZT\\Kang - 2007 - Automatic Removal of Chromatic Aberration from a S.pdf:application/pdf},
}

@inproceedings{karaimer_software_2016,
	address = {Amsterdam, The Netherlands},
	title = {A {Software} {Platform} for {Manipulating} the {Camera} {Imaging} {Pipeline}},
	isbn = {978-3-319-46448-0},
	doi = {10.1007/978-3-319-46448-0_26},
	abstract = {There are a number of processing steps applied onboard a digital camera that collectively make up the camera imaging pipeline. Unfortunately, the imaging pipeline is typically embedded in a camera's hardware making it difficult for researchers working on individual components to do so within the proper context of the full pipeline. This not only hinders research, it makes evaluating the effects from modifying an individual pipeline component on the final camera output challenging, if not impossible. This paper presents a new software platform that allows easy access to each stage of the camera imaging pipeline. The platform allows modification of the parameters for individual components as well as the ability to access and manipulate the intermediate images as they pass through different stages. We detail our platform design and demonstrate its usefulness on a number of examples.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Karaimer, Hakki Can and Brown, Michael S.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {429--444},
	file = {Full Text:C\:\\Users\\wb619\\Zotero\\storage\\FQ5LLF7E\\Karaimer and Brown - 2016 - A Software Platform for Manipulating the Camera Im.pdf:application/pdf},
}

@article{bhukhanwala_automated_1994,
	title = {Automated global enhancement of digitized photographs},
	volume = {40},
	doi = {10.1109/30.273657},
	number = {1},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Bhukhanwala, Saumil A. and Ramabadran, Tenkasi V.},
	year = {1994},
	pages = {1--10},
	file = {Bhukhanwala and Ramabadran - 1994 - Automated global enhancement of digitized photogra.pdf:C\:\\Users\\wb619\\Zotero\\storage\\RFZFA2MG\\Bhukhanwala and Ramabadran - 1994 - Automated global enhancement of digitized photogra.pdf:application/pdf},
}

@inproceedings{messina_image_2003,
	address = {Baltimore, USA},
	title = {Image quality improvement by adaptive exposure correction techniques},
	volume = {1},
	doi = {10.1109/ICME.2003.1220976},
	booktitle = {{IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Messina, Giuseppe and Castorina, Alfio and Battiato, Sebastiano and Bosco, Angelo},
	year = {2003},
	pages = {549--552},
	file = {Messina et al. - 2003 - Image quality improvement by adaptive exposure cor.pdf:C\:\\Users\\wb619\\Zotero\\storage\\9KGLBUVB\\Messina et al. - 2003 - Image quality improvement by adaptive exposure cor.pdf:application/pdf},
}

@article{foi_practical_2008,
	title = {Practical {Poissonian}-{Gaussian} {Noise} {Modeling} and {Fitting} for {Single}-{Image} {Raw}-{Data}},
	volume = {17},
	doi = {10.1109/TIP.2008.2001399},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {Foi, Alessandro and Trimeche, Mejdi and Katkovnik, Vladimir and Egiazarian, Karen},
	year = {2008},
	pages = {1737--1754},
	file = {Foi et al. - 2008 - Practical Poissonian-Gaussian Noise Modeling and F.pdf:C\:\\Users\\wb619\\Zotero\\storage\\GRM3SYA3\\Foi et al. - 2008 - Practical Poissonian-Gaussian Noise Modeling and F.pdf:application/pdf},
}

@book{pharr_physically_2016,
	edition = {3rd},
	title = {Physically based rendering: {From} theory to implementation},
	publisher = {Morgan Kaufmann},
	author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
	year = {2016},
}

@article{santana-cedres_estimation_2017,
	title = {Estimation of the {Lens} {Distortion} {Model} by {Minimizing} a {Line} {Reprojection} {Error}},
	volume = {17},
	doi = {10.1109/JSEN.2017.2677475},
	number = {9},
	journal = {IEEE Sensors Journal},
	author = {Santana-Cedrés, Daniel and Gomez, Luis and Alemán-Flores, Miguel and Salgado, Agustín and Esclarín, Julio and Mazorra, Luis and Alvarez, Luis},
	year = {2017},
	pages = {2848--2855},
	file = {Santana-Cedrés et al. - 2017 - Estimation of the Lens Distortion Model by Minimiz.pdf:C\:\\Users\\wb619\\Zotero\\storage\\HW64YVFV\\Santana-Cedrés et al. - 2017 - Estimation of the Lens Distortion Model by Minimiz.pdf:application/pdf},
}

@inproceedings{chen_no_2017,
	address = {Venice, Italy},
	title = {No {More} {Discrimination}: {Cross} {City} {Adaptation} of {Road} {Scene} {Segmenters}},
	doi = {10.1109/ICCV.2017.220},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Yi-Hsin and Chen, Wei-Yu and Chen, Yu-Ting and Tsai, Bo-Cheng and Wang, Yu-Chiang Frank and Sun, Min},
	year = {2017},
	pages = {2011--2020},
	file = {Chen et al. - 2017 - No More Discrimination Cross City Adaptation of R.pdf:C\:\\Users\\wb619\\Zotero\\storage\\FEK9KQ7G\\Chen et al. - 2017 - No More Discrimination Cross City Adaptation of R.pdf:application/pdf},
}

@article{tang_precision_2017,
	title = {A {Precision} {Analysis} of {Camera} {Distortion} {Models}},
	volume = {26},
	doi = {10.1109/TIP.2017.2686001},
	number = {6},
	journal = {IEEE Transactions on Image Processing},
	author = {Tang, Zhongwei and Grompone von Gioi, Rafael and Monasse, Pascal and Morel, Jean-Michel},
	year = {2017},
	pages = {2694--2704},
	file = {Tang et al. - 2017 - A Precision Analysis of Camera Distortion Models.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MR4NYTQM\\Tang et al. - 2017 - A Precision Analysis of Camera Distortion Models.pdf:application/pdf},
}

@article{yu_practical_2004,
	title = {Practical anti-vignetting methods for digital cameras},
	volume = {50},
	doi = {10.1109/TCE.2004.1362487},
	number = {4},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Yu, Wonpil},
	year = {2004},
	pages = {975--983},
	file = {Yu - 2004 - Practical anti-vignetting methods for digital came.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LP4IIZHS\\Yu - 2004 - Practical anti-vignetting methods for digital came.pdf:application/pdf},
}

@article{zheng_single-image_2009,
	title = {Single-{Image} {Vignetting} {Correction}},
	volume = {31},
	doi = {10.1109/TPAMI.2008.263},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zheng, Yuanjie and Lin, Stephen and Kambhamettu, Chandra and Yu, Jingyi and Kang, Sing Bing},
	year = {2009},
	pages = {2243--2256},
	file = {Zheng et al. - 2009 - Single-Image Vignetting Correction.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MELBC9LI\\Zheng et al. - 2009 - Single-Image Vignetting Correction.pdf:application/pdf},
}

@inproceedings{hullin_physically-based_2011,
	address = {New York, USA},
	series = {{SIGGRAPH} '11},
	title = {Physically-{Based} {Real}-{Time} {Lens} {Flare} {Rendering}},
	isbn = {978-1-4503-0943-1},
	url = {https://doi-org.ezproxy.library.wisc.edu/10.1145/1964921.1965003},
	doi = {10.1145/1964921.1965003},
	abstract = {Lens flare is caused by light passing through a photographic lens system in an unintended way. Often considered a degrading artifact, it has become a crucial component for realistic imagery and an artistic means that can even lead to an increased perceived brightness. So far, only costly offline processes allowed for convincing simulations of the complex light interactions. In this paper, we present a novel method to interactively compute physically-plausible flare renderings for photographic lenses. The underlying model covers many components that are important for realism, such as imperfections, chromatic and geometric lens aberrations, and antireflective lens coatings. Various acceleration strategies allow for a performance/quality tradeoff, making our technique applicable both in real-time applications and in high-quality production rendering. We further outline artistic extensions to our system.},
	booktitle = {{ACM} {SIGGRAPH}},
	publisher = {Association for Computing Machinery},
	author = {Hullin, Matthias and Eisemann, Elmar and Seidel, Hans-Peter and Lee, Sungkil},
	year = {2011},
	note = {event-place: Vancouver, British Columbia, Canada},
	keywords = {lens flare, real-time rendering},
	file = {Hullin et al. - 2011 - Physically-Based Real-Time Lens Flare Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5N5MR8WQ\\Hullin et al. - 2011 - Physically-Based Real-Time Lens Flare Rendering.pdf:application/pdf},
}

@article{lee_practical_2013,
	title = {Practical {Real}-{Time} {Lens}-{Flare} {Rendering}},
	volume = {32},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12145},
	doi = {https://doi.org/10.1111/cgf.12145},
	abstract = {Abstract We present a practical real-time approach for rendering lens-flare effects. While previous work employed costly ray tracing or complex polynomial expressions, we present a coarser, but also significantly faster solution. Our method is based on a first-order approximation of the ray transfer in an optical system, which allows us to derive a matrix that maps lens flare-producing light rays directly to the sensor. The resulting approach is easy to implement and produces physically-plausible images at high framerates on standard off-the-shelf graphics hardware.},
	number = {4},
	journal = {Computer Graphics Forum},
	author = {Lee, Sungkil and Eisemann, Elmar},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12145},
	keywords = {I.3.3 Computer Graphics: Picture/Image Generation—Display algorithms},
	pages = {1--6},
	file = {Lee and Eisemann - 2013 - Practical Real-Time Lens-Flare Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CXSFQLWV\\Lee and Eisemann - 2013 - Practical Real-Time Lens-Flare Rendering.pdf:application/pdf},
}

@inproceedings{ouyang_neural_2021,
	address = {Nashville, USA},
	title = {Neural {Camera} {Simulators}},
	doi = {10.1109/CVPR46437.2021.00761},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ouyang, Hao and Shi, Zifan and Lei, Chenyang and Lung Law, Ka and Chen, Qifeng},
	year = {2021},
	pages = {7696--7705},
	file = {Ouyang et al. - 2021 - Neural Camera Simulators.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CUGNPBBB\\Ouyang et al. - 2021 - Neural Camera Simulators.pdf:application/pdf},
}

@inproceedings{li_igibson_2021,
	address = {London, UK},
	series = {Proceedings of {Machine} {Learning} {Research} ({PMLR})},
	title = {{iGibson} 2.0: {Object}-{Centric} {Simulation} for {Robot} {Learning} of {Everyday} {Household} {Tasks}},
	volume = {164},
	url = {https://proceedings.mlr.press/v164/li22b.html},
	abstract = {Recent research in embodied AI has been boosted by the use of simulation environments to develop and train robot learning approaches. However, the use of simulation has skewed the attention to tasks that only require what robotics simulators can simulate: motion and physical contact. We present iGibson 2.0, an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. First, iGibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. Second, iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked. Additionally, given a logic state, iGibson 2.0 can sample valid physical states that satisfy it. This functionality can generate potentially infinite instances of tasks with minimal effort from the users. The sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations. As a result, we can collect demonstrations from humans on these new types of tasks, and use them for imitation learning. We evaluate the new capabilities of iGibson 2.0 to enable robot learning of novel tasks, in the hope of demonstrating the potential of this new simulator to support new research in embodied AI. iGibson 2.0 and its new dataset are publicly available at http://svl.stanford.edu/igibson/.},
	booktitle = {Conference on {Robot} {Learning} ({CoRL})},
	publisher = {PMLR},
	author = {Li, Chengshu and Xia, Fei and Martín-Martín, Roberto and Lingelbach, Michael and Srivastava, Sanjana and Shen, Bokui and Vainio, Kent Elliott and Gokmen, Cem and Dharan, Gokul and Jain, Tanish and Kurenkov, Andrey and Liu, Karen and Gweon, Hyowon and Wu, Jiajun and Fei-Fei, Li and Savarese, Silvio},
	editor = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
	month = nov,
	year = {2021},
	pages = {455--465},
	file = {Li et al. - 2022 - iGibson 2.0 Object-Centric Simulation for Robot L.pdf:C\:\\Users\\wb619\\Zotero\\storage\\IXWE7KYM\\Li et al. - 2022 - iGibson 2.0 Object-Centric Simulation for Robot L.pdf:application/pdf},
}

@inproceedings{blasinski_optimizing_2018,
	address = {San Francisco, USA},
	title = {Optimizing {Image} {Acquisition} {Systems} for {Autonomous} {Driving}},
	doi = {10.2352/ISSN.2470-1173.2018.05.PMII-161},
	booktitle = {{IS}\&{T} {International} {Symposium} on {Electronic} {Imaging}: {Photography}, {Mobile}, and {Immersive} {Imaging}},
	author = {Blasinski, Henryk and Farrell, Joyce and Lian, Trisha and Liu, Zhenyi and Wandell, Brian},
	year = {2018},
	pages = {161--1 -- 161--7},
	file = {Henryk Blasinski et al. - 2018 - Optimizing Image Acquisition Systems for Autonomou.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5RKUGV6W\\Henryk Blasinski et al. - 2018 - Optimizing Image Acquisition Systems for Autonomou.pdf:application/pdf},
}

@article{wang_validating_2005,
	title = {Validating {USARsim} for use in {HRI} {Research}},
	volume = {49},
	url = {https://doi.org/10.1177/154193120504900351},
	doi = {10.1177/154193120504900351},
	abstract = {HRI is an excellent candidate for simulator based research because of the relative simplicity of the systems being modeled, the behavioral fidelity possible with current physics engines and the capability of modern graphics cards to approximate camera video. In this paper we briefly introduce the USARsim simulation and discuss efforts to validate its behavior for use in Human Robot Interaction (HRI) research.},
	number = {3},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Wang, Jijun and Lewis, Michael and Hughes, Stephen and Koes, Mary and Carpin, Stefano},
	year = {2005},
	note = {\_eprint: https://doi.org/10.1177/154193120504900351},
	pages = {457--461},
	file = {Wang et al. - 2005 - Validating USARsim for use in HRI Research.pdf:C\:\\Users\\wb619\\Zotero\\storage\\NYIJBJG9\\Wang et al. - 2005 - Validating USARsim for use in HRI Research.pdf:application/pdf},
}

@inproceedings{sewtz_ursim_2022,
	address = {Big Sky, Montana, USA},
	title = {{URSim} - {A} {Versatile} {Robot} {Simulator} for {Extra}-{Terrestrial} {Exploration}},
	doi = {10.1109/AERO53065.2022.9843576},
	booktitle = {{IEEE} {Aerospace} {Conference} ({AERO})},
	author = {Sewtz, Marco and Lehner, Hannah and Fanger, Yunis and Eberle, Jan and Wudenka, Martin and Müller, Marcus G. and Bodenmüller, Tim and Schuster, Martin J.},
	year = {2022},
	pages = {1--14},
	file = {Sewtz et al. - 2022 - URSim - A Versatile Robot Simulator for Extra-Terr.pdf:C\:\\Users\\wb619\\Zotero\\storage\\SXLMNIKL\\Sewtz et al. - 2022 - URSim - A Versatile Robot Simulator for Extra-Terr.pdf:application/pdf},
}

@article{castilla-arquillo_hardware-accelerated_2022,
	title = {Hardware-{Accelerated} {Mars} {Sample} {Localization} {Via} {Deep} {Transfer} {Learning} {From} {Photorealistic} {Simulations}},
	volume = {7},
	doi = {10.1109/LRA.2022.3219306},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Castilla-Arquillo, Raul and Pérez-del-Pulgar, Carlos and Paz-Delgado, Gonzalo Jesus and Gerdes, Levin},
	year = {2022},
	pages = {12555--12561},
	file = {Castilla-Arquillo et al. - 2022 - Hardware-Accelerated Mars Sample Localization Via .pdf:C\:\\Users\\wb619\\Zotero\\storage\\RLUHZGLH\\Castilla-Arquillo et al. - 2022 - Hardware-Accelerated Mars Sample Localization Via .pdf:application/pdf},
}

@inproceedings{allan_planetary_2019,
	address = {Big Sky, Montana, USA},
	title = {Planetary {Rover} {Simulation} for {Lunar} {Exploration} {Missions}},
	doi = {10.1109/AERO.2019.8741780},
	booktitle = {{IEEE} {Aerospace} {Conference} ({AERO})},
	author = {Allan, Mark and Wong, Uland and Furlong, P. Michael and Rogg, Arno and McMichael, Scott and Welsh, Terry and Chen, Ian and Peters, Steven and Gerkey, Brian and Quigley, Moraan and Shirley, Mark and Deans, Matthew and Cannon, Howard and Fong, Terry},
	year = {2019},
	pages = {1--19},
	file = {Allan et al. - 2019 - Planetary Rover Simulation for Lunar Exploration M.pdf:C\:\\Users\\wb619\\Zotero\\storage\\KXD9ZFNM\\Allan et al. - 2019 - Planetary Rover Simulation for Lunar Exploration M.pdf:application/pdf},
}

@inproceedings{lemaignan_simulation_2014,
	address = {Cham},
	title = {Simulation and {HRI} {Recent} {Perspectives} with the {MORSE} {Simulator}},
	isbn = {978-3-319-11900-7},
	doi = {10.1007/978-3-319-11900-7_2},
	abstract = {Simulation in robotics is often a love-hate relationship: while simulators do save us a lot of time and effort compared to regular deployment of complex software architectures on complex hardware, simulators are also known to evade many of the real issues that robots need to manage when they enter the real world. Because humans are the paragon of dynamic, unpredictable, complex, real world entities, simulation of human-robot interactions may look condemn to fail, or, in the best case, to be mostly useless. This collective article reports on five independent applications of the MORSE simulator in the field of human-robot interaction: It appears that simulation is already useful, if not essential, to successfully carry out research in the field of HRI, and sometimes in scenarios we do not anticipate.},
	booktitle = {International {Conference} on {Simulation}, {Modeling}, and {Programming} for {Autonomous} {Robots}},
	publisher = {Springer International Publishing},
	author = {Lemaignan, Séverin and Hanheide, Marc and Karg, Michael and Khambhaita, Harmish and Kunze, Lars and Lier, Florian and Lütkebohle, Ingo and Milliez, Grégoire},
	editor = {Brugali, Davide and Broenink, Jan F. and Kroeger, Torsten and MacDonald, Bruce A.},
	year = {2014},
	pages = {13--24},
	file = {Lemaignan et al. - 2014 - Simulation and HRI Recent Perspectives with the MO.pdf:C\:\\Users\\wb619\\Zotero\\storage\\RFCK3GCJ\\Lemaignan et al. - 2014 - Simulation and HRI Recent Perspectives with the MO.pdf:application/pdf},
}

@article{denninger_blenderproc2_2023,
	title = {{BlenderProc2}: {A} {Procedural} {Pipeline} for {Photorealistic} {Rendering}},
	volume = {8},
	url = {https://doi.org/10.21105/joss.04901},
	doi = {10.21105/joss.04901},
	number = {82},
	journal = {Journal of Open Source Software},
	author = {Denninger, Maximilian and Winkelbauer, Dominik and Sundermeyer, Martin and Boerdijk, Wout and Knauer, Markus and Strobl, Klaus H. and Humt, Matthias and Triebel, Rudolph},
	year = {2023},
	note = {Publisher: The Open Journal},
	pages = {4901},
	file = {Denninger et al. - 2023 - BlenderProc2 A Procedural Pipeline for Photoreali.pdf:C\:\\Users\\wb619\\Zotero\\storage\\J32BSE5W\\Denninger et al. - 2023 - BlenderProc2 A Procedural Pipeline for Photoreali.pdf:application/pdf},
}

@inproceedings{hinterstoisser_annotation_2019,
	address = {Seoul, South Korea},
	title = {An {Annotation} {Saved} is an {Annotation} {Earned}: {Using} {Fully} {Synthetic} {Training} for {Object} {Detection}},
	doi = {10.1109/ICCVW.2019.00340},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV}) {Workshop}},
	author = {Hinterstoisser, Stefan and Pauly, Olivier and Heibel, Hauke and Martina, Marek and Bokeloh, Martin},
	month = oct,
	year = {2019},
	pages = {2787--2796},
	file = {Hinterstoisser et al. - 2019 - An Annotation Saved is an Annotation Earned Using.pdf:C\:\\Users\\wb619\\Zotero\\storage\\9RHSMPVY\\Hinterstoisser et al. - 2019 - An Annotation Saved is an Annotation Earned Using.pdf:application/pdf},
}

@article{lewis_developing_2003,
	title = {Developing a {Testbed} for {Studying} {Human}-{Robot} {Interaction} in {Urban} {Search} and {Rescue}},
	journal = {Simulation},
	author = {Lewis, Michael and Sycara, Katia P. and Nourbakhsh, Illah Reza},
	year = {2003},
}

@inproceedings{lewis_network-centric_2011,
	title = {Network-{Centric} {Control} for {Multirobot} {Teams} in {Urban} {Search} and {Rescue}},
	doi = {10.1109/HICSS.2011.315},
	booktitle = {2011 44th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Lewis, Michael and Sycara, Katia},
	year = {2011},
	pages = {1--10},
	file = {Lewis and Sycara - 2011 - Network-Centric Control for Multirobot Teams in Ur.pdf:C\:\\Users\\wb619\\Zotero\\storage\\JKR2QSJF\\Lewis and Sycara - 2011 - Network-Centric Control for Multirobot Teams in Ur.pdf:application/pdf},
}

@article{elmquist_modeling_2021,
	title = {Modeling {Cameras} for {Autonomous} {Vehicle} and {Robot} {Simulation}: {An} {Overview}},
	volume = {21},
	doi = {10.1109/JSEN.2021.3118952},
	number = {22},
	journal = {IEEE Sensors Journal},
	author = {Elmquist, Asher and Negrut, Dan},
	year = {2021},
	pages = {25547--25560},
	file = {Elmquist and Negrut - 2021 - Modeling Cameras for Autonomous Vehicle and Robot .pdf:C\:\\Users\\wb619\\Zotero\\storage\\UHEANW4J\\Elmquist and Negrut - 2021 - Modeling Cameras for Autonomous Vehicle and Robot .pdf:application/pdf},
}

@inproceedings{garcia-garcia_robotrix_2018,
	address = {Madrid, Spain},
	title = {The {RobotriX}: {An} {Extremely} {Photorealistic} and {Very}-{Large}-{Scale} {Indoor} {Dataset} of {Sequences} with {Robot} {Trajectories} and {Interactions}},
	doi = {10.1109/IROS.2018.8594495},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Garcia-Garcia, Alberto and Martinez-Gonzalez, Pablo and Oprea, Sergiu and Castro-Vargas, John Alejandro and Orts-Escolano, Sergio and Garcia-Rodriguez, Jose and Jover-Alvarez, Alvaro},
	month = oct,
	year = {2018},
	pages = {6790--6797},
	file = {Garcia-Garcia et al. - 2018 - The RobotriX An Extremely Photorealistic and Very.pdf:C\:\\Users\\wb619\\Zotero\\storage\\UUH8W5WA\\Garcia-Garcia et al. - 2018 - The RobotriX An Extremely Photorealistic and Very.pdf:application/pdf},
}

@article{liu_using_2023,
	title = {Using simulation to quantify the performance of automotive perception systems},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	url = {https://arxiv.org/abs/2303.00983},
	doi = {10.48550/ARXIV.2303.00983},
	journal = {arXiv},
	author = {Liu, Zhenyi and Shah, Devesh and Rahimpour, Alireza and Upadhyay, Devesh and Farrell, Joyce and Wandell, Brian A},
	year = {2023},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Graphics (cs.GR), electronic engineering, FOS: Electrical engineering, Image and Video Processing (eess.IV), information engineering},
	file = {Liu et al. - 2023 - Using simulation to quantify the performance of au.pdf:C\:\\Users\\wb619\\Zotero\\storage\\L8PTG8CQ\\Liu et al. - 2023 - Using simulation to quantify the performance of au.pdf:application/pdf},
}

@inproceedings{weinmann_material_2014,
	address = {Zurich, Switzerland},
	title = {Material {Classification} {Based} on {Training} {Data} {Synthesized} {Using} a {BTF} {Database}},
	isbn = {978-3-319-10578-9},
	doi = {10.1007/978-3-319-10578-9_11},
	abstract = {To cope with the richness in appearance variation found in real-world data under natural illumination, we propose to synthesize training data capturing these variations for material classification. Using synthetic training data created from separately acquired material and illumination characteristics allows to overcome the problems of existing material databases which only include a tiny fraction of the possible real-world conditions under controlled laboratory environments. However, it is essential to utilize a representation for material appearance which preserves fine details in the reflectance behavior of the digitized materials. As BRDFs are not sufficient for many materials due to the lack of modeling mesoscopic effects, we present a high-quality BTF database with 22,801 densely measured view-light configurations including surface geometry measurements for each of the 84 measured material samples. This representation is used to generate a database of synthesized images depicting the materials under different view-light conditions with their characteristic surface geometry using image-based lighting to simulate the complexity of real-world scenarios. We demonstrate that our synthesized data allows classifying materials under complex real-world scenarios.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Weinmann, Michael and Gall, Juergen and Klein, Reinhard},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	month = sep,
	year = {2014},
	pages = {156--171},
	file = {Weinmann et al. - 2014 - Material Classification Based on Training Data Syn.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LJLRKCQ6\\Weinmann et al. - 2014 - Material Classification Based on Training Data Syn.pdf:application/pdf},
}

@article{dana_reflectance_1999,
	title = {Reflectance and {Texture} of {Real}-{World} {Surfaces}},
	volume = {18},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/300776.300778},
	doi = {10.1145/300776.300778},
	abstract = {In this work, we investigate the visual appearance of real-world surfaces and the dependence of appearance on the geometry of imaging conditions. We discuss a new texture representation called the BTF (bidirectional texture function) which captures the variation in texture with illumination and viewing direction. We present a BTF database with image textures from over 60 different samples, each observed with over 200 different combinations of viewing and illumination directions. We describe the methods involved in collecting the database as well as the importqance and uniqueness of this database for computer graphics. A related quantity to the BTF is the familiar BRDF (bidirectional reflectance distribution function). The measurement methods involved in the BTF database are conducive to simultaneous measurement of the BRDF. Accordingly, we also present a BRDF database with reflectance measurements for over 60 different samples, each observed with over 200 different combinations of viewing and illumination directions. Both of these unique databases are publicly available and have important implications for computer graphics.},
	number = {1},
	journal = {ACM Transactions on Graphics},
	author = {Dana, Kristin J. and van Ginneken, Bram and Nayar, Shree K. and Koenderink, Jan J.},
	year = {1999},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {1--34},
	file = {Dana et al. - 1999 - Reflectance and Texture of Real-World Surfaces.pdf:C\:\\Users\\wb619\\Zotero\\storage\\U5PWS47A\\Dana et al. - 1999 - Reflectance and Texture of Real-World Surfaces.pdf:application/pdf},
}

@article{filip_template-based_2014,
	title = {Template-{Based} {Sampling} of {Anisotropic} {BRDFs}},
	volume = {33},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12477},
	doi = {https://doi.org/10.1111/cgf.12477},
	abstract = {Abstract BRDFs are commonly used to represent given materials’ appearance in computer graphics and related fields. Although, in the recent past, BRDFs have been extensively measured, compressed, and fitted by a variety of analytical models, most research has been primarily focused on simplified isotropic BRDFs. In this paper, we present a unique database of 150 BRDFs representing a wide range of materials; the majority exhibiting anisotropic behavior. Since time-consuming BRDF measurement represents a major obstacle in the digital material appearance reproduction pipeline, we tested several approaches estimating a very limited set of samples capable of high quality appearance reconstruction. Initially, we aligned all measured BRDFs according to the location of the anisotropic highlights. Then we propose an adaptive sampling method based on analysis of the measured BRDFs. For each BRDF, a unique sampling pattern was computed, given a predefined count of samples. Further, template-based methods are introduced based on reusing of the precomputed sampling patterns. This approach enables a more efficient measurement of unknown BRDFs while preserving the visual fidelity for the majority of tested materials. Our method exhibits better performance and stability than competing sparse sampling approaches; especially for higher numbers of samples.},
	number = {7},
	journal = {Computer Graphics Forum},
	author = {Filip, J. and Vávra, R.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12477},
	keywords = {and texture, Categories and Subject Descriptors (according to ACM CCS), shading, shadowing, I.3.4 Computer Graphics: Digitization and Image Capture—Reflectance, I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Color},
	pages = {91--99},
	file = {Filip and Vávra - 2014 - Template-Based Sampling of Anisotropic BRDFs.pdf:C\:\\Users\\wb619\\Zotero\\storage\\M6Q9Q3UA\\Filip and Vávra - 2014 - Template-Based Sampling of Anisotropic BRDFs.pdf:application/pdf},
}

@article{matusik_data-driven_2003,
	title = {A {Data}-{Driven} {Reflectance} {Model}},
	volume = {22},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/882262.882343},
	doi = {10.1145/882262.882343},
	abstract = {We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs.},
	number = {3},
	journal = {ACM Transactions on Graphics},
	author = {Matusik, Wojciech and Pfister, Hanspeter and Brand, Matt and McMillan, Leonard},
	year = {2003},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {BRDF, image-based modeling, light reflection models, photometric measurements, reflectance},
	pages = {759--769},
	file = {Matusik et al. - 2003 - A Data-Driven Reflectance Model.pdf:C\:\\Users\\wb619\\Zotero\\storage\\G7QZBM3T\\Matusik et al. - 2003 - A Data-Driven Reflectance Model.pdf:application/pdf},
}

@book{lambert_photometria_1760,
	title = {Photometria sive de mensura et gradibus luminis, colorum et umbrae},
	publisher = {Klett},
	author = {Lambert, Johann Heinrich},
	year = {1760},
}

@article{phong_illumination_1975,
	title = {Illumination for {Computer} {Generated} {Pictures}},
	volume = {18},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/360825.360839},
	doi = {10.1145/360825.360839},
	abstract = {The quality of computer generated images of three-dimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen. The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm. The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here. Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images.},
	number = {6},
	journal = {Communications of the ACM},
	author = {Phong, Bui Tuong},
	year = {1975},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {shading, computer graphics, graphic display, hidden surface removal},
	pages = {311--317},
	file = {Phong - 1975 - Illumination for Computer Generated Pictures.pdf:C\:\\Users\\wb619\\Zotero\\storage\\M3DKLV8L\\Phong - 1975 - Illumination for Computer Generated Pictures.pdf:application/pdf},
}

@misc{walt_disney_animation_studios_brdf_2012,
	title = {{BRDF} {Explorer}},
	url = {https://github.com/wdas/brdf},
	urldate = {2023-03-12},
	author = {Walt Disney Animation Studios},
	year = {2012},
	note = {Publication Title: GitHub repository},
}

@article{trowbridge_average_1975,
	title = {Average irregularity representation of a rough surface for ray reflection},
	volume = {65},
	url = {https://opg.optica.org/abstract.cfm?URI=josa-65-5-531},
	doi = {10.1364/JOSA.65.000531},
	abstract = {A new ray model is presented for the reflection of electromagnetic radiation from the rough air-material interface of a randomly rough surface. Unlike previous derivations that modeled the rough interface as consisting of microareas randomly oriented but flat (facets), this derivation models it as consisting of microareas not only randomly oriented but also randomly curved. Physically, the models are the same, but this new derivation leads to some new results. (1) For any given rough surface, there exists a single, optically smooth, curved surface of revolution of very restricted shape that will reflect radiation in the same distribution as that reflected by the rough interface. (2) Modeling that surface as an ellipsoid of revolution gives a surface-structure function that appears more accurate and useful than existing ones. (3) Unlike the facet derivations, this derivation lends itself to a normalization that gives the absolute, instead of just a comparative, reflectance-distribution function.},
	number = {5},
	journal = {Journal of the Optical Society of America},
	author = {Trowbridge, T. S. and Reitz, K. P.},
	year = {1975},
	note = {Publisher: Optica Publishing Group},
	keywords = {Diffraction, Electromagnetic radiation, Fresnel equations, Reflection, Refractive index, Surfaces},
	pages = {531--536},
	file = {Trowbridge and Reitz - 1975 - Average irregularity representation of a rough sur.pdf:C\:\\Users\\wb619\\Zotero\\storage\\PECJDSSV\\Trowbridge and Reitz - 1975 - Average irregularity representation of a rough sur.pdf:application/pdf},
}

@inproceedings{walter_microfacet_2007,
	address = {Grenoble, France},
	title = {Microfacet models for refraction through rough surfaces},
	booktitle = {Eurographics conference on {Rendering} {Techniques}},
	author = {Walter, Bruce and Marschner, Stephen R and Li, Hongsong and Torrance, Kenneth E},
	month = jun,
	year = {2007},
	pages = {195--206},
	file = {Walter et al. - 2007 - Microfacet models for refraction through rough sur.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LCNBZRD5\\Walter et al. - 2007 - Microfacet models for refraction through rough sur.pdf:application/pdf},
}

@article{hapke_bidirectional_1981,
	title = {Bidirectional reflectance spectroscopy: 1. {Theory}},
	volume = {86},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/JB086iB04p03039},
	doi = {10.1029/JB086iB04p03039},
	abstract = {An approximate analytic solution to the radiative transfer equation describing the scattering of light from particulate surfaces is derived. Multiple scattering and mutual shadowing are taken into account. Analytic expressions for the following quantities are found: bidirectional reflectance, radiance factor, radiance coefficient, normal, hemispherical, Bond, and physical albedos, integral phase function, phase integral, and limb-darkening profile. Scattering functions for mixtures can be calculated, as well as corrections for comparing experimental laboratory transmission or reflection spectra with observational planetary spectra. An expression for the scattering efficiency of an irregular particle large compared with the wavelength is derived. For closely spaced, nonopaque particles this efficiency is approximated by (1 + αDe)−l, where α is the true absorption coefficient and De is an effective particle diameter of the order of twice the mean particle size. For monomineralic surfaces it is shown that α = ( 1 − w)/wDe, where w is the single-scattering albedo and can be determined from reflectance measurements of a powder, so that α may be calculated from reflectance. This theory should be useful for interpretations of reflectance spectroscopy of laboratory surfaces and photometry of solar system objects. From photometric observations of a body the following may be estimated: average single-scattering albedo, average particle phase function, average macroscopic slope, and porosity.},
	number = {B4},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Hapke, Bruce},
	year = {1981},
	pages = {3039--3054},
	file = {Hapke - 1981 - Bidirectional reflectance spectroscopy 1. Theory.pdf:C\:\\Users\\wb619\\Zotero\\storage\\IPQN38P4\\Hapke - 1981 - Bidirectional reflectance spectroscopy 1. Theory.pdf:application/pdf},
}

@article{sato_resolved_2014,
	title = {Resolved {Hapke} parameter maps of the {Moon}},
	volume = {119},
	url = {https://agupubs-onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/abs/10.1002/2013JE004580},
	doi = {10.1002/2013JE004580},
	abstract = {AbstractWe derived spatially resolved near-global Hapke photometric parameter maps of the Moon from 21 months of Lunar Reconnaissance Orbiter Camera (LROC) Wide Angle Camera (WAC) multispectral observations using a novel “tile-by-tile method” (1° latitude by 1° longitude bins). The derived six parameters (w,b,c,BS0,hS, and) for each tile were used to normalize the observed reflectance (standard angles i = g = 60°, e = 0° instead of the traditional angles i = g = 30°, e = 0°) within each tile, resulting in accurate normalization optimized for the local photometric response. Each pixel in the seven-color near-global mosaic (70°S to 70°N and 0°E to 360°E) was computed by the median of normalized reflectance from large numbers of repeated observations (UV: ∼50 and visible: ∼126 on average). The derived mosaic exhibits no significant artifacts with latitude or along the tile boundaries, demonstrating the quality of the normalization procedure. The derived Hapke parameter maps reveal regional photometric response variations across the lunar surface. The b, c (Henyey-Greenstein double-lobed phase function parameters) maps demonstrate decreased backscattering in the maria relative to the highlands (except 321 nm band), probably due to the higher content of both SMFe (submicron iron) and ilmenite in the interiors of back scattering agglutinates in the maria. The hS (angular width of shadow hiding opposition effect) map exhibits relatively lower values in the maria than the highlands and slightly higher values for immature highland crater ejecta, possibly related to the variation in a grain size distribution of regolith.},
	number = {8},
	journal = {Journal of Geophysical Research: Planets},
	author = {Sato, H. and Robinson, M. S. and Hapke, B. and Denevi, B. W. and Boyd, A. K.},
	year = {2014},
	keywords = {LROC, Moon, photometry, surface reflectance},
	pages = {1775--1805},
	file = {Sato et al. - 2014 - Resolved Hapke parameter maps of the Moon.pdf:C\:\\Users\\wb619\\Zotero\\storage\\47VAR87R\\Sato et al. - 2014 - Resolved Hapke parameter maps of the Moon.pdf:application/pdf},
}

@phdthesis{kuzminykh_physically_2021,
	type = {Bachelor {Thesis}},
	title = {Physically {Based} {Real}-{Time} {Rendering} of the {Moon}},
	language = {English},
	urldate = {2023-03-13},
	school = {Course of Studies B. Sc. Media Design Computing, Hochschule Hannover},
	author = {Kuzminykh, Alexander},
	month = aug,
	year = {2021},
	file = {Kuzminykh - 2021 - Physically Based Real-Time Rendering of the Moon.pdf:C\:\\Users\\wb619\\Zotero\\storage\\D2ZPKBH6\\Kuzminykh - 2021 - Physically Based Real-Time Rendering of the Moon.pdf:application/pdf},
}

@article{elmquist_sensor_2021,
	title = {A {Sensor} {Simulation} {Framework} for {Training} and {Testing} {Robots} and {Autonomous} {Vehicles}},
	volume = {1},
	issn = {2690-702X},
	url = {https://doi.org/10.1115/1.4050080},
	doi = {10.1115/1.4050080},
	abstract = {Computer simulation can be a useful tool when designing robots expected to operate independently in unstructured environments. In this context, one needs to simulate the dynamics of the robot’s mechanical system, the environment in which the robot operates, and the sensors which facilitate the robot’s perception of the environment. Herein, we focus on the sensing simulation task by presenting a virtual sensing framework built alongside an open-source, multi-physics simulation platform called Chrono. This framework supports camera, lidar, GPS, and IMU simulation. We discuss their modeling as well as the noise and distortion implemented to increase the realism of the synthetic sensor data. We close with two examples that show the sensing simulation framework at work: one pertains to a reduced scale autonomous vehicle and the second is related to a vehicle driven in a digital replica of a Madison neighborhood.},
	number = {2},
	journal = {Journal of Autonomous Vehicles and Systems},
	author = {Elmquist, Asher and Serban, Radu and Negrut, Dan},
	year = {2021},
	note = {\_eprint: https://asmedigitalcollection.asme.org/autonomousvehicles/article-pdf/1/2/021001/6647233/javs\_1\_2\_021001.pdf},
	pages = {021001--1-- 021001--10},
	file = {Elmquist et al. - 2021 - A Sensor Simulation Framework for Training and Tes.pdf:C\:\\Users\\wb619\\Zotero\\storage\\DT5AM259\\Elmquist et al. - 2021 - A Sensor Simulation Framework for Training and Tes.pdf:application/pdf},
}

@article{serban_chronovehicle_2019,
	title = {Chrono::{Vehicle}: template-based ground vehicle modelling and simulation},
	volume = {5},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJVP.2019.097096},
	doi = {10.1504/IJVP.2019.097096},
	abstract = {Chrono::Vehicle is a module of the open-source multi-physics simulation package Chrono, aimed at modelling, simulation, and visualisation of wheeled and tracked ground vehicle multi-body systems. Its software architecture and design was dictated by the desire to provide an expeditious and user friendly mechanism for assembling complex vehicle models, while leveraging the underlying Chrono modelling and simulation capabilities, allowing seamless interfacing to other optional Chrono modules (e.g., its granular dynamics and fluid-solid interaction (FSI) capabilities), and providing a modular and expressive API to facilitate its use in third-party applications. Vehicle models are specified as a hierarchy of subsystems, each of which is an instantiation of a predefined subsystem template. Written in C++, Chrono::Vehicle is offered as a middleware library. In this paper, we provide an overview of the Chrono::Vehicle software design philosophy, its main capabilities and features, describe the types of ground vehicle mobility simulations it enables, and outline several directions of future development and planned extensions.},
	number = {1},
	journal = {International Journal of Vehicle Performance},
	author = {Serban, Radu and Taylor, Michael and Negrut, Dan and Tasora, Alessandro},
	year = {2019},
	note = {\_eprint: https://www.inderscienceonline.com/doi/pdf/10.1504/IJVP.2019.097096},
	pages = {18--39},
}

@inproceedings{koenig_design_2004,
	address = {Sendai, Japan},
	title = {Design and use paradigms for {Gazebo}, an open-source multi-robot simulator},
	volume = {3},
	doi = {10.1109/IROS.2004.1389727},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Koenig, Nathan and Howard, Andrew},
	month = sep,
	year = {2004},
	pages = {2149--2154},
	file = {Koenig and Howard - 2004 - Design and use paradigms for Gazebo, an open-sourc.pdf:C\:\\Users\\wb619\\Zotero\\storage\\9BX3EDSK\\Koenig and Howard - 2004 - Design and use paradigms for Gazebo, an open-sourc.pdf:application/pdf},
}

@article{chen_digital_2009,
	title = {Digital {Camera} {Imaging} {System} {Simulation}},
	volume = {56},
	doi = {10.1109/TED.2009.2030995},
	number = {11},
	journal = {IEEE Transactions on Electron Devices},
	author = {Chen, Junqing and Venkataraman, Kartik and Bakin, Dmitry and Rodricks, Brian and Gravelle, Robert and Rao, Pravin and Ni, Yongshen},
	year = {2009},
	pages = {2496--2505},
	file = {Chen et al. - 2009 - Digital Camera Imaging System Simulation.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4CVZDMP9\\Chen et al. - 2009 - Digital Camera Imaging System Simulation.pdf:application/pdf},
}

@article{ulbricht_verification_2006,
	title = {Verification of {Physically} {Based} {Rendering} {Algorithms}},
	volume = {25},
	url = {https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/abs/10.1111/j.1467-8659.2006.00938.x},
	doi = {https://doi-org.ezproxy.library.wisc.edu/10.1111/j.1467-8659.2006.00938.x},
	abstract = {Abstract Within computer graphics, the field of predictive rendering is concerned with those methods of image synthesis that yield results that do not only look real, but are also radiometrically correct renditions of nature, i.e. which are accurate predictions of what a real scene would look like under given lighting conditions. In order to guarantee the correctness of the results obtained by such techniques, three stages of such a rendering system have to be verified with particular care: the light reflection models, the light transport simulation and the perceptually based calculations used at display time. In this report, we will concentrate on the state of the art with respect to the second step in this chain. Various approaches for experimental verification of the implementation of a physically based rendering system have been proposed so far. However, the problem of proving that the results are correct is not fully solved yet, and no standardized methodology is available. We give an overview of existing literature, discuss the strengths and weaknesses of the described methods and illustrate the unsolved problems. We also briefly discuss the related issue of image quality metrics.},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Ulbricht, Christiane and Wilkie, Alexander and Purgathofer, Werner},
	year = {2006},
	note = {\_eprint: https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/pdfdirect/10.1111/j.1467-8659.2006.00938.x},
	keywords = {global illumination, experimental validation, I.6.4 Simulation and Modelling Model Validation and Analysis, images quality metrics, physically based rendering, test scenes, visual comparisons, Color metric},
	pages = {237--255},
	file = {Ulbricht et al. - 2006 - Verification of Physically Based Rendering Algorit.pdf:C\:\\Users\\wb619\\Zotero\\storage\\J4U3VJIY\\Ulbricht et al. - 2006 - Verification of Physically Based Rendering Algorit.pdf:application/pdf},
}

@article{meyer_experimental_1986,
	title = {An {Experimental} {Evaluation} of {Computer} {Graphics} {Imagery}},
	volume = {5},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/7529.7920},
	doi = {10.1145/7529.7920},
	abstract = {Accurate simulation of light propagation within an environment and perceptually based imaging techniques are necessary for the creation of realistic images. A physical experiment that verifies the simulation of reflected light intensities for diffuse environments was conducted. Measurements of radiant energy flux densities are compared with predictions using the radiosity method for those physical environments. By using color science procedures the results of the light model simulation are then transformed to produce a color television image. The final image compares favorably with the original physical model. The experiment indicates that, when the physical model and the simulation were viewed through a view camera, subjects could not distinguish between them. The results and comparison of both test procedures are presented within this paper.},
	number = {1},
	journal = {ACM Transactions on Graphics},
	author = {Meyer, Gary W. and Rushmeier, Holly E. and Cohen, Michael F. and Greenberg, Donald P. and Torrance, Kenneth E.},
	year = {1986},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cornell box},
	pages = {30--50},
	file = {Meyer et al. - 1986 - An Experimental Evaluation of Computer Graphics Im.pdf:C\:\\Users\\wb619\\Zotero\\storage\\TF43T8RI\\Meyer et al. - 1986 - An Experimental Evaluation of Computer Graphics Im.pdf:application/pdf},
}

@article{lyu_validation_2022,
	title = {Validation of {Physics}-{Based} {Image} {Systems} {Simulation} {With} 3-{D} {Scenes}},
	volume = {22},
	doi = {10.1109/JSEN.2022.3199699},
	number = {20},
	journal = {IEEE Sensors Journal},
	author = {Lyu, Zheng and Goossens, Thomas and Wandell, Brian A. and Farrell, Joyce},
	year = {2022},
	keywords = {Color metric},
	pages = {19400--19410},
	file = {Lyu et al. - 2022 - Validation of Physics-Based Image Systems Simulati.pdf:C\:\\Users\\wb619\\Zotero\\storage\\VUU2QSQX\\Lyu et al. - 2022 - Validation of Physics-Based Image Systems Simulati.pdf:application/pdf},
}

@article{grapinet_characterization_2013,
	title = {Characterization and simulation of optical sensors},
	volume = {60},
	issn = {0001-4575},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457513001693},
	doi = {10.1016/j.aap.2013.04.026},
	abstract = {Numerical simulation is gradually becoming an advantage in active safety. This is why the development of realistic numerical models enabling to substitute real truth by simulated truth is primordial. In order to provide an accurate and cost effective solution to simulate real optical sensor behavior, the software Pro-SiVIC™ has been developed. Simulations with the software Pro-SiVIC™ can replace real tests with optical sensors and hence allow substantial cost and time savings during the development of solutions for driver assistance systems. An optical platform has been developed by IFSTTAR (French Institute of Science and Technology for Transport, Development and Networks) to characterize and validate any existing camera, in order to measure their characteristics as distortion, vignetting, focal length, etc. By comparing real and simulated sensors with this platform, this paper demonstrates that Pro-SiVIC™ accurately reproduces real sensors’ behavior.},
	journal = {Accident Analysis and Prevention},
	author = {Grapinet, M. and Souza, Ph De and Smal, J.-C. and Blosseville, J.-M.},
	year = {2013},
	keywords = {Characterization platform, Driving assistance system, Numerical simulation, Optical sensor, Pro-SiVIC},
	pages = {344--352},
	file = {Grapinet et al. - 2013 - Characterization and simulation of optical sensors.pdf:C\:\\Users\\wb619\\Zotero\\storage\\BKJ7MBVM\\Grapinet et al. - 2013 - Characterization and simulation of optical sensors.pdf:application/pdf},
}

@inproceedings{gruyer_modeling_2012,
	address = {Madrid, Spain},
	title = {Modeling and validation of a new generic virtual optical sensor for {ADAS} prototyping},
	doi = {10.1109/IVS.2012.6232260},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Gruyer, D. and Grapinet, M. and De Souza, P.},
	month = jun,
	year = {2012},
	pages = {969--974},
	file = {Gruyer et al. - 2012 - Modeling and validation of a new generic virtual o.pdf:C\:\\Users\\wb619\\Zotero\\storage\\3XFBVNI4\\Gruyer et al. - 2012 - Modeling and validation of a new generic virtual o.pdf:application/pdf},
}

@inproceedings{peterson_surface_2004,
	address = {Denver, Colorado, United States},
	title = {Surface and buried landmine scene generation and validation using the digital imaging and remote sensing image generation model},
	volume = {5546},
	url = {https://doi.org/10.1117/12.561264},
	doi = {10.1117/12.561264},
	booktitle = {Imaging {Spectrometry} {X}},
	publisher = {SPIE},
	author = {Peterson, Erin D. and Brown, Scott D. and Hattenberger, Timothy J. and Schott, John R.},
	editor = {Shen, Sylvia S. and Lewis, Paul E.},
	year = {2004},
	note = {Backup Publisher: International Society for Optics and Photonics},
	keywords = {Color metric, DIRSIG, hyperspectral image simulation, long wave infrared, mine detection, reststrahlen},
	pages = {312 -- 323},
	file = {Peterson et al. - 2004 - Surface and buried landmine scene generation and v.pdf:C\:\\Users\\wb619\\Zotero\\storage\\Q6TBT4ET\\Peterson et al. - 2004 - Surface and buried landmine scene generation and v.pdf:application/pdf},
}

@article{clausen_acquisition_2018,
	title = {Acquisition and {Validation} of {Spectral} {Ground} {Truth} {Data} for {Predictive} {Rendering} of {Rough} {Surfaces}},
	volume = {37},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13470},
	doi = {10.1111/cgf.13470},
	abstract = {Abstract Physically based rendering uses principles of physics to model the interaction of light with matter. Even though it is possible to achieve photorealistic renderings, it often fails to be predictive. There are two major issues: first, there is no analytic material model that considers all appearance critical characteristics; second, light is in many cases described by only 3 RGB-samples. This leads to the problem that there are different models for different material types and that wavelength dependent phenomena are only approximated. In order to be able to analyze the influence of both problems on the appearance of real world materials, an accurate comparison between rendering and reality is necessary. Therefore, in this work, we acquired a set of precisely and spectrally resolved ground truth data. It consists of the precise description of a new developed reference scene including isotropic BRDFs of 24 color patches, as well as the reference measurements of all patches under 13 different angles inside the reference scene. Our reference data covers rough materials with many different spectral distributions and various illumination situations, from direct light to indirect light dominated situations.},
	number = {4},
	journal = {Computer Graphics Forum},
	author = {Clausen, O. and Marroquim, R. and Fuhrmann, A.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13470},
	keywords = {Color metric, •Computing methodologies → Reflectance modeling, CCS Concepts},
	pages = {1--12},
	file = {Clausen et al. - 2018 - Acquisition and Validation of Spectral Ground Trut.pdf:C\:\\Users\\wb619\\Zotero\\storage\\QY36YMJN\\Clausen et al. - 2018 - Acquisition and Validation of Spectral Ground Trut.pdf:application/pdf},
}

@inproceedings{hasirlioglu_model-based_2018,
	address = {Maui, HI, USA},
	title = {A {Model}-{Based} {Approach} to {Simulate} {Rain} {Effects} on {Automotive} {Surround} {Sensor} {Data}},
	doi = {10.1109/ITSC.2018.8569907},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Hasirlioglu, Sinan and Riener, Andreas},
	month = nov,
	year = {2018},
	pages = {2609--2615},
	file = {Hasirlioglu and Riener - 2018 - A Model-Based Approach to Simulate Rain Effects on.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6VRIVUND\\Hasirlioglu and Riener - 2018 - A Model-Based Approach to Simulate Rain Effects on.pdf:application/pdf},
}

@article{hasirlioglu_general_2020,
	title = {A {General} {Approach} for {Simulating} {Rain} {Effects} on {Sensor} {Data} in {Real} and {Virtual} {Environments}},
	volume = {5},
	doi = {10.1109/TIV.2019.2960944},
	number = {3},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Hasirlioglu, Sinan and Riener, Andreas},
	year = {2020},
	pages = {426--438},
	file = {Hasirlioglu and Riener - 2020 - A General Approach for Simulating Rain Effects on .pdf:C\:\\Users\\wb619\\Zotero\\storage\\649KKHCF\\Hasirlioglu and Riener - 2020 - A General Approach for Simulating Rain Effects on .pdf:application/pdf},
}

@article{lyu_simulations_2021,
	title = {Simulations of fluorescence imaging in the oral cavity},
	volume = {12},
	doi = {10.1364/BOE.429995},
	abstract = {We describe an end-to-end image systems simulation that models a device capable of measuring fluorescence in the oral cavity. Our software includes a 3D model of the oral cavity and excitation-emission matrices of endogenous fluorophores that predict the spectral radiance of oral mucosal tissue. The predicted radiance is transformed by a model of the optics and image sensor to generate expected sensor image values. We compare simulated and real camera data from tongues in healthy individuals and show that the camera sensor chromaticity values can be used to quantify the fluorescence from porphyrins relative to the bulk fluorescence from multiple fluorophores (elastin, NADH, FAD, and collagen). Validation of the simulations supports the use of soft-prototyping in guiding system design for fluorescence imaging.},
	number = {7},
	journal = {Biomedical Optics Express},
	author = {Lyu, Zheng and Jiang, Haomiao and Xiao, Feng and Rong, Jian and Zhang, Tingcheng and Wandell, Brian and Farrell, Joyce},
	year = {2021},
	pages = {4276--4292},
	file = {Lyu et al. - 2021 - Simulations of fluorescence imaging in the oral ca.pdf:C\:\\Users\\wb619\\Zotero\\storage\\9XCRFAVY\\Lyu et al. - 2021 - Simulations of fluorescence imaging in the oral ca.pdf:application/pdf},
}

@inproceedings{liu_soft_2019,
	address = {Seoul, South Korea},
	title = {Soft {Prototyping} {Camera} {Designs} for {Car} {Detection} {Based} on a {Convolutional} {Neural} {Network}},
	doi = {10.1109/ICCVW.2019.00292},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV}) {Workshop}},
	author = {Liu, Zhenyi and Lian, Trisha and Farrell, Joyce and Wandell, Brian},
	month = oct,
	year = {2019},
	pages = {2383--2392},
	file = {Liu et al. - 2019 - Soft Prototyping Camera Designs for Car Detection .pdf:C\:\\Users\\wb619\\Zotero\\storage\\7YALMMAW\\Liu et al. - 2019 - Soft Prototyping Camera Designs for Car Detection .pdf:application/pdf},
}

@misc{tsirikoglou_procedural_2017,
	title = {Procedural {Modeling} and {Physically} {Based} {Rendering} for {Synthetic} {Data} {Generation} in {Automotive} {Applications}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1710.06270},
	publisher = {arXiv},
	author = {Tsirikoglou, Apostolia and Kronander, Joel and Wrenninge, Magnus and Unger, Jonas},
	year = {2017},
	doi = {10.48550/ARXIV.1710.06270},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, I.4.8, Domain randomization},
	file = {Tsirikoglou et al. - 2017 - Procedural Modeling and Physically Based Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4NEJ2E9A\\Tsirikoglou et al. - 2017 - Procedural Modeling and Physically Based Rendering.pdf:application/pdf},
}

@inproceedings{zhang_physically-based_2017,
	address = {Honolulu, HI, USA},
	title = {Physically-{Based} {Rendering} for {Indoor} {Scene} {Understanding} {Using} {Convolutional} {Neural} {Networks}},
	doi = {10.1109/CVPR.2017.537},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Yinda and Song, Shuran and Yumer, Ersin and Savva, Manolis and Lee, Joon-Young and Jin, Hailin and Funkhouser, Thomas},
	month = jul,
	year = {2017},
	pages = {5057--5065},
	file = {Zhang et al. - 2017 - Physically-Based Rendering for Indoor Scene Unders.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8RR3CCRN\\Zhang et al. - 2017 - Physically-Based Rendering for Indoor Scene Unders.pdf:application/pdf},
}

@inproceedings{hagn_improved_2021,
	address = {Ingolstadt, Germany},
	title = {Improved {Sensor} {Model} for {Realistic} {Synthetic} {Data} {Generation}},
	isbn = {978-1-4503-9139-9},
	doi = {10.1145/3488904.3493383},
	abstract = {Synthetic, i.e., computer generated-imagery (CGI) data is a key component for training and validating deep-learning-based perceptive functions due to its ability to simulate rare cases, avoidance of privacy issues and easy generation of huge datasets with pixel accurate ground-truth data. Recent simulation and rendering engines simulate already a wealth of realistic optical effects, but are mainly focused on the human perception system. But, perceptive functions require realistic images modeled with sensor artifacts as close as possible towards the sensor the training data has been recorded with. In this paper we propose a method to improve the data synthesis by introducing a more realistic sensor model that implements a number of sensor and lens artifacts. We further propose a Wasserstein distance (earth mover’s distance, EMD) based domain divergence measure and use it as minimization criterion to adapt the parameters of our sensor artifact simulation from synthetic to real images. With the optimized sensor parameters applied to the synthetic images for training, the mIoU of a semantic segmentation network (DeeplabV3+) solely trained on synthetic images is increased from 40.36\% to 47.63\%.},
	booktitle = {{ACM} {Computer} {Science} in {Cars} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Hagn, Korbinian and Grau, Oliver},
	month = nov,
	year = {2021},
	note = {event-place: Ingolstadt, Germany},
	keywords = {image synthesis, neural networks, datasets, domain adaptation, sensor simulation},
	file = {Hagn and Grau - 2021 - Improved Sensor Model for Realistic Synthetic Data.pdf:C\:\\Users\\wb619\\Zotero\\storage\\76B8MDE6\\Hagn and Grau - 2021 - Improved Sensor Model for Realistic Synthetic Data.pdf:application/pdf},
}

@article{elmquist_performance_2022,
	title = {A performance contextualization approach to validating camera models for robot simulation},
	doi = {10.48550/arXiv.2208.01022},
	journal = {arXiv e-prints},
	author = {Elmquist, Asher and Serban, Radu and Negrut, Dan},
	month = aug,
	year = {2022},
	note = {\_eprint: 2208.01022},
	keywords = {Computer Science - Robotics},
	pages = {arXiv:2208.01022},
	file = {Elmquist et al. - 2022 - A performance contextualization approach to valida.pdf:C\:\\Users\\wb619\\Zotero\\storage\\F2BBXLVH\\Elmquist et al. - 2022 - A performance contextualization approach to valida.pdf:application/pdf},
}

@inproceedings{foo_image_2022,
	address = {Bordeaux, France},
	title = {Image {Data} {Augmentation} with {Unpaired} {Image}-to-{Image} {Camera} {Model} {Translation}},
	doi = {10.1109/ICIP46576.2022.9897671},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Foo, Chi Fa and Winkler, Stefan},
	month = oct,
	year = {2022},
	pages = {3246--3250},
	file = {Foo and Winkler - 2022 - Image Data Augmentation with Unpaired Image-to-Ima.pdf:C\:\\Users\\wb619\\Zotero\\storage\\M3H5KSL7\\Foo and Winkler - 2022 - Image Data Augmentation with Unpaired Image-to-Ima.pdf:application/pdf},
}

@article{hapke_bidirectional_1986,
	title = {Bidirectional reflectance spectroscopy: 4. {The} extinction coefficient and the opposition effect},
	volume = {67},
	issn = {0019-1035},
	url = {https://www.sciencedirect.com/science/article/pii/0019103586901089},
	doi = {10.1016/0019-1035(86)90108-9},
	abstract = {The extinction coefficient and the opposition effect in a particulate medium are discussed. Simple analytic expressions that describe these quantities are rigorously derived using a few physically realistic mathematical approximations. The particles of the medium may have a distribution of sizes, and the particle density is allowed to vary with depth. The expression for the extinction coefficient is valid for both large and small porosities and is more accurate than the one commonly used. The opposition effect arises from the hiding of extinction shadows and occurs even if the particles are transparent. The angular half-width of the opposition peak is shown to be equal to the ratio of the average particle radius to extinction length at unit slant path optical depth in the medium, and depends on both the filling factor (ratio of bulk to solid density) F and the particle size distribution. To illustrate the theory, it is fitted to observations of the Moon, an asteroid, and a satellite of Uranus; Europa is also discussed. For the Moon, a value of F = 0.41 is derived, in good agreement with data on Apollo soils. For Oberon, the width of the opposition effect peak gives F = 0.10, which is similar to values for terrestrial frosts and snow. Thus, the narrow opposition effects of the Uranian satellites do not require any unusual particles or microstructures on their surfaces. More photometric observations of Europa are needed.},
	number = {2},
	journal = {Icarus},
	author = {Hapke, Bruce},
	year = {1986},
	pages = {264--280},
	file = {Hapke - 1986 - Bidirectional reflectance spectroscopy 4. The ext.pdf:C\:\\Users\\wb619\\Zotero\\storage\\AEZJX5W5\\Hapke - 1986 - Bidirectional reflectance spectroscopy 4. The ext.pdf:application/pdf},
}

@inproceedings{szot_habitat_2021,
	title = {Habitat 2.0: {Training} {Home} {Assistants} to {Rearrange} their {Habitat}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/021bbc7ee20b71134d53e20206bd6feb-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Szot, Andrew and Clegg, Alexander and Undersander, Eric and Wijmans, Erik and Zhao, Yili and Turner, John and Maestre, Noah and Mukadam, Mustafa and Chaplot, Devendra Singh and Maksymets, Oleksandr and Gokaslan, Aaron and Vondruš, Vladimír and Dharur, Sameer and Meier, Franziska and Galuba, Wojciech and Chang, Angel and Kira, Zsolt and Koltun, Vladlen and Malik, Jitendra and Savva, Manolis and Batra, Dhruv},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	month = dec,
	year = {2021},
	pages = {251--266},
	file = {Szot et al. - 2021 - Habitat 2.0 Training Home Assistants to Rearrange.pdf:C\:\\Users\\wb619\\Zotero\\storage\\T8GQMQ3C\\Szot et al. - 2021 - Habitat 2.0 Training Home Assistants to Rearrange.pdf:application/pdf},
}

@inproceedings{shen_igibson_2021,
	address = {Prague, Czech},
	title = {{iGibson} 1.0: {A} {Simulation} {Environment} for {Interactive} {Tasks} in {Large} {Realistic} {Scenes}},
	doi = {10.1109/IROS51168.2021.9636667},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Shen, Bokui and Xia, Fei and Li, Chengshu and Martín-Martín, Roberto and Fan, Linxi and Wang, Guanzhi and Pérez-D’Arpino, Claudia and Buch, Shyamal and Srivastava, Sanjana and Tchapmi, Lyne and Tchapmi, Micael and Vainio, Kent and Wong, Josiah and Fei-Fei, Li and Savarese, Silvio},
	month = sep,
	year = {2021},
	pages = {7520--7527},
}

@inproceedings{amini_vista_2022,
	address = {Philadelphia, PA, USA},
	title = {{VISTA} 2.0: {An} {Open}, {Data}-driven {Simulator} for {Multimodal} {Sensing} and {Policy} {Learning} for {Autonomous} {Vehicles}},
	doi = {10.1109/ICRA46639.2022.9812276},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Amini, Alexander and Wang, Tsun-Hsuan and Gilitschenski, Igor and Schwarting, Wilko and Liu, Zhijian and Han, Song and Karaman, Sertac and Rus, Daniela},
	month = may,
	year = {2022},
	pages = {2419--2426},
	file = {Amini et al. - 2022 - VISTA 2.0 An Open, Data-driven Simulator for Mult.pdf:C\:\\Users\\wb619\\Zotero\\storage\\ZFBW6PLD\\Amini et al. - 2022 - VISTA 2.0 An Open, Data-driven Simulator for Mult.pdf:application/pdf},
}

@article{amini_learning_2020,
	title = {Learning {Robust} {Control} {Policies} for {End}-to-{End} {Autonomous} {Driving} {From} {Data}-{Driven} {Simulation}},
	volume = {5},
	doi = {10.1109/LRA.2020.2966414},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Amini, Alexander and Gilitschenski, Igor and Phillips, Jacob and Moseyko, Julia and Banerjee, Rohan and Karaman, Sertac and Rus, Daniela},
	year = {2020},
	pages = {1143--1150},
	file = {Amini et al. - 2020 - Learning Robust Control Policies for End-to-End Au.pdf:C\:\\Users\\wb619\\Zotero\\storage\\GTDDFR6X\\Amini et al. - 2020 - Learning Robust Control Policies for End-to-End Au.pdf:application/pdf},
}

@article{mazhar_chrono_2013,
	title = {{CHRONO}: a parallel multi-physics library for rigid-body, flexible-body, and fluid dynamics},
	volume = {4},
	url = {https://ms.copernicus.org/articles/4/49/2013/},
	doi = {10.5194/ms-4-49-2013},
	number = {1},
	journal = {Mechanical Sciences},
	author = {Mazhar, H. and Heyn, T. and Pazouki, A. and Melanz, D. and Seidl, A. and Bartholomew, A. and Tasora, A. and Negrut, D.},
	year = {2013},
	pages = {49--64},
	file = {Mazhar et al. - 2013 - CHRONO a parallel multi-physics library for rigid.pdf:C\:\\Users\\wb619\\Zotero\\storage\\P5EH4ULH\\Mazhar et al. - 2013 - CHRONO a parallel multi-physics library for rigid.pdf:application/pdf},
}

@inproceedings{huber_real-time_2009,
	address = {Kyoto, Japan},
	title = {Real-time photo-realistic visualization of {3D} environments for enhanced tele-operation of vehicles},
	doi = {10.1109/ICCVW.2009.5457431},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV}) {Workshops}},
	author = {Huber, Daniel and Herman, Herman and Kelly, Alonzo and Rander, Pete and Ziglar, Jason},
	month = sep,
	year = {2009},
	pages = {1518--1525},
	file = {Huber et al. - 2009 - Real-time photo-realistic visualization of 3D envi.pdf:C\:\\Users\\wb619\\Zotero\\storage\\DWJBM6AR\\Huber et al. - 2009 - Real-time photo-realistic visualization of 3D envi.pdf:application/pdf},
}

@article{kelly_real-time_2011,
	title = {Real-time photorealistic virtualized reality interface for remote mobile robot control},
	volume = {30},
	url = {https://doi.org/10.1177/0278364910383724},
	doi = {10.1177/0278364910383724},
	abstract = {The task of teleoperating a robot over a wireless video link is known to be very difficult. Teleoperation becomes even more difficult when the robot is surrounded by dense obstacles, or speed requirements are high, or video quality is poor, or wireless links are subject to latency. Due to high-quality lidar data, and improvements in computing and video compression, virtualized reality has the capacity to dramatically improve teleoperation performance — even in high-speed situations that were formerly impossible. In this paper, we demonstrate the conversion of dense geometry and appearance data, generated on-the-move by a mobile robot, into a photorealistic rendering model that gives the user a synthetic exterior line-of-sight view of the robot, including the context of its surrounding terrain. This technique converts teleoperation into virtual line-of-sight remote control. The underlying metrically consistent environment model also introduces the capacity to remove latency and enhance video compression. Display quality is sufficiently high that the user experience is similar to a driving video game where the surfaces used are textured with live video.},
	number = {3},
	journal = {The International Journal of Robotics Research},
	author = {Kelly, Alonzo and Chan, Nicholas and Herman, Herman and Huber, Daniel and Meyers, Robert and Rander, Pete and Warner, Randy and Ziglar, Jason and Capstick, Erin},
	year = {2011},
	note = {\_eprint: https://doi.org/10.1177/0278364910383724},
	pages = {384--404},
	file = {Kelly et al. - 2011 - Real-time photorealistic virtualized reality inter.pdf:C\:\\Users\\wb619\\Zotero\\storage\\AWFLJ3K7\\Kelly et al. - 2011 - Real-time photorealistic virtualized reality inter.pdf:application/pdf},
}

@inproceedings{hu_-line_2015,
	address = {Seattle, WA, USA},
	title = {On-line reconstruction based predictive display in unknown environment},
	doi = {10.1109/ICRA.2015.7139814},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Hu, Huan and Quintero, Camilo Perez and Sun, Hanxu and Jagersand, Martin},
	month = may,
	year = {2015},
	pages = {4446--4451},
	file = {Hu et al. - 2015 - On-line reconstruction based predictive display in.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6SCFMLPL\\Hu et al. - 2015 - On-line reconstruction based predictive display in.pdf:application/pdf},
}

@inproceedings{hofbauer_telecarla_2020,
	address = {Las Vegas, NV, USA},
	title = {{TELECARLA}: {An} {Open} {Source} {Extension} of the {CARLA} {Simulator} for {Teleoperated} {Driving} {Research} {Using} {Off}-the-{Shelf} {Components}},
	doi = {10.1109/IV47402.2020.9304676},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Hofbauer, Markus and Kuhn, Christopher B. and Petrovic, Goran and Steinbach, Eckehard},
	month = oct,
	year = {2020},
	pages = {335--340},
	file = {Hofbauer et al. - 2020 - TELECARLA An Open Source Extension of the CARLA S.pdf:C\:\\Users\\wb619\\Zotero\\storage\\TFFNEZRU\\Hofbauer et al. - 2020 - TELECARLA An Open Source Extension of the CARLA S.pdf:application/pdf},
}

@inproceedings{xie_generative_2021,
	address = {Xi'an, China},
	title = {A {Generative} {Model}-{Based} {Predictive} {Display} for {Robotic} {Teleoperation}},
	doi = {10.1109/ICRA48506.2021.9561787},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Xie, Bowen and Han, Mingjie and Jin, Jun and Barczyk, Martin and Jägersand, Martin},
	month = may,
	year = {2021},
	pages = {2407--2413},
	file = {Xie et al. - 2021 - A Generative Model-Based Predictive Display for Ro.pdf:C\:\\Users\\wb619\\Zotero\\storage\\QKPJPVY8\\Xie et al. - 2021 - A Generative Model-Based Predictive Display for Ro.pdf:application/pdf},
}

@inproceedings{marc_generator_2012,
	address = {Anchorage, AK, USA},
	title = {Generator of road marking textures and associated ground truth applied to the evaluation of road marking detection},
	doi = {10.1109/ITSC.2012.6338773},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Marc, Revilloud and Dominique, Gruyer and Evangeline, Pollard},
	month = sep,
	year = {2012},
	pages = {933--938},
	file = {Marc et al. - 2012 - Generator of road marking textures and associated .pdf:C\:\\Users\\wb619\\Zotero\\storage\\8E9GL5LL\\Marc et al. - 2012 - Generator of road marking textures and associated .pdf:application/pdf},
}

@inproceedings{rohmer_v-rep_2013,
	address = {Tokyo, Japan},
	title = {V-{REP}: {A} versatile and scalable robot simulation framework},
	doi = {10.1109/IROS.2013.6696520},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Rohmer, Eric and Singh, Surya P. N. and Freese, Marc},
	month = nov,
	year = {2013},
	pages = {1321--1326},
	file = {Rohmer et al. - 2013 - V-REP A versatile and scalable robot simulation f.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4WDHX723\\Rohmer et al. - 2013 - V-REP A versatile and scalable robot simulation f.pdf:application/pdf},
}

@article{sturm_camera_2011,
	title = {Camera {Models} and {Fundamental} {Concepts} {Used} in {Geometric} {Computer} {Vision}},
	volume = {6},
	issn = {1572-2740},
	url = {http://dx.doi.org/10.1561/0600000023},
	doi = {10.1561/0600000023},
	number = {1–2},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	author = {Sturm, Peter and Ramalingam, Srikumar and Tardif, Jean-Philippe and Gasparini, Simone and Barreto, João},
	year = {2011},
	pages = {1--183},
}

@article{tsirikoglou_survey_2020,
	title = {A {Survey} of {Image} {Synthesis} {Methods} for {Visual} {Machine} {Learning}},
	volume = {39},
	url = {https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/abs/10.1111/cgf.14047},
	doi = {https://doi-org.ezproxy.library.wisc.edu/10.1111/cgf.14047},
	abstract = {Abstract Image synthesis designed for machine learning applications provides the means to efficiently generate large quantities of training data while controlling the generation process to provide the best distribution and content variety. With the demands of deep learning applications, synthetic data have the potential of becoming a vital component in the training pipeline. Over the last decade, a wide variety of training data generation methods has been demonstrated. The potential of future development calls to bring these together for comparison and categorization. This survey provides a comprehensive list of the existing image synthesis methods for visual machine learning. These are categorized in the context of image generation, using a taxonomy based on modelling and rendering, while a classification is also made concerning the computer vision applications they are used. We focus on the computer graphics aspects of the methods, to promote future image generation for machine learning. Finally, each method is assessed in terms of quality and reported performance, providing a hint on its expected learning potential. The report serves as a comprehensive reference, targeting both groups of the applications and data development sides. A list of all methods and papers reviewed herein can be found at https://computergraphics.on.liu.se/image\_synthesis\_methods\_for\_visual\_machine\_learning/.},
	number = {6},
	journal = {Computer Graphics Forum},
	author = {Tsirikoglou, A. and Eilertsen, G. and Unger, J.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/pdfdirect/10.1111/cgf.14047},
	keywords = {methods and applications},
	pages = {426--451},
	file = {Tsirikoglou et al. - 2020 - A Survey of Image Synthesis Methods for Visual Mac.pdf:C\:\\Users\\wb619\\Zotero\\storage\\2538MVN9\\Tsirikoglou et al. - 2020 - A Survey of Image Synthesis Methods for Visual Mac.pdf:application/pdf},
}

@inproceedings{isola_image--image_2017,
	address = {Honolulu, HI, USA},
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	doi = {10.1109/CVPR.2017.632},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = jul,
	year = {2017},
	keywords = {GAN},
	pages = {5967--5976},
	file = {Isola et al. - 2017 - Image-to-Image Translation with Conditional Advers.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CTYCB6RZ\\Isola et al. - 2017 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf},
}

@inproceedings{park_contrastive_2020,
	address = {Online},
	title = {Contrastive {Learning} for {Unpaired} {Image}-to-{Image} {Translation}},
	isbn = {978-3-030-58545-7},
	doi = {10.1007/978-3-030-58545-7_19},
	abstract = {In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so – maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each “domain” is only a single image.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Park, Taesung and Efros, Alexei A. and Zhang, Richard and Zhu, Jun-Yan},
	month = aug,
	year = {2020},
	keywords = {GAN},
	pages = {319--345},
	file = {Park et al. - 2020 - Contrastive Learning for Unpaired Image-to-Image T.pdf:C\:\\Users\\wb619\\Zotero\\storage\\JCFDIG2V\\Park et al. - 2020 - Contrastive Learning for Unpaired Image-to-Image T.pdf:application/pdf},
}

@inproceedings{bi_deep_2019,
	title = {Deep CG2Real: synthetic-to-real translation via image disentanglement},
	doi = {10.1109/ICCV.2019.00282},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Bi, Sai and Sunkavalli, Kalyan and Perazzi, Federico and Shechtman, Eli and Kim, Vladimir and Ramamoorthi, Ravi},
	year = {2019},
	pages = {2730--2739},
	file = {Bi et al. - 2019 - Deep CG2Real Synthetic-to-Real Translation via Im.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4SZZAZM2\\Bi et al. - 2019 - Deep CG2Real Synthetic-to-Real Translation via Im.pdf:application/pdf},
}

@article{battiato_high_2001,
	title = {High dynamic range imaging: {Overview} and application},
	volume = {2},
	number = {2},
	journal = {ST Journal of System Research},
	author = {Battiato, Sebastiano and Castorina, Alfio},
	year = {2001},
	keywords = {exposure},
	pages = {1--18},
	file = {Battiato and Castorina - 2001 - High dynamic range imaging Overview and applicati.pdf:C\:\\Users\\wb619\\Zotero\\storage\\VARIISJG\\Battiato and Castorina - 2001 - High dynamic range imaging Overview and applicati.pdf:application/pdf},
}
