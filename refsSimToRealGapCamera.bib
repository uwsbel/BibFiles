



@inproceedings{zhang_unreasonable_2018,
	address = {Salt Lake City, UT, USA},
	title = {The {Unreasonable} {Effectiveness} of {Deep} {Features} as a {Perceptual} {Metric}},
	doi = {10.1109/CVPR.2018.00068},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
	year = {2018},
	pages = {586--595},
}

@article{wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	doi = {10.1109/TIP.2003.819861},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	year = {2004},
	pages = {600--612},
}

@inproceedings{ani_quantifying_2021,
	address = {Milan, Italy},
	title = {Quantifying the {Use} of {Domain} {Randomization}},
	doi = {10.1109/ICPR48806.2021.9412118},
	booktitle = {International {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Ani, Mohammad and Basevi, Hector and Leonardis, Aleš},
	year = {2021},
	pages = {6128--6135},
	file = {Ani et al. - 2021 - Quantifying the Use of Domain Randomization.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6HEP79SF\\Ani et al. - 2021 - Quantifying the Use of Domain Randomization.pdf:application/pdf},
}

@inproceedings{hu_sim--real_2022,
	address = {Aachen, Germany},
	title = {Sim-to-{Real} {Domain} {Adaptation} for {Lane} {Detection} and {Classification} in {Autonomous} {Driving}},
	doi = {10.1109/IV51971.2022.9827450},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Hu, Chuqing and Hudson, Sinclair and Ethier, Martin and Al-Sharman, Mohammad and Rayside, Derek and Melek, William},
	year = {2022},
	keywords = {Algorithm’s domain adaptation},
	pages = {457--463},
	file = {Hu et al. - 2022 - Sim-to-Real Domain Adaptation for Lane Detection a.pdf:C\:\\Users\\wb619\\Zotero\\storage\\28JBXVMT\\Hu et al. - 2022 - Sim-to-Real Domain Adaptation for Lane Detection a.pdf:application/pdf},
}

@inproceedings{guizilini_geometric_2021,
	address = {Montreal, QC, Canada},
	title = {Geometric {Unsupervised} {Domain} {Adaptation} for {Semantic} {Segmentation}},
	doi = {10.1109/ICCV48922.2021.00842},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Guizilini, Vitor and Li, Jie and Ambruş, Rareş and Gaidon, Adrien},
	year = {2021},
	keywords = {Algorithm’s domain adaptation},
	pages = {8517--8527},
	file = {Guizilini et al. - 2021 - Geometric Unsupervised Domain Adaptation for Seman.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MG9DNUTF\\Guizilini et al. - 2021 - Geometric Unsupervised Domain Adaptation for Seman.pdf:application/pdf},
}

@inproceedings{shyam_infra_2022,
	address = {Aachen, Germany},
	title = {Infra {Sim}-to-{Real}: {An} efficient baseline and dataset for {Infrastructure} based {Online} {Object} {Detection} and {Tracking} using {Domain} {Adaptation}},
	doi = {10.1109/IV51971.2022.9827395},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	publisher = {IEEE},
	author = {Shyam, Pranjay and Mishra, Sumit and Yoon, Kuk-Jin and Kim, Kyung-Soo},
	year = {2022},
	keywords = {Algorithm’s domain adaptation},
	pages = {1393--1399},
	file = {Shyam et al. - 2022 - Infra Sim-to-Real An efficient baseline and datas.pdf:C\:\\Users\\wb619\\Zotero\\storage\\E6IDNN76\\Shyam et al. - 2022 - Infra Sim-to-Real An efficient baseline and datas.pdf:application/pdf},
}

@inproceedings{james_sim--real_2019,
	title = {Sim-{To}-{Real} via {Sim}-{To}-{Sim}: {Data}-{Efficient} {Robotic} {Grasping} via {Randomized}-{To}-{Canonical} {Adaptation} {Networks}},
	doi = {10.1109/CVPR.2019.01291},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
	year = {2019},
	pages = {12619--12629},
	file = {James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.pdf:C\:\\Users\\wb619\\Zotero\\storage\\3MWRRLKP\\James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.pdf:application/pdf},
}

@inproceedings{movshovitz-attias_how_2016,
	address = {Cham},
	title = {How {Useful} {Is} {Photo}-{Realistic} {Rendering} for {Visual} {Learning}?},
	isbn = {978-3-319-49409-8},
	doi = {10.1007/978-3-319-49409-8_18},
	abstract = {Data seems cheap to get, and in many ways it is, but the process of creating a high quality labeled dataset from a mass of data is time-consuming and expensive.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV}) {Workshops}},
	publisher = {Springer International Publishing},
	author = {Movshovitz-Attias, Yair and Kanade, Takeo and Sheikh, Yaser},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	pages = {202--217},
	file = {Movshovitz-Attias et al. - 2016 - How Useful Is Photo-Realistic Rendering for Visual.pdf:C\:\\Users\\wb619\\Zotero\\storage\\XLQKRPNL\\Movshovitz-Attias et al. - 2016 - How Useful Is Photo-Realistic Rendering for Visual.pdf:application/pdf},
}

@inproceedings{carlson_modeling_2018,
	title = {Modeling {Camera} {Effects} to {Improve} {Visual} {Learning} from {Synthetic} {Data}},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV}) {Workshops}},
	author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
	month = sep,
	year = {2018},
	file = {Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:C\:\\Users\\wb619\\Zotero\\storage\\ZCA74XHE\\Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:application/pdf},
}

@article{carlson_sensor_2019,
	title = {Sensor Transfer: Learning Optimal Sensor Effect Image Augmentation for Sim-to-Real Domain Adaptation},
	volume = {4},
	doi = {10.1109/LRA.2019.2896470},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
	year = {2019},
	pages = {2431--2438},
	file = {Carlson et al. - 2019 - Sensor Transfer Learning Optimal Sensor Effect Im.pdf:C\:\\Users\\wb619\\Zotero\\storage\\SI5QAAKF\\Carlson et al. - 2019 - Sensor Transfer Learning Optimal Sensor Effect Im.pdf:application/pdf},
}

@article{schellenberg_photoacoustic_2022,
	title = {Photoacoustic image synthesis with generative adversarial networks},
	volume = {28},
	issn = {2213-5979},
	url = {https://www.sciencedirect.com/science/article/pii/S2213597922000672},
	doi = {10.1016/j.pacs.2022.100402},
	abstract = {Photoacoustic tomography (PAT) has the potential to recover morphological and functional tissue properties with high spatial resolution. However, previous attempts to solve the optical inverse problem with supervised machine learning were hampered by the absence of labeled reference data. While this bottleneck has been tackled by simulating training data, the domain gap between real and simulated images remains an unsolved challenge. We propose a novel approach to PAT image synthesis that involves subdividing the challenge of generating plausible simulations into two disjoint problems: (1) Probabilistic generation of realistic tissue morphology, and (2) pixel-wise assignment of corresponding optical and acoustic properties. The former is achieved with Generative Adversarial Networks (GANs) trained on semantically annotated medical imaging data. According to a validation study on a downstream task our approach yields more realistic synthetic images than the traditional model-based approach and could therefore become a fundamental step for deep learning-based quantitative PAT (qPAT).},
	journal = {Photoacoustics},
	author = {Schellenberg, Melanie and Gröhl, Janek and Dreher, Kris K. and Nölke, Jan-Hinrich and Holzwarth, Niklas and Tizabi, Minu D. and Seitel, Alexander and Maier-Hein, Lena},
	year = {2022},
	keywords = {Deep learning, Generative adversarial networks, Optoacoustic imaging, Optoacoustic tomography, Photoacoustic imaging, Photoacoustic tomography, Synthetic data},
	pages = {100402},
	file = {Schellenberg et al. - 2022 - Photoacoustic image synthesis with generative adve.pdf:C\:\\Users\\wb619\\Zotero\\storage\\AQQQNF9N\\Schellenberg et al. - 2022 - Photoacoustic image synthesis with generative adve.pdf:application/pdf},
}

@inproceedings{alghonaim_benchmarking_2021,
	title = {Benchmarking {Domain} {Randomisation} for {Visual} {Sim}-to-{Real} {Transfer}},
	doi = {10.1109/ICRA48506.2021.9561134},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Alghonaim, Raghad and Johns, Edward},
	year = {2021},
	pages = {12802--12808},
	file = {Alghonaim and Johns - 2021 - Benchmarking Domain Randomisation for Visual Sim-t.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CC2VFCXA\\Alghonaim and Johns - 2021 - Benchmarking Domain Randomisation for Visual Sim-t.pdf:application/pdf},
}

@inproceedings{zakharov_photo-realistic_2022,
	address = {Cham},
	title = {Photo-realistic {Neural} {Domain} {Randomization}},
	isbn = {978-3-031-19806-9},
	abstract = {Synthetic data is a scalable alternative to manual supervision, but it requires overcoming the sim-to-real domain gap. This discrepancy between virtual and real worlds is addressed by two seemingly opposed approaches: improving the realism of simulation or foregoing realism entirely via domain randomization. In this paper, we show that the recent progress in neural rendering enables a new unified approach we call Photo-realistic Neural Domain Randomization (PNDR). We propose to learn a composition of neural networks that acts as a physics-based ray tracer generating high-quality renderings from scene geometry alone. Our approach is modular, composed of different neural networks for materials, lighting, and rendering, thus enabling randomization of different key image generation components in a differentiable pipeline. Once trained, our method can be combined with other methods and used to generate photo-realistic image augmentations online and significantly more efficiently than via traditional ray-tracing. We demonstrate the usefulness of PNDR through two downstream tasks: 6D object detection and monocular depth estimation. Our experiments show that training with PNDR enables generalization to novel scenes and significantly outperforms the state of the art in terms of real-world transfer.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer Nature Switzerland},
	author = {Zakharov, Sergey and Ambruș, Rareș and Guizilini, Vitor and Kehl, Wadim and Gaidon, Adrien},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {310--327},
	file = {Zakharov et al. - 2022 - Photo-realistic Neural Domain Randomization.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8F2SYL4R\\Zakharov et al. - 2022 - Photo-realistic Neural Domain Randomization.pdf:application/pdf},
}

@inproceedings{bousmalis_using_2018,
	title = {Using {Simulation} and {Domain} {Adaptation} to {Improve} {Efficiency} of {Deep} {Robotic} {Grasping}},
	doi = {10.1109/ICRA.2018.8460875},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
	year = {2018},
	keywords = {Domain adaptation},
	pages = {4243--4250},
	file = {Bousmalis et al. - 2018 - Using Simulation and Domain Adaptation to Improve .pdf:C\:\\Users\\wb619\\Zotero\\storage\\J7H55PWN\\Bousmalis et al. - 2018 - Using Simulation and Domain Adaptation to Improve .pdf:application/pdf},
}

@inproceedings{bousmalis_unsupervised_2017,
	title = {Unsupervised pixel-level domain adaptation with generative adversarial networks},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
	month = jul,
	year = {2017},
	keywords = {Improve - GAN},
	file = {Bousmalis et al. - 2017 - Unsupervised pixel-level domain adaptation with ge.pdf:C\:\\Users\\wb619\\Zotero\\storage\\RKPH8SAB\\Bousmalis et al. - 2017 - Unsupervised pixel-level domain adaptation with ge.pdf:application/pdf},
}

@article{andrychowicz_learning_2020,
	title = {Learning dexterous in-hand manipulation},
	volume = {39},
	url = {https://doi.org/10.1177/0278364919887447},
	doi = {10.1177/0278364919887447},
	abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.},
	number = {1},
	journal = {International Journal of Robotics Research},
	author = {Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Józefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
	year = {2020},
	note = {\_eprint: https://doi.org/10.1177/0278364919887447},
	keywords = {Domain randomization},
	pages = {3--20},
	file = {Andrychowicz et al. - 2020 - Learning dexterous in-hand manipulation.pdf:C\:\\Users\\wb619\\Zotero\\storage\\DZBYQU63\\Andrychowicz et al. - 2020 - Learning dexterous in-hand manipulation.pdf:application/pdf},
}

@inproceedings{domainRandomization2017,
	title = {Domain randomization for transferring deep neural networks from simulation to the real world},
	doi = {10.1109/IROS.2017.8202133},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	year = {2017},
	keywords = {Domain randomization},
	pages = {23--30},
	file = {Tobin et al. - 2017 - Domain randomization for transferring deep neural .pdf:C\:\\Users\\wb619\\Zotero\\storage\\8W3CFVJP\\Tobin et al. - 2017 - Domain randomization for transferring deep neural .pdf:application/pdf},
}

@article{liu_neural_2020,
	title = {Neural {Network} {Generalization}: {The} {Impact} of {Camera} {Parameters}},
	volume = {8},
	doi = {10.1109/ACCESS.2020.2965089},
	journal = {IEEE Access},
	author = {Liu, Zhenyi and Lian, Trisha and Farrell, Joyce and Wandell, Brian A.},
	year = {2020},
	pages = {10443--10454},
	file = {Liu et al. - 2020 - Neural Network Generalization The Impact of Camer.pdf:C\:\\Users\\wb619\\Zotero\\storage\\IWRMTYV6\\Liu et al. - 2020 - Neural Network Generalization The Impact of Camer.pdf:application/pdf},
}

@article{goossens_ray-transfer_2022,
	title = {Ray-transfer functions for camera simulation of {3D} scenes with hidden lens design},
	volume = {30},
	url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-30-13-24031},
	doi = {10.1364/OE.457496},
	abstract = {Combining image sensor simulation tools with physically based ray tracing enables the design and evaluation (soft prototyping) of novel imaging systems. These methods can also synthesize physically accurate, labeled images for machine learning applications. One practical limitation of soft prototyping has been simulating the optics precisely: lens manufacturers generally prefer to keep lens design confidential. We present a pragmatic solution to this problem using a black box lens model in Zemax; such models provide necessary optical information while preserving the lens designer's intellectual property. First, we describe and provide software to construct a polynomial ray transfer function that characterizes how rays entering the lens at any position and angle subsequently exit the lens. We implement the ray-transfer calculation as a camera model in PBRT and confirm that the PBRT ray-transfer calculations match the Zemax lens calculations for edge spread functions and relative illumination.},
	number = {13},
	journal = {Optics Express},
	author = {Goossens, Thomas and Lyu, Zheng and Ko, Jamyuen and Wan, Gordon C. and Farrell, Joyce and Wandell, Brian},
	year = {2022},
	note = {Publisher: Optica Publishing Group},
	keywords = {Chromatic aberrations, Image sensors, Imaging systems, Lens design, Position sensing equipment, Ray tracing, Color metric},
	pages = {24031--24047},
	file = {Goossens et al. - 2022 - Ray-transfer functions for camera simulation of 3D.pdf:C\:\\Users\\wb619\\Zotero\\storage\\VFJBSIJL\\Goossens et al. - 2022 - Ray-transfer functions for camera simulation of 3D.pdf:application/pdf},
}

@inproceedings{stein_genesis-rt_2018,
	address = {Brisbane, Australia},
	title = {{GeneSIS}-{RT}: {Generating} {Synthetic} {Images} for {Training} {Secondary} {Real}-{World} {Tasks}},
	doi = {10.1109/ICRA.2018.8462971},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Stein, Gregory J. and Roy, Nicholas},
	year = {2018},
	keywords = {Improve - GAN},
	pages = {7151--7158},
	file = {Stein and Roy - 2018 - GeneSIS-RT Generating Synthetic Images for Traini.pdf:C\:\\Users\\wb619\\Zotero\\storage\\VFX2ZUED\\Stein and Roy - 2018 - GeneSIS-RT Generating Synthetic Images for Traini.pdf:application/pdf},
}

@article{hapke_bidirectional_2008,
	title = {Bidirectional reflectance spectroscopy: 6. {Effects} of porosity},
	author = {Hapke, Bruce},
	volume = {195},
	issn = {0019-1035},
	doi = {10.1016/j.icarus.2008.01.003},
	abstract = {It is well known that the bidirectional reflectance of a particulate medium such as a planetary regolith depends on the porosity, in contrast to predictions of models based on the equation of radiative transfer as usually formulated. It is shown that this failure to predict porosity dependence arises from an incorrect treatment of the light that passes between the particles. In this paper a more physically correct treatment that takes account of the necessity of preventing particles from interpenetrating is used together with the two-stream approximation to solve the radiative transfer equation and derive improved expressions for the bidirectional and directional-hemispherical reflectances. It is found that increasing the filling factor (decreasing the porosity) increases the reflectance of low and medium albedo powders, but decreases it for ones with very high albedos. The model agrees qualitatively with measured data.},
	number = {2},
	journal = {Icarus},
	year = {2008},
	keywords = {Radiative transfer, Regoliths, Spectrophotometry},
	pages = {918--926},
	file = {Hapke - 2008 - Bidirectional reflectance spectroscopy 6. Effects.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MQ488AW6\\Hapke - 2008 - Bidirectional reflectance spectroscopy 6. Effects.pdf:application/pdf},
}


@inproceedings{munkberg_extracting_2022,
	address = {New Orleans, USA},
	title = {Extracting {Triangular} {3D} {Models}, {Materials}, and {Lighting} from {Images}},
	doi = {10.1109/CVPR52688.2022.00810},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Munkberg, Jacob and Chen, Wenzheng and Hasselgren, Jon and Evans, Alex and Shen, Tianchang and Müller, Thomas and Gao, Jun and Fidler, Sanja},
	year = {2022},
	keywords = {Neural renderer},
	pages = {8270--8280},
	file = {Munkberg et al. - 2022 - Extracting Triangular 3D Models, Materials, and Li.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CL396VUP\\Munkberg et al. - 2022 - Extracting Triangular 3D Models, Materials, and Li.pdf:application/pdf;NVDiffRec_supple.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LN4NRVUN\\NVDiffRec_supple.pdf:application/pdf},
}

@inproceedings{zhang_physg_2021,
	address = {Nashville, USA},
	title = {{PhySG}: {Inverse} {Rendering} with {Spherical} {Gaussians} for {Physics}-based {Material} {Editing} and {Relighting}},
	doi = {10.1109/CVPR46437.2021.00541},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Kai and Luan, Fujun and Wang, Qianqian and Bala, Kavita and Snavely, Noah},
	year = {2021},
	keywords = {Neural renderer},
	pages = {5449--5458},
	file = {Zhang et al. - 2021 - PhySG Inverse Rendering with Spherical Gaussians .pdf:C\:\\Users\\wb619\\Zotero\\storage\\KBMN7GL7\\Zhang et al. - 2021 - PhySG Inverse Rendering with Spherical Gaussians .pdf:application/pdf},
}

@inproceedings{mildenhall_nerf_2020,
	address = {Online},
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	isbn = {978-3-030-58451-1},
	doi = {10.1007/978-3-030-58452-8_24},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,\&nbsp;y,\&nbsp;z) and viewing direction ) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	year = {2020},
	keywords = {3D deep learning, Image-based rendering, Scene representation, View synthesis, Volume rendering, Neural renderer},
	pages = {405--421},
	file = {Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:C\:\\Users\\wb619\\Zotero\\storage\\TIFYGBGD\\Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf},
}

@article{muller_instant_2022,
	title = {Instant {Neural} {Graphics} {Primitives} with a {Multiresolution} {Hash} {Encoding}},
	volume = {41},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	year = {2022},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {encodings, function approximation, GPUs, hashing, image synthesis, neural networks, parallel computation},
	pages = {1--15},
	file = {InstantNGP.pdf:C\:\\Users\\wb619\\Zotero\\storage\\BE9969KK\\InstantNGP.pdf:application/pdf;Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:C\:\\Users\\wb619\\Zotero\\storage\\2I4EWIC7\\Müller et al. - 2022 - Instant Neural Graphics Primitives with a Multires.pdf:application/pdf},
}

@inproceedings{boss_nerd_2021,
	address = {Montreal, Canada},
	title = {{NeRD}: {Neural} {Reflectance} {Decomposition} from {Image} {Collections}},
	doi = {10.1109/ICCV48922.2021.01245},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Boss, Mark and Braun, Raphael and Jampani, Varun and Barron, Jonathan T. and Liu, Ce and Lensch, Hendrik P.A.},
	year = {2021},
	keywords = {Neural renderer},
	pages = {12664--12674},
	file = {Boss et al. - 2021 - NeRD Neural Reflectance Decomposition from Image .pdf:C\:\\Users\\wb619\\Zotero\\storage\\X7L2WYWT\\Boss et al. - 2021 - NeRD Neural Reflectance Decomposition from Image .pdf:application/pdf},
}

@article{zhang_nerfactor_2021,
	title = {{NeRFactor}: {Neural} {Factorization} of {Shape} and {Reflectance} under an {Unknown} {Illumination}},
	volume = {40},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3478513.3480496},
	doi = {10.1145/3478513.3480496},
	abstract = {We address the problem of recovering the shape and spatially-varying reflectance of an object from multi-view images (and their camera poses) of an object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our videos, code, and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.},
	number = {6},
	journal = {ACM Transactions on Graphics},
	author = {Zhang, Xiuming and Srinivasan, Pratul P. and Deng, Boyang and Debevec, Paul and Freeman, William T. and Barron, Jonathan T.},
	month = dec,
	year = {2021},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {appearance factorization, inverse rendering, lighting estimation, material editing, reflectance estimation, relighting, shape estimation, view synthesis, Neural renderer},
	pages = {1--18},
	file = {Zhang et al. - 2021 - NeRFactor Neural Factorization of Shape and Refle.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8HVLWKRR\\Zhang et al. - 2021 - NeRFactor Neural Factorization of Shape and Refle.pdf:application/pdf},
}

@inproceedings{carlson_modeling_2018-1,
	title = {Modeling {Camera} {Effects} to {Improve} {Visual} {Learning} from {Synthetic} {Data}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1803.07721},
	doi = {10.48550/ARXIV.1803.07721},
	publisher = {arXiv},
	author = {Carlson, Alexandra and Skinner, Katherine A. and Vasudevan, Ram and Johnson-Roberson, Matthew},
	year = {2018},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
	file = {Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5V3SRTLT\\Carlson et al. - 2018 - Modeling Camera Effects to Improve Visual Learning.pdf:application/pdf},
}

@inproceedings{barron_mip-nerf_2021,
	address = {Montreal, Canada},
	title = {Mip-{NeRF}: {A} {Multiscale} {Representation} for {Anti}-{Aliasing} {Neural} {Radiance} {Fields}},
	doi = {10.1109/ICCV48922.2021.00580},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	year = {2021},
	pages = {5835--5844},
	file = {Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf:C\:\\Users\\wb619\\Zotero\\storage\\QKWX5RDP\\Barron et al. - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf:application/pdf},
}

@inproceedings{wang_neus_2021,
	title = {{NeuS}: {Learning} {Neural} {Implicit} {Surfaces} by {Volume} {Rendering} for {Multi}-view {Reconstruction}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/e41e164f7485ec4a28741a2d0ea41c74-Paper.pdf},
	booktitle = {Neural {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {27171--27183},
	file = {Wang et al. - 2021 - NeuS Learning Neural Implicit Surfaces by Volume .pdf:C\:\\Users\\wb619\\Zotero\\storage\\JIGT56RR\\Wang et al. - 2021 - NeuS Learning Neural Implicit Surfaces by Volume .pdf:application/pdf},
}

@article{denninger_blenderproc_2019,
	title = {{BlenderProc}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/arXiv.1911.01911},
	journal = {arXiv},
	author = {Denninger, Maximilian and Sundermeyer, Martin and Winkelbauer, Dominik and Zidan, Youssef and Olefir, Dmitry and Elbadrawy, Mohamad and Lodhi, Ahsan and Katam, Harinandan},
	year = {2019},
	doi = {10.48550/ARXIV.1911.01911},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Robotics (cs.RO), Graphics (cs.GR)},
	file = {Denninger et al. - 2019 - BlenderProc.pdf:C\:\\Users\\wb619\\Zotero\\storage\\GJGFEM9V\\Denninger et al. - 2019 - BlenderProc.pdf:application/pdf},
}

@inproceedings{hodan_photorealistic_2019,
	address = {Taipei, Taiwan},
	title = {Photorealistic {Image} {Synthesis} for {Object} {Instance} {Detection}},
	doi = {10.1109/ICIP.2019.8803821},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Hodaň, Tomáš and Vineet, Vibhav and Gal, Ran and Shalev, Emanuel and Hanzelka, Jon and Connell, Treb and Urbina, Pedro and Sinha, Sudipta N. and Guenter, Brian},
	month = sep,
	year = {2019},
	pages = {66--70},
	file = {Hodaň et al. - 2019 - Photorealistic Image Synthesis for Object Instance.pdf:C\:\\Users\\wb619\\Zotero\\storage\\C3JQF52Z\\Hodaň et al. - 2019 - Photorealistic Image Synthesis for Object Instance.pdf:application/pdf},
}

@inproceedings{chen_dib-r_2021,
	title = {{DIB}-{R}++: {Learning} to {Predict} {Lighting} and {Material} with a {Hybrid} {Differentiable} {Renderer}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/c0f971d8cd24364f2029fcb9ac7b71f5-Paper.pdf},
	booktitle = {Neural {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Wenzheng and Litalien, Joey and Gao, Jun and Wang, Zian and Fuji Tsang, Clement and Khamis, Sameh and Litany, Or and Fidler, Sanja},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {22834--22848},
}

@article{li_learning_2018,
	title = {Learning to {Reconstruct} {Shape} and {Spatially}-{Varying} {Reflectance} from a {Single} {Image}},
	volume = {37},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3272127.3275055},
	doi = {10.1145/3272127.3275055},
	abstract = {Reconstructing shape and reflectance properties from images is a highly under-constrained problem, and has previously been addressed by using specialized hardware to capture calibrated data or by assuming known (or highly constrained) shape or reflectance. In contrast, we demonstrate that we can recover non-Lambertian, spatially-varying BRDFs and complex geometry belonging to any arbitrary shape class, from a single RGB image captured under a combination of unknown environment illumination and flash lighting. We achieve this by training a deep neural network to regress shape and reflectance from the image. Our network is able to address this problem because of three novel contributions: first, we build a large-scale dataset of procedurally generated shapes and real-world complex SVBRDFs that approximate real world appearance well. Second, single image inverse rendering requires reasoning at multiple scales, and we propose a cascade network structure that allows this in a tractable manner. Finally, we incorporate an in-network rendering layer that aids the reconstruction task by handling global illumination effects that are important for real-world scenes. Together, these contributions allow us to tackle the entire inverse rendering problem in a holistic manner and produce state-of-the-art results on both synthetic and real data.},
	number = {6},
	journal = {ACM Transactions on Graphics},
	author = {Li, Zhengqin and Xu, Zexiang and Ramamoorthi, Ravi and Sunkavalli, Kalyan and Chandraker, Manmohan},
	month = dec,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {cascade network, deep learning, flash light, global illumination, rendering layer, single image, SVBRDF},
	pages = {1--11},
	file = {Li et al. - 2018 - Learning to Reconstruct Shape and Spatially-Varyin.pdf:C\:\\Users\\wb619\\Zotero\\storage\\ZFPRB9A2\\Li et al. - 2018 - Learning to Reconstruct Shape and Spatially-Varyin.pdf:application/pdf},
}

@article{guarnera_brdf_2016,
	title = {{BRDF} {Representation} and {Acquisition}},
	volume = {35},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12867},
	doi = {10.1111/cgf.12867},
	abstract = {Abstract Photorealistic rendering of real world environments is important in a range of different areas; including Visual Special effects, Interior/Exterior Modelling, Architectural Modelling, Cultural Heritage, Computer Games and Automotive Design. Currently, rendering systems are able to produce photorealistic simulations of the appearance of many real-world materials. In the real world, viewer perception of objects depends on the lighting and object/material/surface characteristics, the way a surface interacts with the light and on how the light is reflected, scattered, absorbed by the surface and the impact these characteristics have on material appearance. In order to re-produce this, it is necessary to understand how materials interact with light. Thus the representation and acquisition of material models has become such an active research area. This survey of the state-of-the-art of BRDF Representation and Acquisition presents an overview of BRDF (Bidirectional Reflectance Distribution Function) models used to represent surface/material reflection characteristics, and describes current acquisition methods for the capture and rendering of photorealistic materials.},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Guarnera, D. and Guarnera, G.C. and Ghosh, A. and Denk, C. and Glencross, M.},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12867},
	keywords = {and texture, Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation, I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Colour, I.6.8 Computer Graphics: Types of simulation—Monte Carlo, shading, shadowing},
	pages = {625--650},
	file = {Guarnera et al. - 2016 - BRDF Representation and Acquisition.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4QSWRQBX\\Guarnera et al. - 2016 - BRDF Representation and Acquisition.pdf:application/pdf},
}

@article{schlick_inexpensive_1994,
	title = {An {Inexpensive} {BRDF} {Model} for {Physically}-based {Rendering}},
	volume = {13},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.1330233},
	doi = {10.1111/1467-8659.1330233},
	abstract = {Abstract: A new BRDF model is presented which can be viewed as an kind of intermediary model between empirism and theory. Main results of physics are observed (energy conservation, reciprocity rule, microfacet theory) and numerous phenomena involved in light reflection are accounted for, in a physically plausible way (incoherent and coherent reflection, spectrum modifications, anisotropy, self-shadowing, multiple surface and subsurface reflection, differences between homogeneous and heterogeneous materials). The model has been especially intended for computer graphics applications and therefore includes two main features: simplicity (a small number of intuitively understandable parameters controls the model) and efficiency (the formulation provides adequation to Monte-Carlo rendering techniques and/or hardware implementations).},
	number = {3},
	journal = {Computer Graphics Forum},
	author = {Schlick, Christophe},
	year = {1994},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-8659.1330233},
	keywords = {Bidirectional Reflectance Distribution Function, Optimization, Physically-Based Rendering},
	pages = {233--246},
	file = {Schlick - 1994 - An Inexpensive BRDF Model for Physically-based Ren.pdf:C\:\\Users\\wb619\\Zotero\\storage\\W5655TK5\\Schlick - 1994 - An Inexpensive BRDF Model for Physically-based Ren.pdf:application/pdf},
}

@article{lee_real-time_2010,
	title = {Real-{Time} {Lens} {Blur} {Effects} and {Focus} {Control}},
	volume = {29},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1778765.1778802},
	doi = {10.1145/1778765.1778802},
	abstract = {We present a novel rendering system for defocus blur and lens effects. It supports physically-based rendering and outperforms previous approaches by involving a novel GPU-based tracing method. Our solution achieves more precision than competing real-time solutions and our results are mostly indistinguishable from offline rendering. Our method is also more general and can integrate advanced simulations, such as simple geometric lens models enabling various lens aberration effects. These latter is crucial for realism, but are often employed in artistic contexts, too. We show that available artistic lenses can be simulated by our method. In this spirit, our work introduces an intuitive control over depth-of-field effects. The physical basis is crucial as a starting point to enable new artistic renderings based on a generalized focal surface to emphasize particular elements in the scene while retaining a realistic look. Our real-time solution provides realistic, as well as plausible expressive results.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Lee, Sungkil and Eisemann, Elmar and Seidel, Hans-Peter},
	year = {2010},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	file = {Lee et al. - 2010 - Real-Time Lens Blur Effects and Focus Control.pdf:C\:\\Users\\wb619\\Zotero\\storage\\3Z6PMGWL\\Lee et al. - 2010 - Real-Time Lens Blur Effects and Focus Control.pdf:application/pdf},
}

@inproceedings{kang_automatic_2007,
	address = {Minneapolis, USA},
	title = {Automatic {Removal} of {Chromatic} {Aberration} from a {Single} {Image}},
	doi = {10.1109/CVPR.2007.383214},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kang, Sing Bing},
	year = {2007},
	pages = {1--8},
	file = {Kang - 2007 - Automatic Removal of Chromatic Aberration from a S.pdf:C\:\\Users\\wb619\\Zotero\\storage\\YD7GIMZT\\Kang - 2007 - Automatic Removal of Chromatic Aberration from a S.pdf:application/pdf},
}

@inproceedings{karaimer_software_2016,
	address = {Amsterdam, The Netherlands},
	title = {A {Software} {Platform} for {Manipulating} the {Camera} {Imaging} {Pipeline}},
	isbn = {978-3-319-46448-0},
	doi = {10.1007/978-3-319-46448-0_26},
	abstract = {There are a number of processing steps applied onboard a digital camera that collectively make up the camera imaging pipeline. Unfortunately, the imaging pipeline is typically embedded in a camera's hardware making it difficult for researchers working on individual components to do so within the proper context of the full pipeline. This not only hinders research, it makes evaluating the effects from modifying an individual pipeline component on the final camera output challenging, if not impossible. This paper presents a new software platform that allows easy access to each stage of the camera imaging pipeline. The platform allows modification of the parameters for individual components as well as the ability to access and manipulate the intermediate images as they pass through different stages. We detail our platform design and demonstrate its usefulness on a number of examples.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Karaimer, Hakki Can and Brown, Michael S.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {429--444},
	file = {Full Text:C\:\\Users\\wb619\\Zotero\\storage\\FQ5LLF7E\\Karaimer and Brown - 2016 - A Software Platform for Manipulating the Camera Im.pdf:application/pdf},
}

@article{bhukhanwala_automated_1994,
	title = {Automated global enhancement of digitized photographs},
	volume = {40},
	doi = {10.1109/30.273657},
	number = {1},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Bhukhanwala, Saumil A. and Ramabadran, Tenkasi V.},
	year = {1994},
	pages = {1--10},
	file = {Bhukhanwala and Ramabadran - 1994 - Automated global enhancement of digitized photogra.pdf:C\:\\Users\\wb619\\Zotero\\storage\\RFZFA2MG\\Bhukhanwala and Ramabadran - 1994 - Automated global enhancement of digitized photogra.pdf:application/pdf},
}

@inproceedings{messina_image_2003,
	address = {Baltimore, USA},
	title = {Image quality improvement by adaptive exposure correction techniques},
	volume = {1},
	doi = {10.1109/ICME.2003.1220976},
	booktitle = {{IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	author = {Messina, Giuseppe and Castorina, Alfio and Battiato, Sebastiano and Bosco, Angelo},
	year = {2003},
	pages = {549--552},
	file = {Messina et al. - 2003 - Image quality improvement by adaptive exposure cor.pdf:C\:\\Users\\wb619\\Zotero\\storage\\9KGLBUVB\\Messina et al. - 2003 - Image quality improvement by adaptive exposure cor.pdf:application/pdf},
}

@article{foi_practical_2008,
	title = {Practical {Poissonian}-{Gaussian} {Noise} {Modeling} and {Fitting} for {Single}-{Image} {Raw}-{Data}},
	volume = {17},
	doi = {10.1109/TIP.2008.2001399},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {Foi, Alessandro and Trimeche, Mejdi and Katkovnik, Vladimir and Egiazarian, Karen},
	year = {2008},
	pages = {1737--1754},
	file = {Foi et al. - 2008 - Practical Poissonian-Gaussian Noise Modeling and F.pdf:C\:\\Users\\wb619\\Zotero\\storage\\GRM3SYA3\\Foi et al. - 2008 - Practical Poissonian-Gaussian Noise Modeling and F.pdf:application/pdf},
}

@book{pharr_physically_2016,
	edition = {3rd},
	title = {Physically based rendering: {From} theory to implementation},
	publisher = {Morgan Kaufmann},
	author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
	year = {2016},
}

@article{santana-cedres_estimation_2017,
	title = {Estimation of the {Lens} {Distortion} {Model} by {Minimizing} a {Line} {Reprojection} {Error}},
	volume = {17},
	doi = {10.1109/JSEN.2017.2677475},
	number = {9},
	journal = {IEEE Sensors Journal},
	author = {Santana-Cedrés, Daniel and Gomez, Luis and Alemán-Flores, Miguel and Salgado, Agustín and Esclarín, Julio and Mazorra, Luis and Alvarez, Luis},
	year = {2017},
	pages = {2848--2855},
	file = {Santana-Cedrés et al. - 2017 - Estimation of the Lens Distortion Model by Minimiz.pdf:C\:\\Users\\wb619\\Zotero\\storage\\HW64YVFV\\Santana-Cedrés et al. - 2017 - Estimation of the Lens Distortion Model by Minimiz.pdf:application/pdf},
}

@inproceedings{chen_no_2017,
	address = {Venice, Italy},
	title = {No {More} {Discrimination}: {Cross} {City} {Adaptation} of {Road} {Scene} {Segmenters}},
	doi = {10.1109/ICCV.2017.220},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Yi-Hsin and Chen, Wei-Yu and Chen, Yu-Ting and Tsai, Bo-Cheng and Wang, Yu-Chiang Frank and Sun, Min},
	year = {2017},
	pages = {2011--2020},
	file = {Chen et al. - 2017 - No More Discrimination Cross City Adaptation of R.pdf:C\:\\Users\\wb619\\Zotero\\storage\\FEK9KQ7G\\Chen et al. - 2017 - No More Discrimination Cross City Adaptation of R.pdf:application/pdf},
}

@article{tang_precision_2017,
	title = {A {Precision} {Analysis} of {Camera} {Distortion} {Models}},
	volume = {26},
	doi = {10.1109/TIP.2017.2686001},
	number = {6},
	journal = {IEEE Transactions on Image Processing},
	author = {Tang, Zhongwei and Grompone von Gioi, Rafael and Monasse, Pascal and Morel, Jean-Michel},
	year = {2017},
	pages = {2694--2704},
	file = {Tang et al. - 2017 - A Precision Analysis of Camera Distortion Models.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MR4NYTQM\\Tang et al. - 2017 - A Precision Analysis of Camera Distortion Models.pdf:application/pdf},
}

@article{yu_practical_2004,
	title = {Practical anti-vignetting methods for digital cameras},
	volume = {50},
	doi = {10.1109/TCE.2004.1362487},
	number = {4},
	journal = {IEEE Transactions on Consumer Electronics},
	author = {Yu, Wonpil},
	year = {2004},
	pages = {975--983},
	file = {Yu - 2004 - Practical anti-vignetting methods for digital came.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LP4IIZHS\\Yu - 2004 - Practical anti-vignetting methods for digital came.pdf:application/pdf},
}

@article{zheng_single-image_2009,
	title = {Single-{Image} {Vignetting} {Correction}},
	volume = {31},
	doi = {10.1109/TPAMI.2008.263},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zheng, Yuanjie and Lin, Stephen and Kambhamettu, Chandra and Yu, Jingyi and Kang, Sing Bing},
	year = {2009},
	pages = {2243--2256},
	file = {Zheng et al. - 2009 - Single-Image Vignetting Correction.pdf:C\:\\Users\\wb619\\Zotero\\storage\\MELBC9LI\\Zheng et al. - 2009 - Single-Image Vignetting Correction.pdf:application/pdf},
}

@inproceedings{hullin_physically-based_2011,
	address = {New York, USA},
	series = {{SIGGRAPH} '11},
	title = {Physically-{Based} {Real}-{Time} {Lens} {Flare} {Rendering}},
	isbn = {978-1-4503-0943-1},
	url = {https://doi-org.ezproxy.library.wisc.edu/10.1145/1964921.1965003},
	doi = {10.1145/1964921.1965003},
	abstract = {Lens flare is caused by light passing through a photographic lens system in an unintended way. Often considered a degrading artifact, it has become a crucial component for realistic imagery and an artistic means that can even lead to an increased perceived brightness. So far, only costly offline processes allowed for convincing simulations of the complex light interactions. In this paper, we present a novel method to interactively compute physically-plausible flare renderings for photographic lenses. The underlying model covers many components that are important for realism, such as imperfections, chromatic and geometric lens aberrations, and antireflective lens coatings. Various acceleration strategies allow for a performance/quality tradeoff, making our technique applicable both in real-time applications and in high-quality production rendering. We further outline artistic extensions to our system.},
	booktitle = {{ACM} {SIGGRAPH}},
	publisher = {Association for Computing Machinery},
	author = {Hullin, Matthias and Eisemann, Elmar and Seidel, Hans-Peter and Lee, Sungkil},
	year = {2011},
	note = {event-place: Vancouver, British Columbia, Canada},
	keywords = {lens flare, real-time rendering},
	file = {Hullin et al. - 2011 - Physically-Based Real-Time Lens Flare Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5N5MR8WQ\\Hullin et al. - 2011 - Physically-Based Real-Time Lens Flare Rendering.pdf:application/pdf},
}

@article{lee_practical_2013,
	title = {Practical {Real}-{Time} {Lens}-{Flare} {Rendering}},
	volume = {32},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12145},
	doi = {https://doi.org/10.1111/cgf.12145},
	abstract = {Abstract We present a practical real-time approach for rendering lens-flare effects. While previous work employed costly ray tracing or complex polynomial expressions, we present a coarser, but also significantly faster solution. Our method is based on a first-order approximation of the ray transfer in an optical system, which allows us to derive a matrix that maps lens flare-producing light rays directly to the sensor. The resulting approach is easy to implement and produces physically-plausible images at high framerates on standard off-the-shelf graphics hardware.},
	number = {4},
	journal = {Computer Graphics Forum},
	author = {Lee, Sungkil and Eisemann, Elmar},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12145},
	keywords = {I.3.3 Computer Graphics: Picture/Image Generation—Display algorithms},
	pages = {1--6},
	file = {Lee and Eisemann - 2013 - Practical Real-Time Lens-Flare Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CXSFQLWV\\Lee and Eisemann - 2013 - Practical Real-Time Lens-Flare Rendering.pdf:application/pdf},
}

@inproceedings{ouyang_neural_2021,
	address = {Nashville, USA},
	title = {Neural {Camera} {Simulators}},
	doi = {10.1109/CVPR46437.2021.00761},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ouyang, Hao and Shi, Zifan and Lei, Chenyang and Lung Law, Ka and Chen, Qifeng},
	year = {2021},
	pages = {7696--7705},
	file = {Ouyang et al. - 2021 - Neural Camera Simulators.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CUGNPBBB\\Ouyang et al. - 2021 - Neural Camera Simulators.pdf:application/pdf},
}

@inproceedings{li_igibson_2021,
	address = {London, UK},
	series = {Proceedings of {Machine} {Learning} {Research} ({PMLR})},
	title = {{iGibson} 2.0: {Object}-{Centric} {Simulation} for {Robot} {Learning} of {Everyday} {Household} {Tasks}},
	volume = {164},
	url = {https://proceedings.mlr.press/v164/li22b.html},
	abstract = {Recent research in embodied AI has been boosted by the use of simulation environments to develop and train robot learning approaches. However, the use of simulation has skewed the attention to tasks that only require what robotics simulators can simulate: motion and physical contact. We present iGibson 2.0, an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. First, iGibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. Second, iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked. Additionally, given a logic state, iGibson 2.0 can sample valid physical states that satisfy it. This functionality can generate potentially infinite instances of tasks with minimal effort from the users. The sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations. As a result, we can collect demonstrations from humans on these new types of tasks, and use them for imitation learning. We evaluate the new capabilities of iGibson 2.0 to enable robot learning of novel tasks, in the hope of demonstrating the potential of this new simulator to support new research in embodied AI. iGibson 2.0 and its new dataset are publicly available at http://svl.stanford.edu/igibson/.},
	booktitle = {Conference on {Robot} {Learning} ({CoRL})},
	publisher = {PMLR},
	author = {Li, Chengshu and Xia, Fei and Martín-Martín, Roberto and Lingelbach, Michael and Srivastava, Sanjana and Shen, Bokui and Vainio, Kent Elliott and Gokmen, Cem and Dharan, Gokul and Jain, Tanish and Kurenkov, Andrey and Liu, Karen and Gweon, Hyowon and Wu, Jiajun and Fei-Fei, Li and Savarese, Silvio},
	editor = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
	month = nov,
	year = {2021},
	pages = {455--465},
	file = {Li et al. - 2022 - iGibson 2.0 Object-Centric Simulation for Robot L.pdf:C\:\\Users\\wb619\\Zotero\\storage\\IXWE7KYM\\Li et al. - 2022 - iGibson 2.0 Object-Centric Simulation for Robot L.pdf:application/pdf},
}

@inproceedings{blasinski_optimizing_2018,
	address = {San Francisco, USA},
	title = {Optimizing {Image} {Acquisition} {Systems} for {Autonomous} {Driving}},
	doi = {10.2352/ISSN.2470-1173.2018.05.PMII-161},
	booktitle = {{IS}\&{T} {International} {Symposium} on {Electronic} {Imaging}: {Photography}, {Mobile}, and {Immersive} {Imaging}},
	author = {Blasinski, Henryk and Farrell, Joyce and Lian, Trisha and Liu, Zhenyi and Wandell, Brian},
	year = {2018},
	pages = {161--1 -- 161--7},
	file = {Henryk Blasinski et al. - 2018 - Optimizing Image Acquisition Systems for Autonomou.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5RKUGV6W\\Henryk Blasinski et al. - 2018 - Optimizing Image Acquisition Systems for Autonomou.pdf:application/pdf},
}

@article{wang_validating_2005,
	title = {Validating {USARsim} for use in {HRI} {Research}},
	volume = {49},
	url = {https://doi.org/10.1177/154193120504900351},
	doi = {10.1177/154193120504900351},
	abstract = {HRI is an excellent candidate for simulator based research because of the relative simplicity of the systems being modeled, the behavioral fidelity possible with current physics engines and the capability of modern graphics cards to approximate camera video. In this paper we briefly introduce the USARsim simulation and discuss efforts to validate its behavior for use in Human Robot Interaction (HRI) research.},
	number = {3},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Wang, Jijun and Lewis, Michael and Hughes, Stephen and Koes, Mary and Carpin, Stefano},
	year = {2005},
	note = {\_eprint: https://doi.org/10.1177/154193120504900351},
	pages = {457--461},
	file = {Wang et al. - 2005 - Validating USARsim for use in HRI Research.pdf:C\:\\Users\\wb619\\Zotero\\storage\\NYIJBJG9\\Wang et al. - 2005 - Validating USARsim for use in HRI Research.pdf:application/pdf},
}

@inproceedings{sewtz_ursim_2022,
	address = {Big Sky, Montana, USA},
	title = {{URSim} - {A} {Versatile} {Robot} {Simulator} for {Extra}-{Terrestrial} {Exploration}},
	doi = {10.1109/AERO53065.2022.9843576},
	booktitle = {{IEEE} {Aerospace} {Conference} ({AERO})},
	author = {Sewtz, Marco and Lehner, Hannah and Fanger, Yunis and Eberle, Jan and Wudenka, Martin and Müller, Marcus G. and Bodenmüller, Tim and Schuster, Martin J.},
	year = {2022},
	pages = {1--14},
	file = {Sewtz et al. - 2022 - URSim - A Versatile Robot Simulator for Extra-Terr.pdf:C\:\\Users\\wb619\\Zotero\\storage\\SXLMNIKL\\Sewtz et al. - 2022 - URSim - A Versatile Robot Simulator for Extra-Terr.pdf:application/pdf},
}

@article{castilla-arquillo_hardware-accelerated_2022,
	title = {Hardware-{Accelerated} {Mars} {Sample} {Localization} {Via} {Deep} {Transfer} {Learning} {From} {Photorealistic} {Simulations}},
	volume = {7},
	doi = {10.1109/LRA.2022.3219306},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Castilla-Arquillo, Raul and Pérez-del-Pulgar, Carlos and Paz-Delgado, Gonzalo Jesus and Gerdes, Levin},
	year = {2022},
	pages = {12555--12561},
	file = {Castilla-Arquillo et al. - 2022 - Hardware-Accelerated Mars Sample Localization Via .pdf:C\:\\Users\\wb619\\Zotero\\storage\\RLUHZGLH\\Castilla-Arquillo et al. - 2022 - Hardware-Accelerated Mars Sample Localization Via .pdf:application/pdf},
}

@inproceedings{lemaignan_simulation_2014,
	address = {Cham},
	title = {Simulation and {HRI} {Recent} {Perspectives} with the {MORSE} {Simulator}},
	isbn = {978-3-319-11900-7},
	doi = {10.1007/978-3-319-11900-7_2},
	abstract = {Simulation in robotics is often a love-hate relationship: while simulators do save us a lot of time and effort compared to regular deployment of complex software architectures on complex hardware, simulators are also known to evade many of the real issues that robots need to manage when they enter the real world. Because humans are the paragon of dynamic, unpredictable, complex, real world entities, simulation of human-robot interactions may look condemn to fail, or, in the best case, to be mostly useless. This collective article reports on five independent applications of the MORSE simulator in the field of human-robot interaction: It appears that simulation is already useful, if not essential, to successfully carry out research in the field of HRI, and sometimes in scenarios we do not anticipate.},
	booktitle = {International {Conference} on {Simulation}, {Modeling}, and {Programming} for {Autonomous} {Robots}},
	publisher = {Springer International Publishing},
	author = {Lemaignan, Séverin and Hanheide, Marc and Karg, Michael and Khambhaita, Harmish and Kunze, Lars and Lier, Florian and Lütkebohle, Ingo and Milliez, Grégoire},
	editor = {Brugali, Davide and Broenink, Jan F. and Kroeger, Torsten and MacDonald, Bruce A.},
	year = {2014},
	pages = {13--24},
	file = {Lemaignan et al. - 2014 - Simulation and HRI Recent Perspectives with the MO.pdf:C\:\\Users\\wb619\\Zotero\\storage\\RFCK3GCJ\\Lemaignan et al. - 2014 - Simulation and HRI Recent Perspectives with the MO.pdf:application/pdf},
}

@article{denninger_blenderproc2_2023,
	title = {{BlenderProc2}: {A} {Procedural} {Pipeline} for {Photorealistic} {Rendering}},
	volume = {8},
	doi = {10.21105/joss.04901},
	number = {82},
	journal = {Journal of Open Source Software},
	author = {Denninger, Maximilian and Winkelbauer, Dominik and Sundermeyer, Martin and Boerdijk, Wout and Knauer, Markus and Strobl, Klaus H. and Humt, Matthias and Triebel, Rudolph},
	year = {2023},
	pages = {4901},
	file = {Denninger et al. - 2023 - BlenderProc2 A Procedural Pipeline for Photoreali.pdf:C\:\\Users\\wb619\\Zotero\\storage\\J32BSE5W\\Denninger et al. - 2023 - BlenderProc2 A Procedural Pipeline for Photoreali.pdf:application/pdf},
}

@inproceedings{hinterstoisser_annotation_2019,
	address = {Seoul, South Korea},
	title = {An {Annotation} {Saved} is an {Annotation} {Earned}: {Using} {Fully} {Synthetic} {Training} for {Object} {Detection}},
	doi = {10.1109/ICCVW.2019.00340},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV}) {Workshop}},
	author = {Hinterstoisser, Stefan and Pauly, Olivier and Heibel, Hauke and Martina, Marek and Bokeloh, Martin},
	month = oct,
	year = {2019},
	pages = {2787--2796},
	file = {Hinterstoisser et al. - 2019 - An Annotation Saved is an Annotation Earned Using.pdf:C\:\\Users\\wb619\\Zotero\\storage\\9RHSMPVY\\Hinterstoisser et al. - 2019 - An Annotation Saved is an Annotation Earned Using.pdf:application/pdf},
}

@article{lewis_developing_2003,
	title = {Developing a {Testbed} for {Studying} {Human}-{Robot} {Interaction} in {Urban} {Search} and {Rescue}},
	journal = {Simulation},
	author = {Lewis, Michael and Sycara, Katia P. and Nourbakhsh, Illah Reza},
	year = {2003},
}

@inproceedings{lewis_network-centric_2011,
	title = {Network-{Centric} {Control} for {Multirobot} {Teams} in {Urban} {Search} and {Rescue}},
	doi = {10.1109/HICSS.2011.315},
	booktitle = {2011 44th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Lewis, Michael and Sycara, Katia},
	year = {2011},
	pages = {1--10},
	file = {Lewis and Sycara - 2011 - Network-Centric Control for Multirobot Teams in Ur.pdf:C\:\\Users\\wb619\\Zotero\\storage\\JKR2QSJF\\Lewis and Sycara - 2011 - Network-Centric Control for Multirobot Teams in Ur.pdf:application/pdf},
}

@article{elmquist_modeling_2021,
	title = {Modeling {Cameras} for {Autonomous} {Vehicle} and {Robot} {Simulation}: {An} {Overview}},
	volume = {21},
	doi = {10.1109/JSEN.2021.3118952},
	number = {22},
	journal = {IEEE Sensors Journal},
	author = {Elmquist, Asher and Negrut, Dan},
	year = {2021},
	pages = {25547--25560},
	file = {Elmquist and Negrut - 2021 - Modeling Cameras for Autonomous Vehicle and Robot .pdf:C\:\\Users\\wb619\\Zotero\\storage\\UHEANW4J\\Elmquist and Negrut - 2021 - Modeling Cameras for Autonomous Vehicle and Robot .pdf:application/pdf},
}

@inproceedings{garcia-garcia_robotrix_2018,
	address = {Madrid, Spain},
	title = {The {RobotriX}: {An} {Extremely} {Photorealistic} and {Very}-{Large}-{Scale} {Indoor} {Dataset} of {Sequences} with {Robot} {Trajectories} and {Interactions}},
	doi = {10.1109/IROS.2018.8594495},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Garcia-Garcia, Alberto and Martinez-Gonzalez, Pablo and Oprea, Sergiu and Castro-Vargas, John Alejandro and Orts-Escolano, Sergio and Garcia-Rodriguez, Jose and Jover-Alvarez, Alvaro},
	month = oct,
	year = {2018},
	pages = {6790--6797},
	file = {Garcia-Garcia et al. - 2018 - The RobotriX An Extremely Photorealistic and Very.pdf:C\:\\Users\\wb619\\Zotero\\storage\\UUH8W5WA\\Garcia-Garcia et al. - 2018 - The RobotriX An Extremely Photorealistic and Very.pdf:application/pdf},
}

@article{liu_using_2023,
	title = {Using simulation to quantify the performance of automotive perception systems},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	url = {https://arxiv.org/abs/2303.00983},
	doi = {10.48550/ARXIV.2303.00983},
	journal = {arXiv},
	author = {Liu, Zhenyi and Shah, Devesh and Rahimpour, Alireza and Upadhyay, Devesh and Farrell, Joyce and Wandell, Brian A},
	year = {2023},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Graphics (cs.GR), electronic engineering, FOS: Electrical engineering, Image and Video Processing (eess.IV), information engineering},
	file = {Liu et al. - 2023 - Using simulation to quantify the performance of au.pdf:C\:\\Users\\wb619\\Zotero\\storage\\L8PTG8CQ\\Liu et al. - 2023 - Using simulation to quantify the performance of au.pdf:application/pdf},
}

@inproceedings{weinmann_material_2014,
	address = {Zurich, Switzerland},
	title = {Material {Classification} {Based} on {Training} {Data} {Synthesized} {Using} a {BTF} {Database}},
	isbn = {978-3-319-10578-9},
	doi = {10.1007/978-3-319-10578-9_11},
	abstract = {To cope with the richness in appearance variation found in real-world data under natural illumination, we propose to synthesize training data capturing these variations for material classification. Using synthetic training data created from separately acquired material and illumination characteristics allows to overcome the problems of existing material databases which only include a tiny fraction of the possible real-world conditions under controlled laboratory environments. However, it is essential to utilize a representation for material appearance which preserves fine details in the reflectance behavior of the digitized materials. As BRDFs are not sufficient for many materials due to the lack of modeling mesoscopic effects, we present a high-quality BTF database with 22,801 densely measured view-light configurations including surface geometry measurements for each of the 84 measured material samples. This representation is used to generate a database of synthesized images depicting the materials under different view-light conditions with their characteristic surface geometry using image-based lighting to simulate the complexity of real-world scenarios. We demonstrate that our synthesized data allows classifying materials under complex real-world scenarios.},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Weinmann, Michael and Gall, Juergen and Klein, Reinhard},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	month = sep,
	year = {2014},
	pages = {156--171},
	file = {Weinmann et al. - 2014 - Material Classification Based on Training Data Syn.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LJLRKCQ6\\Weinmann et al. - 2014 - Material Classification Based on Training Data Syn.pdf:application/pdf},
}

@article{filip_template-based_2014,
	title = {Template-{Based} {Sampling} of {Anisotropic} {BRDFs}},
	volume = {33},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12477},
	doi = {https://doi.org/10.1111/cgf.12477},
	abstract = {Abstract BRDFs are commonly used to represent given materials’ appearance in computer graphics and related fields. Although, in the recent past, BRDFs have been extensively measured, compressed, and fitted by a variety of analytical models, most research has been primarily focused on simplified isotropic BRDFs. In this paper, we present a unique database of 150 BRDFs representing a wide range of materials; the majority exhibiting anisotropic behavior. Since time-consuming BRDF measurement represents a major obstacle in the digital material appearance reproduction pipeline, we tested several approaches estimating a very limited set of samples capable of high quality appearance reconstruction. Initially, we aligned all measured BRDFs according to the location of the anisotropic highlights. Then we propose an adaptive sampling method based on analysis of the measured BRDFs. For each BRDF, a unique sampling pattern was computed, given a predefined count of samples. Further, template-based methods are introduced based on reusing of the precomputed sampling patterns. This approach enables a more efficient measurement of unknown BRDFs while preserving the visual fidelity for the majority of tested materials. Our method exhibits better performance and stability than competing sparse sampling approaches; especially for higher numbers of samples.},
	number = {7},
	journal = {Computer Graphics Forum},
	author = {Filip, J. and Vávra, R.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12477},
	keywords = {and texture, Categories and Subject Descriptors (according to ACM CCS), shading, shadowing, I.3.4 Computer Graphics: Digitization and Image Capture—Reflectance, I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Color},
	pages = {91--99},
	file = {Filip and Vávra - 2014 - Template-Based Sampling of Anisotropic BRDFs.pdf:C\:\\Users\\wb619\\Zotero\\storage\\M6Q9Q3UA\\Filip and Vávra - 2014 - Template-Based Sampling of Anisotropic BRDFs.pdf:application/pdf},
}

@article{matusik_data-driven_2003,
	title = {A {Data}-{Driven} {Reflectance} {Model}},
	volume = {22},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/882262.882343},
	doi = {10.1145/882262.882343},
	abstract = {We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs.},
	number = {3},
	journal = {ACM Transactions on Graphics},
	author = {Matusik, Wojciech and Pfister, Hanspeter and Brand, Matt and McMillan, Leonard},
	year = {2003},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {BRDF, image-based modeling, light reflection models, photometric measurements, reflectance},
	pages = {759--769},
	file = {Matusik et al. - 2003 - A Data-Driven Reflectance Model.pdf:C\:\\Users\\wb619\\Zotero\\storage\\G7QZBM3T\\Matusik et al. - 2003 - A Data-Driven Reflectance Model.pdf:application/pdf},
}

@book{lambert_photometria_1760,
	title = {Photometria sive de mensura et gradibus luminis, colorum et umbrae},
	publisher = {Klett},
	author = {Lambert, Johann Heinrich},
	year = {1760},
}

@article{phong_illumination_1975,
	title = {Illumination for {Computer} {Generated} {Pictures}},
	volume = {18},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/360825.360839},
	doi = {10.1145/360825.360839},
	abstract = {The quality of computer generated images of three-dimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen. The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm. The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here. Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images.},
	number = {6},
	journal = {Communications of the ACM},
	author = {Phong, Bui Tuong},
	year = {1975},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {shading, computer graphics, graphic display, hidden surface removal},
	pages = {311--317},
	file = {Phong - 1975 - Illumination for Computer Generated Pictures.pdf:C\:\\Users\\wb619\\Zotero\\storage\\M3DKLV8L\\Phong - 1975 - Illumination for Computer Generated Pictures.pdf:application/pdf},
}

@article{walt_disney_animation_studios_brdf_2012,
	title = {{BRDF} {Explorer}},
	journal = {GitHub repository},
	url = {https://github.com/wdas/brdf},
	urldate = {2023-03-12},
	author = {Walt Disney Animation Studios},
	year = {2012},
}

@article{trowbridge_average_1975,
	title = {Average irregularity representation of a rough surface for ray reflection},
	volume = {65},
	url = {https://opg.optica.org/abstract.cfm?URI=josa-65-5-531},
	doi = {10.1364/JOSA.65.000531},
	abstract = {A new ray model is presented for the reflection of electromagnetic radiation from the rough air-material interface of a randomly rough surface. Unlike previous derivations that modeled the rough interface as consisting of microareas randomly oriented but flat (facets), this derivation models it as consisting of microareas not only randomly oriented but also randomly curved. Physically, the models are the same, but this new derivation leads to some new results. (1) For any given rough surface, there exists a single, optically smooth, curved surface of revolution of very restricted shape that will reflect radiation in the same distribution as that reflected by the rough interface. (2) Modeling that surface as an ellipsoid of revolution gives a surface-structure function that appears more accurate and useful than existing ones. (3) Unlike the facet derivations, this derivation lends itself to a normalization that gives the absolute, instead of just a comparative, reflectance-distribution function.},
	number = {5},
	journal = {Journal of the Optical Society of America},
	author = {Trowbridge, T. S. and Reitz, K. P.},
	year = {1975},
	note = {Publisher: Optica Publishing Group},
	keywords = {Diffraction, Electromagnetic radiation, Fresnel equations, Reflection, Refractive index, Surfaces},
	pages = {531--536},
	file = {Trowbridge and Reitz - 1975 - Average irregularity representation of a rough sur.pdf:C\:\\Users\\wb619\\Zotero\\storage\\PECJDSSV\\Trowbridge and Reitz - 1975 - Average irregularity representation of a rough sur.pdf:application/pdf},
}

@inproceedings{walter_microfacet_2007,
	address = {Grenoble, France},
	title = {Microfacet models for refraction through rough surfaces},
	booktitle = {Eurographics conference on {Rendering} {Techniques}},
	author = {Walter, Bruce and Marschner, Stephen R and Li, Hongsong and Torrance, Kenneth E},
	month = jun,
	year = {2007},
	pages = {195--206},
	file = {Walter et al. - 2007 - Microfacet models for refraction through rough sur.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LCNBZRD5\\Walter et al. - 2007 - Microfacet models for refraction through rough sur.pdf:application/pdf},
}

@article{hapke_bidirectional_1981,
	title = {Bidirectional reflectance spectroscopy: 1. {Theory}},
	volume = {86},
	doi = {10.1029/JB086iB04p03039},
	abstract = {An approximate analytic solution to the radiative transfer equation describing the scattering of light from particulate surfaces is derived. Multiple scattering and mutual shadowing are taken into account. Analytic expressions for the following quantities are found: bidirectional reflectance, radiance factor, radiance coefficient, normal, hemispherical, Bond, and physical albedos, integral phase function, phase integral, and limb-darkening profile. Scattering functions for mixtures can be calculated, as well as corrections for comparing experimental laboratory transmission or reflection spectra with observational planetary spectra. An expression for the scattering efficiency of an irregular particle large compared with the wavelength is derived. For closely spaced, nonopaque particles this efficiency is approximated by (1 + αDe)−l, where α is the true absorption coefficient and De is an effective particle diameter of the order of twice the mean particle size. For monomineralic surfaces it is shown that α = ( 1 − w)/wDe, where w is the single-scattering albedo and can be determined from reflectance measurements of a powder, so that α may be calculated from reflectance. This theory should be useful for interpretations of reflectance spectroscopy of laboratory surfaces and photometry of solar system objects. From photometric observations of a body the following may be estimated: average single-scattering albedo, average particle phase function, average macroscopic slope, and porosity.},
	number = {B4},
	journal = {Journal of Geophysical Research: Solid Earth},
	author = {Hapke, Bruce},
	year = {1981},
	pages = {3039--3054},
	file = {Hapke - 1981 - Bidirectional reflectance spectroscopy 1. Theory.pdf:C\:\\Users\\wb619\\Zotero\\storage\\IPQN38P4\\Hapke - 1981 - Bidirectional reflectance spectroscopy 1. Theory.pdf:application/pdf},
}

@article{sato_resolved_2014,
	title = {Resolved {Hapke} parameter maps of the {Moon}},
	volume = {119},
	doi = {10.1002/2013JE004580},
	abstract = {AbstractWe derived spatially resolved near-global Hapke photometric parameter maps of the Moon from 21 months of Lunar Reconnaissance Orbiter Camera (LROC) Wide Angle Camera (WAC) multispectral observations using a novel “tile-by-tile method” (1° latitude by 1° longitude bins). The derived six parameters (w,b,c,BS0,hS, and) for each tile were used to normalize the observed reflectance (standard angles i = g = 60°, e = 0° instead of the traditional angles i = g = 30°, e = 0°) within each tile, resulting in accurate normalization optimized for the local photometric response. Each pixel in the seven-color near-global mosaic (70°S to 70°N and 0°E to 360°E) was computed by the median of normalized reflectance from large numbers of repeated observations (UV: ∼50 and visible: ∼126 on average). The derived mosaic exhibits no significant artifacts with latitude or along the tile boundaries, demonstrating the quality of the normalization procedure. The derived Hapke parameter maps reveal regional photometric response variations across the lunar surface. The b, c (Henyey-Greenstein double-lobed phase function parameters) maps demonstrate decreased backscattering in the maria relative to the highlands (except 321 nm band), probably due to the higher content of both SMFe (submicron iron) and ilmenite in the interiors of back scattering agglutinates in the maria. The hS (angular width of shadow hiding opposition effect) map exhibits relatively lower values in the maria than the highlands and slightly higher values for immature highland crater ejecta, possibly related to the variation in a grain size distribution of regolith.},
	number = {8},
	journal = {Journal of Geophysical Research: Planets},
	author = {Sato, H. and Robinson, M. S. and Hapke, B. and Denevi, B. W. and Boyd, A. K.},
	year = {2014},
	keywords = {LROC, Moon, photometry, surface reflectance},
	pages = {1775--1805},
	file = {Sato et al. - 2014 - Resolved Hapke parameter maps of the Moon.pdf:C\:\\Users\\wb619\\Zotero\\storage\\47VAR87R\\Sato et al. - 2014 - Resolved Hapke parameter maps of the Moon.pdf:application/pdf},
}

@phdthesis{kuzminykh_physically_2021,
	type = {Bachelor {Thesis}},
	title = {Physically {Based} {Real}-{Time} {Rendering} of the {Moon}},
	language = {English},
	urldate = {2023-03-13},
	school = {Course of Studies B. Sc. Media Design Computing, Hochschule Hannover},
	author = {Kuzminykh, Alexander},
	month = aug,
	year = {2021},
	file = {Kuzminykh - 2021 - Physically Based Real-Time Rendering of the Moon.pdf:C\:\\Users\\wb619\\Zotero\\storage\\D2ZPKBH6\\Kuzminykh - 2021 - Physically Based Real-Time Rendering of the Moon.pdf:application/pdf},
}

@article{serban_chronovehicle_2019,
	title = {Chrono::{Vehicle}: template-based ground vehicle modelling and simulation},
	volume = {5},
	doi = {10.1504/IJVP.2019.097096},
	abstract = {Chrono::Vehicle is a module of the open-source multi-physics simulation package Chrono, aimed at modelling, simulation, and visualisation of wheeled and tracked ground vehicle multi-body systems. Its software architecture and design was dictated by the desire to provide an expeditious and user friendly mechanism for assembling complex vehicle models, while leveraging the underlying Chrono modelling and simulation capabilities, allowing seamless interfacing to other optional Chrono modules (e.g., its granular dynamics and fluid-solid interaction (FSI) capabilities), and providing a modular and expressive API to facilitate its use in third-party applications. Vehicle models are specified as a hierarchy of subsystems, each of which is an instantiation of a predefined subsystem template. Written in C++, Chrono::Vehicle is offered as a middleware library. In this paper, we provide an overview of the Chrono::Vehicle software design philosophy, its main capabilities and features, describe the types of ground vehicle mobility simulations it enables, and outline several directions of future development and planned extensions.},
	number = {1},
	journal = {International Journal of Vehicle Performance},
	author = {Serban, Radu and Taylor, Michael and Negrut, Dan and Tasora, Alessandro},
	year = {2019},
	pages = {18--39},
}

@article{chen_digital_2009,
	title = {Digital {Camera} {Imaging} {System} {Simulation}},
	volume = {56},
	doi = {10.1109/TED.2009.2030995},
	number = {11},
	journal = {IEEE Transactions on Electron Devices},
	author = {Chen, Junqing and Venkataraman, Kartik and Bakin, Dmitry and Rodricks, Brian and Gravelle, Robert and Rao, Pravin and Ni, Yongshen},
	year = {2009},
	pages = {2496--2505},
	file = {Chen et al. - 2009 - Digital Camera Imaging System Simulation.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4CVZDMP9\\Chen et al. - 2009 - Digital Camera Imaging System Simulation.pdf:application/pdf},
}

@article{ulbricht_verification_2006,
	title = {Verification of {Physically} {Based} {Rendering} {Algorithms}},
	volume = {25},
	url = {https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/abs/10.1111/j.1467-8659.2006.00938.x},
	doi = {https://doi-org.ezproxy.library.wisc.edu/10.1111/j.1467-8659.2006.00938.x},
	abstract = {Abstract Within computer graphics, the field of predictive rendering is concerned with those methods of image synthesis that yield results that do not only look real, but are also radiometrically correct renditions of nature, i.e. which are accurate predictions of what a real scene would look like under given lighting conditions. In order to guarantee the correctness of the results obtained by such techniques, three stages of such a rendering system have to be verified with particular care: the light reflection models, the light transport simulation and the perceptually based calculations used at display time. In this report, we will concentrate on the state of the art with respect to the second step in this chain. Various approaches for experimental verification of the implementation of a physically based rendering system have been proposed so far. However, the problem of proving that the results are correct is not fully solved yet, and no standardized methodology is available. We give an overview of existing literature, discuss the strengths and weaknesses of the described methods and illustrate the unsolved problems. We also briefly discuss the related issue of image quality metrics.},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Ulbricht, Christiane and Wilkie, Alexander and Purgathofer, Werner},
	year = {2006},
	note = {\_eprint: https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/pdfdirect/10.1111/j.1467-8659.2006.00938.x},
	keywords = {global illumination, experimental validation, I.6.4 Simulation and Modelling Model Validation and Analysis, images quality metrics, physically based rendering, test scenes, visual comparisons, Color metric},
	pages = {237--255},
	file = {Ulbricht et al. - 2006 - Verification of Physically Based Rendering Algorit.pdf:C\:\\Users\\wb619\\Zotero\\storage\\J4U3VJIY\\Ulbricht et al. - 2006 - Verification of Physically Based Rendering Algorit.pdf:application/pdf},
}

@article{meyer_experimental_1986,
	title = {An {Experimental} {Evaluation} of {Computer} {Graphics} {Imagery}},
	volume = {5},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/7529.7920},
	doi = {10.1145/7529.7920},
	abstract = {Accurate simulation of light propagation within an environment and perceptually based imaging techniques are necessary for the creation of realistic images. A physical experiment that verifies the simulation of reflected light intensities for diffuse environments was conducted. Measurements of radiant energy flux densities are compared with predictions using the radiosity method for those physical environments. By using color science procedures the results of the light model simulation are then transformed to produce a color television image. The final image compares favorably with the original physical model. The experiment indicates that, when the physical model and the simulation were viewed through a view camera, subjects could not distinguish between them. The results and comparison of both test procedures are presented within this paper.},
	number = {1},
	journal = {ACM Transactions on Graphics},
	author = {Meyer, Gary W. and Rushmeier, Holly E. and Cohen, Michael F. and Greenberg, Donald P. and Torrance, Kenneth E.},
	year = {1986},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {Cornell box},
	pages = {30--50},
	file = {Meyer et al. - 1986 - An Experimental Evaluation of Computer Graphics Im.pdf:C\:\\Users\\wb619\\Zotero\\storage\\TF43T8RI\\Meyer et al. - 1986 - An Experimental Evaluation of Computer Graphics Im.pdf:application/pdf},
}

@article{grapinet_characterization_2013,
	title = {Characterization and simulation of optical sensors},
	volume = {60},
	issn = {0001-4575},
	url = {https://www.sciencedirect.com/science/article/pii/S0001457513001693},
	doi = {10.1016/j.aap.2013.04.026},
	abstract = {Numerical simulation is gradually becoming an advantage in active safety. This is why the development of realistic numerical models enabling to substitute real truth by simulated truth is primordial. In order to provide an accurate and cost effective solution to simulate real optical sensor behavior, the software Pro-SiVIC™ has been developed. Simulations with the software Pro-SiVIC™ can replace real tests with optical sensors and hence allow substantial cost and time savings during the development of solutions for driver assistance systems. An optical platform has been developed by IFSTTAR (French Institute of Science and Technology for Transport, Development and Networks) to characterize and validate any existing camera, in order to measure their characteristics as distortion, vignetting, focal length, etc. By comparing real and simulated sensors with this platform, this paper demonstrates that Pro-SiVIC™ accurately reproduces real sensors’ behavior.},
	journal = {Accident Analysis and Prevention},
	author = {Grapinet, M. and Souza, Ph De and Smal, J.-C. and Blosseville, J.-M.},
	year = {2013},
	keywords = {Characterization platform, Driving assistance system, Numerical simulation, Optical sensor, Pro-SiVIC},
	pages = {344--352},
	file = {Grapinet et al. - 2013 - Characterization and simulation of optical sensors.pdf:C\:\\Users\\wb619\\Zotero\\storage\\BKJ7MBVM\\Grapinet et al. - 2013 - Characterization and simulation of optical sensors.pdf:application/pdf},
}

@inproceedings{gruyer_modeling_2012,
	address = {Madrid, Spain},
	title = {Modeling and validation of a new generic virtual optical sensor for {ADAS} prototyping},
	doi = {10.1109/IVS.2012.6232260},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Gruyer, D. and Grapinet, M. and De Souza, P.},
	month = jun,
	year = {2012},
	pages = {969--974},
	file = {Gruyer et al. - 2012 - Modeling and validation of a new generic virtual o.pdf:C\:\\Users\\wb619\\Zotero\\storage\\3XFBVNI4\\Gruyer et al. - 2012 - Modeling and validation of a new generic virtual o.pdf:application/pdf},
}

@inproceedings{peterson_surface_2004,
	address = {Denver, Colorado, United States},
	title = {Surface and buried landmine scene generation and validation using the digital imaging and remote sensing image generation model},
	volume = {5546},
	url = {https://doi.org/10.1117/12.561264},
	doi = {10.1117/12.561264},
	booktitle = {Imaging {Spectrometry} {X}},
	publisher = {SPIE},
	author = {Peterson, Erin D. and Brown, Scott D. and Hattenberger, Timothy J. and Schott, John R.},
	editor = {Shen, Sylvia S. and Lewis, Paul E.},
	year = {2004},
	note = {Backup Publisher: International Society for Optics and Photonics},
	keywords = {Color metric, DIRSIG, hyperspectral image simulation, long wave infrared, mine detection, reststrahlen},
	pages = {312 -- 323},
	file = {Peterson et al. - 2004 - Surface and buried landmine scene generation and v.pdf:C\:\\Users\\wb619\\Zotero\\storage\\Q6TBT4ET\\Peterson et al. - 2004 - Surface and buried landmine scene generation and v.pdf:application/pdf},
}

@article{clausen_acquisition_2018,
	title = {Acquisition and {Validation} of {Spectral} {Ground} {Truth} {Data} for {Predictive} {Rendering} of {Rough} {Surfaces}},
	volume = {37},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13470},
	doi = {10.1111/cgf.13470},
	abstract = {Abstract Physically based rendering uses principles of physics to model the interaction of light with matter. Even though it is possible to achieve photorealistic renderings, it often fails to be predictive. There are two major issues: first, there is no analytic material model that considers all appearance critical characteristics; second, light is in many cases described by only 3 RGB-samples. This leads to the problem that there are different models for different material types and that wavelength dependent phenomena are only approximated. In order to be able to analyze the influence of both problems on the appearance of real world materials, an accurate comparison between rendering and reality is necessary. Therefore, in this work, we acquired a set of precisely and spectrally resolved ground truth data. It consists of the precise description of a new developed reference scene including isotropic BRDFs of 24 color patches, as well as the reference measurements of all patches under 13 different angles inside the reference scene. Our reference data covers rough materials with many different spectral distributions and various illumination situations, from direct light to indirect light dominated situations.},
	number = {4},
	journal = {Computer Graphics Forum},
	author = {Clausen, O. and Marroquim, R. and Fuhrmann, A.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13470},
	keywords = {Color metric, •Computing methodologies → Reflectance modeling, CCS Concepts},
	pages = {1--12},
	file = {Clausen et al. - 2018 - Acquisition and Validation of Spectral Ground Trut.pdf:C\:\\Users\\wb619\\Zotero\\storage\\QY36YMJN\\Clausen et al. - 2018 - Acquisition and Validation of Spectral Ground Trut.pdf:application/pdf},
}

@inproceedings{hasirlioglu_model-based_2018,
	address = {Maui, HI, USA},
	title = {A {Model}-{Based} {Approach} to {Simulate} {Rain} {Effects} on {Automotive} {Surround} {Sensor} {Data}},
	doi = {10.1109/ITSC.2018.8569907},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Hasirlioglu, Sinan and Riener, Andreas},
	month = nov,
	year = {2018},
	pages = {2609--2615},
	file = {Hasirlioglu and Riener - 2018 - A Model-Based Approach to Simulate Rain Effects on.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6VRIVUND\\Hasirlioglu and Riener - 2018 - A Model-Based Approach to Simulate Rain Effects on.pdf:application/pdf},
}

@article{hasirlioglu_general_2020,
	title = {A {General} {Approach} for {Simulating} {Rain} {Effects} on {Sensor} {Data} in {Real} and {Virtual} {Environments}},
	volume = {5},
	doi = {10.1109/TIV.2019.2960944},
	number = {3},
	journal = {IEEE Transactions on Intelligent Vehicles},
	author = {Hasirlioglu, Sinan and Riener, Andreas},
	year = {2020},
	pages = {426--438},
	file = {Hasirlioglu and Riener - 2020 - A General Approach for Simulating Rain Effects on .pdf:C\:\\Users\\wb619\\Zotero\\storage\\649KKHCF\\Hasirlioglu and Riener - 2020 - A General Approach for Simulating Rain Effects on .pdf:application/pdf},
}

@article{lyu_simulations_2021,
	title = {Simulations of fluorescence imaging in the oral cavity},
	volume = {12},
	doi = {10.1364/BOE.429995},
	abstract = {We describe an end-to-end image systems simulation that models a device capable of measuring fluorescence in the oral cavity. Our software includes a 3D model of the oral cavity and excitation-emission matrices of endogenous fluorophores that predict the spectral radiance of oral mucosal tissue. The predicted radiance is transformed by a model of the optics and image sensor to generate expected sensor image values. We compare simulated and real camera data from tongues in healthy individuals and show that the camera sensor chromaticity values can be used to quantify the fluorescence from porphyrins relative to the bulk fluorescence from multiple fluorophores (elastin, NADH, FAD, and collagen). Validation of the simulations supports the use of soft-prototyping in guiding system design for fluorescence imaging.},
	number = {7},
	journal = {Biomedical Optics Express},
	author = {Lyu, Zheng and Jiang, Haomiao and Xiao, Feng and Rong, Jian and Zhang, Tingcheng and Wandell, Brian and Farrell, Joyce},
	year = {2021},
	pages = {4276--4292},
	file = {Lyu et al. - 2021 - Simulations of fluorescence imaging in the oral ca.pdf:C\:\\Users\\wb619\\Zotero\\storage\\9XCRFAVY\\Lyu et al. - 2021 - Simulations of fluorescence imaging in the oral ca.pdf:application/pdf},
}

@article{tsirikoglou_procedural_2017,
	title = {Procedural {Modeling} and {Physically} {Based} {Rendering} for {Synthetic} {Data} {Generation} in {Automotive} {Applications}},
	url = {https://arxiv.org/abs/1710.06270},
	author = {Tsirikoglou, Apostolia and Kronander, Joel and Wrenninge, Magnus and Unger, Jonas},
	year = {2017},
	journal={arXiv preprint arXiv:1710.06270},
	file = {Tsirikoglou et al. - 2017 - Procedural Modeling and Physically Based Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4NEJ2E9A\\Tsirikoglou et al. - 2017 - Procedural Modeling and Physically Based Rendering.pdf:application/pdf},
}

@inproceedings{zhang_physically-based_2017,
	address = {Honolulu, HI, USA},
	title = {Physically-{Based} {Rendering} for {Indoor} {Scene} {Understanding} {Using} {Convolutional} {Neural} {Networks}},
	doi = {10.1109/CVPR.2017.537},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Yinda and Song, Shuran and Yumer, Ersin and Savva, Manolis and Lee, Joon-Young and Jin, Hailin and Funkhouser, Thomas},
	month = jul,
	year = {2017},
	pages = {5057--5065},
	file = {Zhang et al. - 2017 - Physically-Based Rendering for Indoor Scene Unders.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8RR3CCRN\\Zhang et al. - 2017 - Physically-Based Rendering for Indoor Scene Unders.pdf:application/pdf},
}

@inproceedings{hagn_improved_2021,
	address = {Ingolstadt, Germany},
	title = {Improved {Sensor} {Model} for {Realistic} {Synthetic} {Data} {Generation}},
	isbn = {978-1-4503-9139-9},
	doi = {10.1145/3488904.3493383},
	abstract = {Synthetic, i.e., computer generated-imagery (CGI) data is a key component for training and validating deep-learning-based perceptive functions due to its ability to simulate rare cases, avoidance of privacy issues and easy generation of huge datasets with pixel accurate ground-truth data. Recent simulation and rendering engines simulate already a wealth of realistic optical effects, but are mainly focused on the human perception system. But, perceptive functions require realistic images modeled with sensor artifacts as close as possible towards the sensor the training data has been recorded with. In this paper we propose a method to improve the data synthesis by introducing a more realistic sensor model that implements a number of sensor and lens artifacts. We further propose a Wasserstein distance (earth mover’s distance, EMD) based domain divergence measure and use it as minimization criterion to adapt the parameters of our sensor artifact simulation from synthetic to real images. With the optimized sensor parameters applied to the synthetic images for training, the mIoU of a semantic segmentation network (DeeplabV3+) solely trained on synthetic images is increased from 40.36\% to 47.63\%.},
	booktitle = {{ACM} {Computer} {Science} in {Cars} {Symposium}},
	author = {Hagn, Korbinian and Grau, Oliver},
	month = nov,
	year = {2021},
	keywords = {image synthesis, neural networks, datasets, domain adaptation, sensor simulation},
}

@article{elmquist_performance_2022,
	title = {A performance contextualization approach to validating camera models for robot simulation},
	doi = {10.48550/arXiv.2208.01022},
	journal = {arXiv e-prints},
	author = {Elmquist, Asher and Serban, Radu and Negrut, Dan},
	month = aug,
	year = {2022},
	keywords = {Computer Science - Robotics},
	pages = {arXiv:2208.01022},
	file = {Elmquist et al. - 2022 - A performance contextualization approach to valida.pdf:C\:\\Users\\wb619\\Zotero\\storage\\F2BBXLVH\\Elmquist et al. - 2022 - A performance contextualization approach to valida.pdf:application/pdf},
}

@inproceedings{foo_image_2022,
	address = {Bordeaux, France},
	title = {Image {Data} {Augmentation} with {Unpaired} {Image}-to-{Image} {Camera} {Model} {Translation}},
	doi = {10.1109/ICIP46576.2022.9897671},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Foo, Chi Fa and Winkler, Stefan},
	month = oct,
	year = {2022},
	pages = {3246--3250},
	file = {Foo and Winkler - 2022 - Image Data Augmentation with Unpaired Image-to-Ima.pdf:C\:\\Users\\wb619\\Zotero\\storage\\M3H5KSL7\\Foo and Winkler - 2022 - Image Data Augmentation with Unpaired Image-to-Ima.pdf:application/pdf},
}

@article{hapke_bidirectional_1986,
	title = {Bidirectional reflectance spectroscopy: 4. {The} extinction coefficient and the opposition effect},
	volume = {67},
	issn = {0019-1035},
	doi = {10.1016/0019-1035(86)90108-9},
	abstract = {The extinction coefficient and the opposition effect in a particulate medium are discussed. Simple analytic expressions that describe these quantities are rigorously derived using a few physically realistic mathematical approximations. The particles of the medium may have a distribution of sizes, and the particle density is allowed to vary with depth. The expression for the extinction coefficient is valid for both large and small porosities and is more accurate than the one commonly used. The opposition effect arises from the hiding of extinction shadows and occurs even if the particles are transparent. The angular half-width of the opposition peak is shown to be equal to the ratio of the average particle radius to extinction length at unit slant path optical depth in the medium, and depends on both the filling factor (ratio of bulk to solid density) F and the particle size distribution. To illustrate the theory, it is fitted to observations of the Moon, an asteroid, and a satellite of Uranus; Europa is also discussed. For the Moon, a value of F = 0.41 is derived, in good agreement with data on Apollo soils. For Oberon, the width of the opposition effect peak gives F = 0.10, which is similar to values for terrestrial frosts and snow. Thus, the narrow opposition effects of the Uranian satellites do not require any unusual particles or microstructures on their surfaces. More photometric observations of Europa are needed.},
	number = {2},
	journal = {Icarus},
	author = {Hapke, Bruce},
	year = {1986},
	pages = {264--280},
	file = {Hapke - 1986 - Bidirectional reflectance spectroscopy 4. The ext.pdf:C\:\\Users\\wb619\\Zotero\\storage\\AEZJX5W5\\Hapke - 1986 - Bidirectional reflectance spectroscopy 4. The ext.pdf:application/pdf},
}

@inproceedings{szot_habitat_2021,
	title = {Habitat 2.0: {Training} {Home} {Assistants} to {Rearrange} their {Habitat}},
	volume = {34},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Szot, Andrew and Clegg, Alexander and Undersander, Eric and Wijmans, Erik and Zhao, Yili and Turner, John and Maestre, Noah and Mukadam, Mustafa and Chaplot, Devendra Singh and Maksymets, Oleksandr and Gokaslan, Aaron and Vondruš, Vladimír and Dharur, Sameer and Meier, Franziska and Galuba, Wojciech and Chang, Angel and Kira, Zsolt and Koltun, Vladlen and Malik, Jitendra and Savva, Manolis and Batra, Dhruv},
	month = dec,
	year = {2021},
	pages = {251--266},
	file = {Szot et al. - 2021 - Habitat 2.0 Training Home Assistants to Rearrange.pdf:C\:\\Users\\wb619\\Zotero\\storage\\T8GQMQ3C\\Szot et al. - 2021 - Habitat 2.0 Training Home Assistants to Rearrange.pdf:application/pdf},
}

@inproceedings{shen_igibson_2021,
	address = {Prague, Czech},
	title = {{iGibson} 1.0: {A} {Simulation} {Environment} for {Interactive} {Tasks} in {Large} {Realistic} {Scenes}},
	doi = {10.1109/IROS51168.2021.9636667},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Shen, Bokui and Xia, Fei and Li, Chengshu and Martín-Martín, Roberto and Fan, Linxi and Wang, Guanzhi and Pérez-D’Arpino, Claudia and Buch, Shyamal and Srivastava, Sanjana and Tchapmi, Lyne and Tchapmi, Micael and Vainio, Kent and Wong, Josiah and Fei-Fei, Li and Savarese, Silvio},
	month = sep,
	year = {2021},
	pages = {7520--7527},
}

@inproceedings{amini_vista_2022,
	address = {Philadelphia, PA, USA},
	title = {{VISTA} 2.0: {An} {Open}, {Data}-driven {Simulator} for {Multimodal} {Sensing} and {Policy} {Learning} for {Autonomous} {Vehicles}},
	doi = {10.1109/ICRA46639.2022.9812276},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Amini, Alexander and Wang, Tsun-Hsuan and Gilitschenski, Igor and Schwarting, Wilko and Liu, Zhijian and Han, Song and Karaman, Sertac and Rus, Daniela},
	month = may,
	year = {2022},
	pages = {2419--2426},
	file = {Amini et al. - 2022 - VISTA 2.0 An Open, Data-driven Simulator for Mult.pdf:C\:\\Users\\wb619\\Zotero\\storage\\ZFBW6PLD\\Amini et al. - 2022 - VISTA 2.0 An Open, Data-driven Simulator for Mult.pdf:application/pdf},
}

@article{amini_learning_2020,
	title = {Learning {Robust} {Control} {Policies} for {End}-to-{End} {Autonomous} {Driving} {From} {Data}-{Driven} {Simulation}},
	volume = {5},
	doi = {10.1109/LRA.2020.2966414},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Amini, Alexander and Gilitschenski, Igor and Phillips, Jacob and Moseyko, Julia and Banerjee, Rohan and Karaman, Sertac and Rus, Daniela},
	year = {2020},
	pages = {1143--1150},
	file = {Amini et al. - 2020 - Learning Robust Control Policies for End-to-End Au.pdf:C\:\\Users\\wb619\\Zotero\\storage\\GTDDFR6X\\Amini et al. - 2020 - Learning Robust Control Policies for End-to-End Au.pdf:application/pdf},
}

@article{mazhar_chrono_2013,
	title = {{CHRONO}: a parallel multi-physics library for rigid-body, flexible-body, and fluid dynamics},
	volume = {4},
	doi = {10.5194/ms-4-49-2013},
	number = {1},
	journal = {Mechanical Sciences},
	author = {Mazhar, H. and Heyn, T. and Pazouki, A. and Melanz, D. and Seidl, A. and Bartholomew, A. and Tasora, A. and Negrut, D.},
	year = {2013},
	pages = {49--64},
	file = {Mazhar et al. - 2013 - CHRONO a parallel multi-physics library for rigid.pdf:C\:\\Users\\wb619\\Zotero\\storage\\P5EH4ULH\\Mazhar et al. - 2013 - CHRONO a parallel multi-physics library for rigid.pdf:application/pdf},
}

@inproceedings{huber_real-time_2009,
	address = {Kyoto, Japan},
	title = {Real-time photo-realistic visualization of {3D} environments for enhanced tele-operation of vehicles},
	doi = {10.1109/ICCVW.2009.5457431},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV}) {Workshops}},
	author = {Huber, Daniel and Herman, Herman and Kelly, Alonzo and Rander, Pete and Ziglar, Jason},
	month = sep,
	year = {2009},
	pages = {1518--1525},
	file = {Huber et al. - 2009 - Real-time photo-realistic visualization of 3D envi.pdf:C\:\\Users\\wb619\\Zotero\\storage\\DWJBM6AR\\Huber et al. - 2009 - Real-time photo-realistic visualization of 3D envi.pdf:application/pdf},
}

@article{kelly_real-time_2011,
	title = {Real-time photorealistic virtualized reality interface for remote mobile robot control},
	volume = {30},
	url = {https://doi.org/10.1177/0278364910383724},
	doi = {10.1177/0278364910383724},
	abstract = {The task of teleoperating a robot over a wireless video link is known to be very difficult. Teleoperation becomes even more difficult when the robot is surrounded by dense obstacles, or speed requirements are high, or video quality is poor, or wireless links are subject to latency. Due to high-quality lidar data, and improvements in computing and video compression, virtualized reality has the capacity to dramatically improve teleoperation performance — even in high-speed situations that were formerly impossible. In this paper, we demonstrate the conversion of dense geometry and appearance data, generated on-the-move by a mobile robot, into a photorealistic rendering model that gives the user a synthetic exterior line-of-sight view of the robot, including the context of its surrounding terrain. This technique converts teleoperation into virtual line-of-sight remote control. The underlying metrically consistent environment model also introduces the capacity to remove latency and enhance video compression. Display quality is sufficiently high that the user experience is similar to a driving video game where the surfaces used are textured with live video.},
	number = {3},
	journal = {The International Journal of Robotics Research},
	author = {Kelly, Alonzo and Chan, Nicholas and Herman, Herman and Huber, Daniel and Meyers, Robert and Rander, Pete and Warner, Randy and Ziglar, Jason and Capstick, Erin},
	year = {2011},
	note = {\_eprint: https://doi.org/10.1177/0278364910383724},
	pages = {384--404},
	file = {Kelly et al. - 2011 - Real-time photorealistic virtualized reality inter.pdf:C\:\\Users\\wb619\\Zotero\\storage\\AWFLJ3K7\\Kelly et al. - 2011 - Real-time photorealistic virtualized reality inter.pdf:application/pdf},
}

@inproceedings{hu_-line_2015,
	address = {Seattle, WA, USA},
	title = {On-line reconstruction based predictive display in unknown environment},
	doi = {10.1109/ICRA.2015.7139814},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Hu, Huan and Quintero, Camilo Perez and Sun, Hanxu and Jagersand, Martin},
	month = may,
	year = {2015},
	pages = {4446--4451},
	file = {Hu et al. - 2015 - On-line reconstruction based predictive display in.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6SCFMLPL\\Hu et al. - 2015 - On-line reconstruction based predictive display in.pdf:application/pdf},
}

@inproceedings{hofbauer_telecarla_2020,
	address = {Las Vegas, NV, USA},
	title = {{TELECARLA}: {An} {Open} {Source} {Extension} of the {CARLA} {Simulator} for {Teleoperated} {Driving} {Research} {Using} {Off}-the-{Shelf} {Components}},
	doi = {10.1109/IV47402.2020.9304676},
	booktitle = {{IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Hofbauer, Markus and Kuhn, Christopher B. and Petrovic, Goran and Steinbach, Eckehard},
	month = oct,
	year = {2020},
	pages = {335--340},
	file = {Hofbauer et al. - 2020 - TELECARLA An Open Source Extension of the CARLA S.pdf:C\:\\Users\\wb619\\Zotero\\storage\\TFFNEZRU\\Hofbauer et al. - 2020 - TELECARLA An Open Source Extension of the CARLA S.pdf:application/pdf},
}

@inproceedings{xie_generative_2021,
	address = {Xi'an, China},
	title = {A {Generative} {Model}-{Based} {Predictive} {Display} for {Robotic} {Teleoperation}},
	doi = {10.1109/ICRA48506.2021.9561787},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Xie, Bowen and Han, Mingjie and Jin, Jun and Barczyk, Martin and Jägersand, Martin},
	month = may,
	year = {2021},
	pages = {2407--2413},
	file = {Xie et al. - 2021 - A Generative Model-Based Predictive Display for Ro.pdf:C\:\\Users\\wb619\\Zotero\\storage\\QKPJPVY8\\Xie et al. - 2021 - A Generative Model-Based Predictive Display for Ro.pdf:application/pdf},
}

@inproceedings{marc_generator_2012,
	address = {Anchorage, AK, USA},
	title = {Generator of road marking textures and associated ground truth applied to the evaluation of road marking detection},
	doi = {10.1109/ITSC.2012.6338773},
	booktitle = {{IEEE} {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Marc, Revilloud and Dominique, Gruyer and Evangeline, Pollard},
	month = sep,
	year = {2012},
	pages = {933--938},
	file = {Marc et al. - 2012 - Generator of road marking textures and associated .pdf:C\:\\Users\\wb619\\Zotero\\storage\\8E9GL5LL\\Marc et al. - 2012 - Generator of road marking textures and associated .pdf:application/pdf},
}

@inproceedings{rohmer_v-rep_2013,
	address = {Tokyo, Japan},
	title = {V-{REP}: {A} versatile and scalable robot simulation framework},
	doi = {10.1109/IROS.2013.6696520},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Rohmer, Eric and Singh, Surya P. N. and Freese, Marc},
	month = nov,
	year = {2013},
	pages = {1321--1326},
	file = {Rohmer et al. - 2013 - V-REP A versatile and scalable robot simulation f.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4WDHX723\\Rohmer et al. - 2013 - V-REP A versatile and scalable robot simulation f.pdf:application/pdf},
}

@article{sturm_camera_2011,
	title = {Camera {Models} and {Fundamental} {Concepts} {Used} in {Geometric} {Computer} {Vision}},
	volume = {6},
	issn = {1572-2740},
	url = {http://dx.doi.org/10.1561/0600000023},
	doi = {10.1561/0600000023},
	number = {1–2},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	author = {Sturm, Peter and Ramalingam, Srikumar and Tardif, Jean-Philippe and Gasparini, Simone and Barreto, João},
	year = {2011},
	pages = {1--183},
}

@article{tsirikoglou_survey_2020,
	title = {A {Survey} of {Image} {Synthesis} {Methods} for {Visual} {Machine} {Learning}},
	volume = {39},
	url = {https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/abs/10.1111/cgf.14047},
	doi = {https://doi-org.ezproxy.library.wisc.edu/10.1111/cgf.14047},
	abstract = {Abstract Image synthesis designed for machine learning applications provides the means to efficiently generate large quantities of training data while controlling the generation process to provide the best distribution and content variety. With the demands of deep learning applications, synthetic data have the potential of becoming a vital component in the training pipeline. Over the last decade, a wide variety of training data generation methods has been demonstrated. The potential of future development calls to bring these together for comparison and categorization. This survey provides a comprehensive list of the existing image synthesis methods for visual machine learning. These are categorized in the context of image generation, using a taxonomy based on modelling and rendering, while a classification is also made concerning the computer vision applications they are used. We focus on the computer graphics aspects of the methods, to promote future image generation for machine learning. Finally, each method is assessed in terms of quality and reported performance, providing a hint on its expected learning potential. The report serves as a comprehensive reference, targeting both groups of the applications and data development sides. A list of all methods and papers reviewed herein can be found at https://computergraphics.on.liu.se/image\_synthesis\_methods\_for\_visual\_machine\_learning/.},
	number = {6},
	journal = {Computer Graphics Forum},
	author = {Tsirikoglou, A. and Eilertsen, G. and Unger, J.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/pdfdirect/10.1111/cgf.14047},
	keywords = {methods and applications},
	pages = {426--451},
	file = {Tsirikoglou et al. - 2020 - A Survey of Image Synthesis Methods for Visual Mac.pdf:C\:\\Users\\wb619\\Zotero\\storage\\2538MVN9\\Tsirikoglou et al. - 2020 - A Survey of Image Synthesis Methods for Visual Mac.pdf:application/pdf},
}

@article{berger_survey_2017,
	title = {A {Survey} of {Surface} {Reconstruction} from {Point} {Clouds}},
	volume = {36},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12802},
	doi = {10.1111/cgf.12802},
	abstract = {Abstract The area of surface reconstruction has seen substantial progress in the past two decades. The traditional problem addressed by surface reconstruction is to recover the digital representation of a physical shape that has been scanned, where the scanned data contain a wide variety of defects. While much of the earlier work has been focused on reconstructing a piece-wise smooth representation of the original shape, recent work has taken on more specialized priors to address significantly challenging data imperfections, where the reconstruction can take on different representations—not necessarily the explicit geometry. We survey the field of surface reconstruction, and provide a categorization with respect to priors, data imperfections and reconstruction output. By considering a holistic view of surface reconstruction, we show a detailed characterization of the field, highlight similarities between diverse reconstruction techniques and provide directions for future work in surface reconstruction.},
	number = {1},
	journal = {Computer Graphics Forum},
	author = {Berger, Matthew and Tagliasacchi, Andrea and Seversky, Lee M. and Alliez, Pierre and Guennebaud, Gaël and Levine, Joshua A. and Sharf, Andrei and Silva, Claudio T.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12802},
	keywords = {3D acquisition, geometry processin, shape analysis, surface reconstruction, real-world scenario into simulation},
	pages = {301--329},
	file = {Berger et al. - 2017 - A Survey of Surface Reconstruction from Point Clou.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4SLDYBM4\\Berger et al. - 2017 - A Survey of Surface Reconstruction from Point Clou.pdf:application/pdf},
}

@article{smelik_survey_2014,
	title = {A {Survey} on {Procedural} {Modelling} for {Virtual} {Worlds}},
	volume = {33},
	url = {https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/abs/10.1111/cgf.12276},
	doi = {10.1111/cgf.12276},
	abstract = {Abstract Procedural modelling deals with (semi-)automatic content generation by means of a program or procedure. Among other advantages, its data compression and the potential to generate a large variety of detailed content with reduced human intervention, have made procedural modelling attractive for creating virtual environments increasingly used in movies, games and simulations. We survey procedural methods that are useful to generate features of virtual worlds, including terrains, vegetation, rivers, roads, buildings and entire cities. In this survey, we focus particularly on the degree of intuitive control and of interactivity offered by each procedural method, because these properties are instrumental for their typical users: designers and artists. We identify the most promising research results that have been recently achieved, but we also realize that there is far from widespread acceptance of procedural methods among non-technical, creative professionals. We conclude by discussing some of the most important challenges of procedural modelling.},
	number = {6},
	journal = {Computer Graphics Forum},
	author = {Smelik, Ruben M. and Tutenel, Tim and Bidarra, Rafael and Benes, Bedrich},
	year = {2014},
	note = {\_eprint: https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/pdfdirect/10.1111/cgf.12276},
	keywords = {real-world scenario into simulation, Computer Graphics I3.5: Computational Geometry and Object Modelling—Geometric algorithms, languages, procedural content generation, procedural modeling methods, systems, virtual worlds},
	pages = {31--50},
	file = {Smelik et al. - 2014 - A Survey on Procedural Modelling for Virtual World.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8JSW8EJX\\Smelik et al. - 2014 - A Survey on Procedural Modelling for Virtual World.pdf:application/pdf},
}

@article{freiknecht_survey_2017,
	title = {A {Survey} on the {Procedural} {Generation} of {Virtual} {Worlds}},
	volume = {1},
	issn = {2414-4088},
	url = {https://www.mdpi.com/2414-4088/1/4/27},
	doi = {10.3390/mti1040027},
	abstract = {This survey presents algorithms for the automatic generation of content for virtual worlds, in particular for games. After a definition of the term procedural content generation, the algorithms to generate realistic objects such as landscapes and vegetation, road networks, buildings, living beings and stories are introduced in detail. In our discussion, we emphasize a good compromise between the realism of the objects and the performance of the algorithms. The survey also assesses each generated object type in terms of its applicability in games and simulations of virtual worlds.},
	number = {4},
	journal = {Multimodal Technologies and Interaction},
	author = {Freiknecht, Jonas and Effelsberg, Wolfgang},
	year = {2017},
	keywords = {real-world scenario into simulation},
	pages = {27},
	file = {Freiknecht and Effelsberg - 2017 - A Survey on the Procedural Generation of Virtual W.pdf:C\:\\Users\\wb619\\Zotero\\storage\\K9A4MLHM\\Freiknecht and Effelsberg - 2017 - A Survey on the Procedural Generation of Virtual W.pdf:application/pdf},
}

@inproceedings{kazhdan_poisson_2006,
	address = {Cagliari, Sardinia, Italy},
	series = {{SGP} '06},
	title = {Poisson {Surface} {Reconstruction}},
	isbn = {3-905673-36-3},
	abstract = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.},
	booktitle = {Proceedings of the {Eurographics} {Symposium} on {Geometry} {Processing}},
	publisher = {Eurographics Association},
	author = {Kazhdan, Michael and Bolitho, Matthew and Hoppe, Hugues},
	year = {2006},
	keywords = {real-world scenario into simulation},
	pages = {61--70},
	file = {Kazhdan et al. - 2006 - Poisson Surface Reconstruction.pdf:C\:\\Users\\wb619\\Zotero\\storage\\WF52MWVM\\Kazhdan et al. - 2006 - Poisson Surface Reconstruction.pdf:application/pdf},
}

@inproceedings{wang_pixel2mesh_2018,
	address = {Munich, Germany},
	title = {{Pixel2Mesh}: {Generating} {3D} {Mesh} {Models} from {Single} {RGB} {Images}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
	month = sep,
	year = {2018},
	pages = {52--67},
	file = {Wang et al. - 2018 - Pixel2Mesh Generating 3D Mesh Models from Single .pdf:C\:\\Users\\wb619\\Zotero\\storage\\RAB4T7K9\\Wang et al. - 2018 - Pixel2Mesh Generating 3D Mesh Models from Single .pdf:application/pdf},
}

@inproceedings{kato_neural_2018,
	address = {Salt Lake City, UT, USA},
	title = {Neural {3D} {Mesh} {Renderer}},
	doi = {10.1109/CVPR.2018.00411},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
	month = jun,
	year = {2018},
	pages = {3907--3916},
	file = {Kato et al. - 2018 - Neural 3D Mesh Renderer.pdf:C\:\\Users\\wb619\\Zotero\\storage\\8BSEEHV8\\Kato et al. - 2018 - Neural 3D Mesh Renderer.pdf:application/pdf},
}

@inproceedings{gkioxari_mesh_2019,
	address = {Seoul, South Korea},
	title = {Mesh {R}-{CNN}},
	doi = {10.1109/ICCV.2019.00988},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Gkioxari, Georgia and Johnson, Justin and Malik, Jitendra},
	month = oct,
	year = {2019},
	keywords = {real-world scenario into simulation},
	pages = {9784--9794},
	file = {Gkioxari et al. - 2019 - Mesh R-CNN.pdf:C\:\\Users\\wb619\\Zotero\\storage\\XVYGPQXX\\Gkioxari et al. - 2019 - Mesh R-CNN.pdf:application/pdf},
}

@article{bruneton_real-time_2012,
	title = {Real-time {Realistic} {Rendering} and {Lighting} of {Forests}},
	volume = {31},
	url = {https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/abs/10.1111/j.1467-8659.2012.03016.x},
	doi = {10.1111/j.1467-8659.2012.03016.x},
	abstract = {Abstract Realistic real-time rendering and lighting of forests is an important aspect for simulators and video games. This is a difficult problem, due to the massive amount of geometry: aerial forest views display millions of trees on a wide range of distances, from the camera to the horizon. Light interactions, whose effects are visible at all scales, are also a problem: sun and sky dome contributions, shadows between trees, inside trees, on the ground, and view-light masking correlations. In this paper we present a method to render very large forest scenes in real-time, with realistic lighting at all scales, and without popping nor aliasing. Our method is based on two new forest representations, z-fields and shader-maps, with a seamless transition between them. Our first model builds on light fields and height fields to represent and render the nearest trees individually, accounting for all lighting effects. Our second model is a location, view and light dependent shader mapped on the terrain, accounting for the cumulated subpixel effects. Qualitative comparisons with photos show that our method produces realistic results.},
	number = {2pt1},
	journal = {Computer Graphics Forum},
	author = {Bruneton, Eric and Neyret, Fabrice},
	year = {2012},
	note = {\_eprint: https://onlinelibrary-wiley-com.ezproxy.library.wisc.edu/doi/pdfdirect/10.1111/j.1467-8659.2012.03016.x},
	keywords = {real-world scenario into simulation, Computer Graphics I.3.7: Three-Dimensional Graphics and Realism—},
	pages = {373--382},
	file = {Bruneton and Neyret - 2012 - Real-time Realistic Rendering and Lighting of Fore.pdf:C\:\\Users\\wb619\\Zotero\\storage\\Z8H6ZE6J\\Bruneton and Neyret - 2012 - Real-time Realistic Rendering and Lighting of Fore.pdf:application/pdf},
}

@inproceedings{mcguire_real-time_2017,
	address = {San Francisco, CA, USA},
	series = {{I3D} '17},
	title = {Real-{Time} {Global} {Illumination} {Using} {Precomputed} {Light} {Field} {Probes}},
	isbn = {978-1-4503-4886-7},
	url = {https://doi-org.ezproxy.library.wisc.edu/10.1145/3023368.3023378},
	doi = {10.1145/3023368.3023378},
	abstract = {We introduce a new data structure and algorithms that employ it to compute real-time global illumination from static environments. Light field probes encode a scene's full light field and internal visibility. They extend current radiance and irradiance probe structures with per-texel visibility information similar to a G-buffer and variance shadow map. We apply ideas from screen-space and voxel cone tracing techniques to this data structure to efficiently sample radiance on world space rays, with correct visibility information, directly within pixel and compute shaders. From these primitives, we then design two GPU algorithms to efficiently gather real-time, viewer-dependent global illumination onto both static and dynamic objects. These algorithms make different tradeoffs between performance and accuracy. Supplemental GLSL source code is included.},
	booktitle = {{ACM} {SIGGRAPH} {Symposium} on {Interactive} {3D} {Graphics} and {Games}},
	publisher = {Association for Computing Machinery},
	author = {McGuire, Morgan and Mara, Mike and Nowrouzezahrai, Derek and Luebke, David},
	month = feb,
	year = {2017},
	note = {event-place: San Francisco, CA, USA},
	keywords = {global illumination, real-world scenario into simulation, irradiance, light field},
	file = {McGuire et al. - 2017 - Real-Time Global Illumination Using Precomputed Li.pdf:C\:\\Users\\wb619\\Zotero\\storage\\468JHEPA\\McGuire et al. - 2017 - Real-Time Global Illumination Using Precomputed Li.pdf:application/pdf},
}

@article{gardner_learning_2017,
	title = {Learning to {Predict} {Indoor} {Illumination} from a {Single} {Image}},
	volume = {36},
	issn = {0730-0301},
	url = {https://doi-org.ezproxy.library.wisc.edu/10.1145/3130800.3130891},
	doi = {10.1145/3130800.3130891},
	abstract = {We propose an automatic method to infer high dynamic range illumination from a single, limited field-of-view, low dynamic range photograph of an indoor scene. In contrast to previous work that relies on specialized image capture, user input, and/or simple scene models, we train an end-to-end deep neural network that directly regresses a limited field-of-view photo to HDR illumination, without strong assumptions on scene geometry, material properties, or lighting. We show that this can be accomplished in a three step process: 1) we train a robust lighting classifier to automatically annotate the location of light sources in a large dataset of LDR environment maps, 2) we use these annotations to train a deep neural network that predicts the location of lights in a scene from a single limited field-of-view photo, and 3) we fine-tune this network using a small dataset of HDR environment maps to predict light intensities. This allows us to automatically recover high-quality HDR illumination estimates that significantly outperform previous state-of-the-art methods. Consequently, using our illumination estimates for applications like 3D object insertion, produces photo-realistic results that we validate via a perceptual user study.},
	number = {6},
	journal = {ACM Transactions on Graphics},
	author = {Gardner, Marc-André and Sunkavalli, Kalyan and Yumer, Ersin and Shen, Xiaohui and Gambaretto, Emiliano and Gagné, Christian and Lalonde, Jean-François},
	year = {2017},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {deep learning, real-world scenario into simulation, indoor illumination},
	pages = {1--14},
	file = {Gardner et al. - 2017 - Learning to Predict Indoor Illumination from a Sin.pdf:C\:\\Users\\wb619\\Zotero\\storage\\YPTIFDS2\\Gardner et al. - 2017 - Learning to Predict Indoor Illumination from a Sin.pdf:application/pdf},
}

@inproceedings{lee_deep_2021,
	address = {Montreal, QC, Canada},
	title = {Deep {Hough} {Voting} for {Robust} {Global} {Registration}},
	doi = {10.1109/ICCV48922.2021.01569},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Lee, Junha and Kim, Seungwook and Cho, Minsu and Park, Jaesik},
	month = oct,
	year = {2021},
	keywords = {real-world scenario into simulation},
	pages = {15974--15983},
	file = {Lee et al. - 2021 - Deep Hough Voting for Robust Global Registration.pdf:C\:\\Users\\wb619\\Zotero\\storage\\6IRBE79N\\Lee et al. - 2021 - Deep Hough Voting for Robust Global Registration.pdf:application/pdf},
}

@inproceedings{wei_state_2009,
	address = {Munich, Germany},
	title = {State of the {Art} in {Example}-based {Texture} {Synthesis}},
	url = {https://inria.hal.science/inria-00606853},
	booktitle = {Eurographics},
	publisher = {Eurographics Association},
	author = {Wei, Li-Yi and Lefebvre, Sylvain and Kwatra, Vivek and Turk, Greg},
	month = mar,
	year = {2009},
	keywords = {real-time rendering, flow, fluids, geometry, globally varying, inverse synthesis, optimization, parallel computing, patch, pixel, solid, super-resolution, surface, texture mapping, texture synthesis, video},
	pages = {93--117},
	file = {Wei et al. - 2009 - State of the Art in Example-based Texture Synthesi.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LXZDDDNM\\Wei et al. - 2009 - State of the Art in Example-based Texture Synthesi.pdf:application/pdf},
}

@article{weinhaus_texture_1997,
	title = {Texture {Mapping} {3D} {Models} of {Real}-{World} {Scenes}},
	volume = {29},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/267580.267583},
	doi = {10.1145/267580.267583},
	abstract = {Texture mapping has become a popular tool in the computer graphics industry in the last few years because it is an easy way to achieve a high degree of realism in computer-generated imagery with very little effort. Over the last decade, texture-mapping techniques have advanced to the point where it is possible to generate real-time perspective simulations of real-world areas by texture mapping every object surface with texture from photographic images of these real-world areas. The techniques for generating such perspective transformations are variations on traditional texture mapping that in some circles have become known as the Image Perspective Transformation or IPT technology. This article first presents a background survey of traditional texture mapping. It then continues with a description of the texture-mapping variations that achieve these perspective transformations of photographic images of real-world scenes. The style of the presentation is that of a resource survey rather thatn an in-depth analysis.},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Weinhaus, Frederick M. and Devarajan, Venkat},
	year = {1997},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {texture mapping, anti-aliasing, height field, homogeneous coordinates, image perspective transformation, image warping, multiresolution data, perspective projection, polygons, ray tracing, real-time scene generation, rectification, registration, visual simulators, voxels},
	pages = {325--365},
	file = {Weinhaus and Devarajan - 1997 - Texture Mapping 3D Models of Real-World Scenes.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5KQRKASB\\Weinhaus and Devarajan - 1997 - Texture Mapping 3D Models of Real-World Scenes.pdf:application/pdf},
}

@inproceedings{xu_general_2010,
	address = {Hong Kong, China},
	title = {A general texture mapping framework for image-based {3D} modeling},
	doi = {10.1109/ICIP.2010.5653003},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Xu, Lin and Li, Eric and Li, Jianguo and Chen, Yurong and Zhang, Yimin},
	month = sep,
	year = {2010},
	pages = {2713--2716},
	file = {Xu et al. - 2010 - A general texture mapping framework for image-base.pdf:C\:\\Users\\wb619\\Zotero\\storage\\AVL8RPRC\\Xu et al. - 2010 - A general texture mapping framework for image-base.pdf:application/pdf},
}

@article{dupuy_adaptive_2018,
	title = {An {Adaptive} {Parameterization} for {Efficient} {Material} {Acquisition} and {Rendering}},
	volume = {37},
	issn = {0730-0301},
	url = {https://doi-org.ezproxy.library.wisc.edu/10.1145/3272127.3275059},
	doi = {10.1145/3272127.3275059},
	abstract = {One of the key ingredients of any physically based rendering system is a detailed specification characterizing the interaction of light and matter of all materials present in a scene, typically via the Bidirectional Reflectance Distribution Function (BRDF). Despite their utility, access to real-world BRDF datasets remains limited: this is because measurements involve scanning a four-dimensional domain at sufficient resolution, a tedious and often infeasibly time-consuming process.We propose a new parameterization that automatically adapts to the behavior of a material, warping the underlying 4D domain so that most of the volume maps to regions where the BRDF takes on non-negligible values, while irrelevant regions are strongly compressed. This adaptation only requires a brief 1D or 2D measurement of the material's retro-reflective properties. Our parameterization is unified in the sense that it combines several steps that previously required intermediate data conversions: the same mapping can simultaneously be used for BRDF acquisition, storage, and it supports efficient Monte Carlo sample generation.We observe that the above desiderata are satisfied by a core operation present in modern rendering systems, which maps uniform variates to direction samples that are proportional to an analytic BRDF. Based on this insight, we define our adaptive parameterization as an invertible, retro-reflectively driven mapping between the parametric and directional domains. We are able to create noise-free renderings of existing BRDF datasets after conversion into our representation with the added benefit that the warped data is significantly more compact, requiring 16KiB and 544KiB per spectral channel for isotropic and anisotropic specimens, respectively.Finally, we show how to modify an existing gonio-photometer to provide the needed retro-reflection measurements. Acquisition then proceeds within a 4D space that is warped by our parameterization. We demonstrate the efficacy of this scheme by acquiring the first set of spectral BRDFs of surfaces exhibiting arbitrary roughness, including anisotropy.},
	number = {6},
	journal = {ACM Transactions on Graphics},
	author = {Dupuy, Jonathan and Jakob, Wenzel},
	month = dec,
	year = {2018},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {microfacet theory, photorealistic rendering, spectral BRDF},
	pages = {274:1--274:14},
	file = {Dupuy and Jakob - 2018 - An Adaptive Parameterization for Efficient Materia.pdf:C\:\\Users\\wb619\\Zotero\\storage\\PWQS5JAV\\Dupuy and Jakob - 2018 - An Adaptive Parameterization for Efficient Materia.pdf:application/pdf},
}

@inproceedings{chen_feedback_2019,
	address = {Hong Kong, China},
	title = {A {Feedback} {Force} {Controller} {Fusing} {Traditional} {Control} and {Reinforcement} {Learning} {Strategies}},
	doi = {10.1109/AIM.2019.8868711},
	booktitle = {{IEEE}/{ASME} {International} {Conference} on {Advanced} {Intelligent} {Mechatronics} ({AIM})},
	author = {Chen, Bo-Hsun and Wang, Yu-Hsun and Lin, Pei-Chun},
	month = jul,
	year = {2019},
	pages = {259--265},
	file = {Chen et al. - 2019 - A Feedback Force Controller Fusing Traditional Con.pdf:C\:\\Users\\wb619\\Zotero\\storage\\258GMRNU\\Chen et al. - 2019 - A Feedback Force Controller Fusing Traditional Con.pdf:application/pdf},
}

@article{yang_dual-arm_2022,
	title = {A dual-arm manipulation strategy using position/force errors and {Kalman} filter},
	volume = {44},
	url = {https://doi.org/10.1177/01423312211018681},
	doi = {10.1177/01423312211018681},
	abstract = {This paper presents a new coordination manipulation strategy for a custom-made dual-arm robot. With master and slave coordination infrastructure, both spatial relation and sense of touch are considered to hold an object stably. Given the known trajectory of the master arm, the slave arm fuses position and force commands by using the Kalman filter to yield optimal compensation amounts. The proposed strategy has been experimentally evaluated, and the results confirm that it was capable of dealing with fragile and flexible objects. In addition, the influence of the loop time of the digital controller on force control for this task was also investigated in mathematical and simulated ways. Furthermore, a series of experiments were designed to explore the effects that have influences on errors in force control. The main factors that affect force control error were analyzed.},
	number = {4},
	journal = {Transactions of the Institute of Measurement and Control},
	author = {Yang, Wu-Te and Chen, Bo-Hsun and Lin, Pei-Chun},
	year = {2022},
	note = {\_eprint: https://doi.org/10.1177/01423312211018681},
	pages = {820--834},
	file = {Yang et al. - 2022 - A dual-arm manipulation strategy using positionfo.pdf:C\:\\Users\\wb619\\Zotero\\storage\\LX8LEB4Z\\Yang et al. - 2022 - A dual-arm manipulation strategy using positionfo.pdf:application/pdf},
}

@inproceedings{isola_image--image_2017,
	address = {Honolulu, HI, USA},
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	doi = {10.1109/CVPR.2017.632},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = jul,
	year = {2017},
	keywords = {GAN},
	pages = {5967--5976},
	file = {Isola et al. - 2017 - Image-to-Image Translation with Conditional Advers.pdf:C\:\\Users\\wb619\\Zotero\\storage\\CTYCB6RZ\\Isola et al. - 2017 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf},
}

@inproceedings{bi_deep_2019,
	title = {Deep CG2Real: synthetic-to-real translation via image disentanglement},
	doi = {10.1109/ICCV.2019.00282},
	booktitle = {{IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Bi, Sai and Sunkavalli, Kalyan and Perazzi, Federico and Shechtman, Eli and Kim, Vladimir and Ramamoorthi, Ravi},
	year = {2019},
	pages = {2730--2739},
	file = {Bi et al. - 2019 - Deep CG2Real Synthetic-to-Real Translation via Im.pdf:C\:\\Users\\wb619\\Zotero\\storage\\4SZZAZM2\\Bi et al. - 2019 - Deep CG2Real Synthetic-to-Real Translation via Im.pdf:application/pdf},
}

@article{battiato_high_2001,
	title = {High dynamic range imaging: {Overview} and application},
	volume = {2},
	number = {2},
	journal = {ST Journal of System Research},
	author = {Battiato, Sebastiano and Castorina, Alfio},
	year = {2001},
	keywords = {exposure},
	pages = {1--18},
	file = {Battiato and Castorina - 2001 - High dynamic range imaging Overview and applicati.pdf:C\:\\Users\\wb619\\Zotero\\storage\\VARIISJG\\Battiato and Castorina - 2001 - High dynamic range imaging Overview and applicati.pdf:application/pdf},
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	doi = {10.1177/0278364913491297},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10–100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	number = {11},
	journal = {The International Journal of Robotics Research},
	author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
	year = {2013},
	pages = {1231--1237},
	file = {Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf:/home/bohsun/Zotero/storage/JLKM8EBH/Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf:application/pdf},
}

@article{tzutalin_labelimg_2015,
	title = {{LabelImg}},
	url = {https://github.com/tzutalin/labelImg},
	journal = {GitHub repository},
	author = {{Tzutalin}},
	year = {2015},
}

@inproceedings{huang_apolloscape_2018,
	address = {Salt Lake City, UT, USA},
	title = {The {ApolloScape} {Dataset} for {Autonomous} {Driving}},
	doi = {10.1109/CVPRW.2018.00141},
	booktitle = {{IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Workshops}},
	author = {Huang, Xinyu and Cheng, Xinjing and Geng, Qichuan and Cao, Binbin and Zhou, Dingfu and Wang, Peng and Lin, Yuanqing and Yang, Ruigang},
	month = jun,
	year = {2018},
	pages = {1067--1073},
	file = {Huang et al. - 2018 - The ApolloScape Dataset for Autonomous Driving:/home/bohsun/Zotero/storage/WZ2NMU62/Huang et al. - 2018 - The ApolloScape Dataset for Autonomous Driving:application/pdf},
}

@inproceedings{muller_photorealistic_2021,
	address = {Prague, Czech Republic},
	title = {A {Photorealistic} {Terrain} {Simulation} {Pipeline} for {Unstructured} {Outdoor} {Environments}},
	doi = {10.1109/IROS51168.2021.9636644},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Müller, M. G. and Durner, M. and Gawel, A. and Stürzl, W. and Triebel, R. and Siegwart, R.},
	month = sep,
	year = {2021},
	pages = {9765--9772},
	file = {Müller et al. - 2021 - A Photorealistic Terrain Simulation Pipeline for U.pdf:/home/bohsun/Zotero/storage/UTHJZ9U6/Müller et al. - 2021 - A Photorealistic Terrain Simulation Pipeline for U.pdf:application/pdf},
}

@inproceedings{parkes_planet_2004,
	address = {Montreal, Quebec, Canada},
	title = {Planet {Surface} {Simulation} with {PANGU}},
	doi = {10.2514/6.2004-592-389},
	booktitle = {Space {OPS} 2004 {Conference}},
	author = {Parkes, S. M. and Martin, I. and Dunstan, M. and Matthews, D.},
	month = may,
	year = {2004},
	file = {Parkes et al. - 2004 - Planet Surface Simulation with PANGU.pdf:/home/bohsun/Zotero/storage/Q2PC4INX/Parkes et al. - 2004 - Planet Surface Simulation with PANGU.pdf:application/pdf},
}

@article{brochard_scientific_2018,
	title = {Scientific image rendering for space scenes with the {SurRender} software},
	journal = {arXiv:1810.01423},
	author = {Brochard, Roland and Lebreton, Jérémy and Robin, Cyril and Kanani, Keyvan and Jonniaux, Grégory and Masson, Aurore and Despré, Noela and Berjaoui, Ahmad},
	year = {2018},
	file = {Brochard et al. - 2018 - Scientific image rendering for space scenes with t.pdf:/home/bohsun/Zotero/storage/95HUSWLH/Brochard et al. - 2018 - Scientific image rendering for space scenes with t.pdf:application/pdf},
}


@inproceedings{vayugundla_datasets_2018,
	address = {Munich, Germany},
	title = {Datasets of {Long} {Range} {Navigation} {Experiments} in a {Moon} {Analogue} {Environment} on {Mount} {Etna}},
	booktitle = {International {Symposium} on {Robotics} ({ISR})},
	author = {Vayugundla, Mallikarjuna and Steidle, Florian and Smisek, Michal and Schuster, Martin J. and Bussmann, Kristin and Wedler, Armin},
	month = jun,
	year = {2018},
	pages = {77--83},
	file = {Vayugundla et al. - 2018 - Datasets of Long Range Navigation Experiments in a.pdf:/home/bohsun/Zotero/storage/5456MGKE/Vayugundla et al. - 2018 - Datasets of Long Range Navigation Experiments in a.pdf:application/pdf},
}

@article{meyer_madmax_2021,
	title = {The {MADMAX} data set for visual-inertial rover navigation on {Mars}},
	volume = {38},
	doi = {10.1002/rob.22016},
	number = {6},
	journal = {Journal of Field Robotics},
	author = {Meyer, Lukas and Smíšek, Michal and Fontan Villacampa, Alejandro and Oliva Maza, Laura and Medina, Daniel and Schuster, Martin J. and Steidle, Florian and Vayugundla, Mallikarjuna and Müller, Marcus G. and Rebele, Bernhard and Wedler, Armin and Triebel, Rudolph},
	year = {2021},
	keywords = {exploration, extreme environments, navigation, planetary robotics, SLAM},
	pages = {833--853},
	file = {Meyer et al. - 2021 - The MADMAX data set for visual-inertial rover navi.pdf:/home/bohsun/Zotero/storage/DZND7525/Meyer et al. - 2021 - The MADMAX data set for visual-inertial rover navi.pdf:application/pdf},
}

@article{wagstaff_deep_2018,
	title = {Deep {Mars}: {CNN} {Classification} of {Mars} {Imagery} for the {PDS} {Imaging} {Atlas}},
	volume = {32},
	doi = {10.1609/aaai.v32i1.11404},
	number = {1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wagstaff, Kiri and Lu, You and Stanboli, Alice and Grimes, Kevin and Gowda, Thamme and Padams, Jordan},
	year = {2018},
	file = {Wagstaff et al. - 2018 - Deep Mars CNN Classification of Mars Imagery for .pdf:/home/bohsun/Zotero/storage/CBZ5QT2B/Wagstaff et al. - 2018 - Deep Mars CNN Classification of Mars Imagery for .pdf:application/pdf},
}

@article{nasa_deep_2020,
	title = {{Deep Star Maps}},
	url = {https://svs.gsfc.nasa.gov/4851},
	author = {{NASA/Goddard Space Flight Center Scientific Visualization Studio}},
	year = {2020},
}

@article{tewari_state_2020,
	title = {State of the {Art} on {Neural} {Rendering}},
	volume = {39},
	doi = {10.1111/cgf.14022},
	abstract = {Abstract Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministic synthesis. The second half of this state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Tewari, A. and Fried, O. and Thies, J. and Sitzmann, V. and Lombardi, S. and Sunkavalli, K. and Martin-Brualla, R. and Simon, T. and Saragih, J. and Nießner, M. and Pandey, R. and Fanello, S. and Wetzstein, G. and Zhu, J.-Y. and Theobalt, C. and Agrawala, M. and Shechtman, E. and Goldman, D. B and Zollhöfer, M.},
	year = {2020},
	pages = {701--727},
	file = {Tewari et al. - 2020 - State of the Art on Neural Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5YAKYG62\\Tewari et al. - 2020 - State of the Art on Neural Rendering.pdf:application/pdf},
}

@article{tewari_advances_2022,
	title = {Advances in {Neural} {Rendering}},
	volume = {41},
	doi = {10.1111/cgf.14507},
	abstract = {Abstract Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects and scene editing and composition. While most of these approaches are scene-specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state-of-the-art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Tewari, A. and Thies, J. and Mildenhall, B. and Srinivasan, P. and Tretschk, E. and Yifan, W. and Lassner, C. and Sitzmann, V. and Martin-Brualla, R. and Lombardi, S. and Simon, T. and Theobalt, C. and Nießner, M. and Barron, J. T. and Wetzstein, G. and Zollhöfer, M. and Golyanik, V.},
	year = {2022},
	pages = {703--735},
	file = {Tewari et al. - 2022 - Advances in Neural Rendering.pdf:C\:\\Users\\wb619\\Zotero\\storage\\YXKQHS6W\\Tewari et al. - 2022 - Advances in Neural Rendering.pdf:application/pdf},
}

@article{reed_neural_2023,
	title = {Neural {Volumetric} {Reconstruction} for {Coherent} {Synthetic} {Aperture} {Sonar}},
	volume = {42},
	issn = {0730-0301},
	doi = {10.1145/3592141},
	abstract = {Synthetic aperture sonar (SAS) measures a scene from multiple views in order to increase the resolution of reconstructed imagery. Image reconstruction methods for SAS coherently combine measurements to focus acoustic energy onto the scene. However, image formation is typically under-constrained due to a limited number of measurements and bandlimited hardware, which limits the capabilities of existing reconstruction methods. To help meet these challenges, we design an analysis-by-synthesis optimization that leverages recent advances in neural rendering to perform coherent SAS imaging. Our optimization enables us to incorporate physics-based constraints and scene priors into the image formation process. We validate our method on simulation and experimental results captured in both air and water. We demonstrate both quantitatively and qualitatively that our method typically produces superior reconstructions than existing approaches. We share code and data for reproducibility.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Reed, Albert and Kim, Juhyeon and Blanford, Thomas and Pediredla, Adithya and Brown, Daniel and Jayasuriya, Suren},
	month = jul,
	year = {2023},
	keywords = {implicit neural representation, synthetic aperture sonar},
	pages = {1--20},
	file = {Reed et al. - 2023 - Neural Volumetric Reconstruction for Coherent Synt.pdf:C\:\\Users\\wb619\\Zotero\\storage\\PU2KFZBT\\Reed et al. - 2023 - Neural Volumetric Reconstruction for Coherent Synt.pdf:application/pdf},
}


@inproceedings{huang_nerf-texture_2023,
	address = {Los Angeles, CA, USA},
	title = {{NeRF}-{Texture}: {Texture} {Synthesis} with {Neural} {Radiance} {Fields}},
	isbn = {9798400701597},
	doi = {10.1145/3588432.3591484},
	abstract = {Texture synthesis is a fundamental problem in computer graphics that would benefit various applications. Existing methods are effective in handling 2D image textures. In contrast, many real-world textures contain meso-structure in the 3D geometry space, such as grass, leaves, and fabrics, which cannot be effectively modeled using only 2D image textures. We propose a novel texture synthesis method with Neural Radiance Fields (NeRF) to capture and synthesize textures from given multi-view images. In the proposed NeRF texture representation, a scene with fine geometric details is disentangled into the meso-structure textures and the underlying base shape. This allows textures with meso-structure to be effectively learned as latent features situated on the base shape, which are fed into a NeRF decoder trained simultaneously to represent the rich view-dependent appearance. Using this implicit representation, we can synthesize NeRF-based textures through patch matching of latent features. However, inconsistencies between the metrics of the reconstructed content space and the latent feature space may compromise the synthesis quality. To enhance matching performance, we further regularize the distribution of latent features by incorporating a clustering constraint. Experimental results and evaluations demonstrate the effectiveness of our approach.},
	booktitle = {{ACM} {SIGGRAPH} {Conference} {Proceedings}},
	author = {Huang, Yi-Hua and Cao, Yan-Pei and Lai, Yu-Kun and Shan, Ying and Gao, Lin},
	year = {2023},
	month = aug,
	keywords = {meso-structure texture, Neural radiance fields, texture synthesis},
	file = {Full Text:C\:\\Users\\wb619\\Zotero\\storage\\U2LTT9VH\\Huang et al. - 2023 - NeRF-Texture Texture Synthesis with Neural Radian.pdf:application/pdf},
}

@inproceedings{yariv_bakedsdf_2023,
	address = {Los Angeles, CA, USA},
	title = {{BakedSDF}: {Meshing} {Neural} {SDFs} for {Real}-{Time} {View} {Synthesis}},
	isbn = {9798400701597},
	doi = {10.1145/3588432.3591536},
	abstract = {We present a method for reconstructing high-quality meshes of large unbounded real-world scenes suitable for photorealistic novel view synthesis. We first optimize a hybrid neural volume-surface scene representation designed to have well-behaved level sets that correspond to surfaces in the scene. We then bake this representation into a high-quality triangle mesh, which we equip with a simple and fast view-dependent appearance model based on spherical Gaussians. Finally, we optimize this baked representation to best reproduce the captured viewpoints, resulting in a model that can leverage accelerated polygon rasterization pipelines for real-time view synthesis on commodity hardware. Our approach outperforms previous scene representations for real-time rendering in terms of accuracy, speed, and power consumption, and produces high quality meshes that enable applications such as appearance editing and physical simulation.},
	booktitle = {{ACM} {SIGGRAPH} {Conference} {Proceedings}},
	author = {Yariv, Lior and Hedman, Peter and Reiser, Christian and Verbin, Dor and Srinivasan, Pratul P. and Szeliski, Richard and Barron, Jonathan T. and Mildenhall, Ben},
	month = aug,
	year = {2023},
	keywords = {Deep Learning., Image Synthesis, Neural Radiance Fields, Real-Time Rendering, Signed Distance Function, Surface Reconstruction},
	file = {Full Text:C\:\\Users\\wb619\\Zotero\\storage\\XFHVBBIJ\\Yariv et al. - 2023 - BakedSDF Meshing Neural SDFs for Real-Time View S.pdf:application/pdf},
}

@inproceedings{tancik_nerfstudio_2023,
	address = {Los Angeles, CA, USA},
	title = {Nerfstudio: {A} {Modular} {Framework} for {Neural} {Radiance} {Field} {Development}},
	isbn = {9798400701597},
	doi = {10.1145/3588432.3591516},
	abstract = {Neural Radiance Fields (NeRF) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of NeRF research, we propose a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play components for implementing NeRF-based methods, which make it easy for researchers and practitioners to incorporate NeRF into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing.},
	booktitle = {{ACM} {SIGGRAPH} {Conference} {Proceedings}},
	author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and Mcallister, David and Kerr, Justin and Kanazawa, Angjoo},
	month = aug,
	year = {2023},
	keywords = {3D reconstruction, computer graphics, computer vision, machine learning, multi-view, neural rendering},
	file = {Full Text:C\:\\Users\\wb619\\Zotero\\storage\\W84L9UXJ\\Tancik et al. - 2023 - Nerfstudio A Modular Framework for Neural Radianc.pdf:application/pdf},
}

@inproceedings{wu_-nerf_2023,
	address = {Los Angeles, CA, USA},
	title = {{DE}-{NeRF}: {DEcoupled} {Neural} {Radiance} {Fields} for {View}-{Consistent} {Appearance} {Editing} and {High}-{Frequency} {Environmental} {Relighting}},
	isbn = {9798400701597},
	doi = {10.1145/3588432.3591483},
	abstract = {Neural Radiance Fields (NeRF) have shown promising results in novel view synthesis. While achieving state-of-the-art rendering results, NeRF usually encodes all properties related to geometry and appearance of the scene together into several MLP (Multi-Layer Perceptron) networks, which hinders downstream manipulation of geometry, appearance and illumination. Recently researchers made attempts to edit geometry, appearance and lighting for NeRF. However, they fail to render view-consistent results after editing the appearance of the input scene. Moreover, high-frequency environmental relighting is also beyond their capability as lighting is modeled as Spherical Gaussian (SG) and Spherical Harmonic (SH) functions or a low-resolution environment map. To solve the above problems, we propose DE-NeRF to decouple view-independent appearance and view-dependent appearance in the scene with a hybrid lighting representation. Specifically, we first train a signed distance function to reconstruct an explicit mesh for the input scene. Then a decoupled NeRF learns to attach view-independent appearance to the reconstructed mesh by defining learnable disentangled features representing geometry and view-independent appearance on its vertices. For lighting, we approximate it with an explicit learnable environment map and an implicit lighting network to support both low-frequency and high-frequency relighting. By modifying the view-independent appearance, rendered results are consistent across different viewpoints. Our method also supports high-frequency environmental relighting by replacing the explicit environment map with a novel one and fitting the implicit lighting network to the novel environment map. Experiments show that our method achieves better editing and relighting performance both quantitatively and qualitatively compared to previous methods.},
	booktitle = {{ACM} {SIGGRAPH} {Conference} {Proceedings}},
	author = {Wu, Tong and Sun, Jia-Mu and Lai, Yu-Kun and Gao, Lin},
	month = aug,
	year = {2023},
	keywords = {editing, inverse rendering, neural radiance fields},
	file = {Full Text:C\:\\Users\\wb619\\Zotero\\storage\\JQ9GF7M3\\Wu et al. - 2023 - DE-NeRF DEcoupled Neural Radiance Fields for View.pdf:application/pdf},
}

@article{liu_nero_2023,
	title = {{NeRO}: {Neural} {Geometry} and {BRDF} {Reconstruction} of {Reflective} {Objects} from {Multiview} {Images}},
	volume = {42},
	issn = {0730-0301},
	doi = {10.1145/3592134},
	abstract = {We present a neural rendering-based method called NeRO for reconstructing the geometry and the BRDF of reflective objects from multiview images captured in an unknown environment. Multiview reconstruction of reflective objects is extremely challenging because specular reflections are view-dependent and thus violate the multiview consistency, which is the cornerstone for most multiview reconstruction methods. Recent neural rendering techniques can model the interaction between environment lights and the object surfaces to fit the view-dependent reflections, thus making it possible to reconstruct reflective objects from multiview images. However, accurately modeling environment lights in the neural rendering is intractable, especially when the geometry is unknown. Most existing neural rendering methods, which can model environment lights, only consider direct lights and rely on object masks to reconstruct objects with weak specular reflections. Therefore, these methods fail to reconstruct reflective objects, especially when the object mask is not available and the object is illuminated by indirect lights. We propose a two-step approach to tackle this problem. First, by applying the split-sum approximation and the integrated directional encoding to approximate the shading effects of both direct and indirect lights, we are able to accurately reconstruct the geometry of reflective objects without any object masks. Then, with the object geometry fixed, we use more accurate sampling to recover the environment lights and the BRDF of the object. Extensive experiments demonstrate that our method is capable of accurately reconstructing the geometry and the BRDF of reflective objects from only posed RGB images without knowing the environment lights and the object masks. Codes and datasets are available at https://github.com/liuyuan-pal/NeRO.},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Liu, Yuan and Wang, Peng and Lin, Cheng and Long, Xiaoxiao and Wang, Jiepeng and Liu, Lingjie and Komura, Taku and Wang, Wenping},
	month = jul,
	year = {2023},
	keywords = {neural rendering, neural representation, multiview reconstruction},
	pages = {1--22},
	file = {Liu et al. - 2023 - NeRO Neural Geometry and BRDF Reconstruction of R.pdf:C\:\\Users\\wb619\\Zotero\\storage\\5M7DERCP\\Liu et al. - 2023 - NeRO Neural Geometry and BRDF Reconstruction of R.pdf:application/pdf},
}

@inproceedings{zhou_photomat_2023,
	address = {Los Angeles, CA, USA},
	title = {{PhotoMat}: {A} {Material} {Generator} {Learned} from {Single} {Flash} {Photos}},
	isbn = {9798400701597},
	doi = {10.1145/3588432.3591535},
	abstract = {Authoring high-quality digital materials is key to realism in 3D rendering. Previous generative models for materials have been trained exclusively on synthetic data; such data is limited in availability and has a visual gap to real materials. We circumvent this limitation by proposing PhotoMat: the first material generator trained exclusively on real photos of material samples captured using a cell phone camera with flash. Supervision on individual material maps is not available in this setting. Instead, we train a generator for a neural material representation that is rendered with a learned relighting module to create arbitrarily lit RGB images; these are compared against real photos using a discriminator. We train PhotoMat with a new dataset of 12,000 material photos captured with handheld phone cameras under flash lighting. We demonstrate that our generated materials have better visual quality than previous material generators trained on synthetic data. Moreover, we can fit analytical material models to closely match these generated neural materials, thus allowing for further editing and use in 3D rendering.},
	booktitle = {{ACM} {SIGGRAPH} {Conference} {Proceedings}},
	author = {Zhou, Xilong and Hasan, Milos and Deschaintre, Valentin and Guerrero, Paul and Hold-Geoffroy, Yannick and Sunkavalli, Kalyan and Kalantari, Nima Khademi},
	month = aug,
	year = {2023},
	keywords = {GAN, generative models, Materials, SVBRDF},
	file = {Full Text:C\:\\Users\\wb619\\Zotero\\storage\\93KI5HR8\\Zhou et al. - 2023 - PhotoMat A Material Generator Learned from Single.pdf:application/pdf},
}
