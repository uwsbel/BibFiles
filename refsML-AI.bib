@misc{battaglia2018relational,
      title={Relational inductive biases, deep learning, and graph networks}, 
      author={Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
      year={2018},
      eprint={1806.01261},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{velickovic2018graph,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}

@misc{langchain,
	author       = {Harrison Chase},
	title        = {LangChain: Building applications with large language models},
	year         = {2022},
	howpublished = {\url{https://github.com/hwchase17/langchain}},
	note         = {Accessed: 2024-07-03}
}

@misc{babyagi,
	author       = {Yohei Nakajima},
	title        = {BabyAGI: Experimental autonomous agent},
	year         = {2023},
	howpublished = {\url{https://github.com/yoheinakajima/babyagi}},
	note         = {Accessed: 2024-07-03}
}

@inproceedings{hu2023planning,
  title     = {Planning Goals for Exploration},
  author    = {Edward S. Hu and Richard Chang and Oleh Rybkin and Dinesh Jayaraman},
  booktitle = {The Eleventh International Conference on Learning Representations },
  year      = {2023},
  url       = {https://openreview.net/forum?id=6qeBuZSo7Pr}
}

@article{Nagabandi2018LearningTA,
  title   = {Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning},
  author  = {Anusha Nagabandi and Ignasi Clavera and Simin Liu and Ronald S. Fearing and P. Abbeel and Sergey Levine and Chelsea Finn},
  journal = {arXiv: Learning},
  year    = {2018},
  url     = {https://api.semanticscholar.org/CorpusID:56475856}
}

@inproceedings{Zhang2020AdaptiveRM,
  title     = {Adaptive Risk Minimization: Learning to Adapt to Domain Shift},
  author    = {Marvin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},
  booktitle = {Neural Information Processing Systems},
  year      = {2020},
  url       = {https://api.semanticscholar.org/CorpusID:244772951}
}
@inproceedings{Pfrommer2020ContactNetsLO,
  title     = {ContactNets: Learning of Discontinuous Contact Dynamics with Smooth, Implicit Representations},
  author    = {Samuel Pfrommer and Mathew Halm and Michael Posa},
  booktitle = {Conference on Robot Learning},
  year      = {2020},
  url       = {https://api.semanticscholar.org/CorpusID:221857185}
}
@inproceedings{bianchini2023simultaneous,
  title     = {Simultaneous Learning of Contact and Continuous Dynamics},
  author    = {Bibit Bianchini and Mathew Halm and Michael Posa},
  booktitle = {7th Annual Conference on Robot Learning},
  year      = {2023},
  url       = {https://openreview.net/forum?id=-3G6_D66Aua}
}
@inproceedings{Toshev2024NeuralSI,
  title  = {Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics},
  author = {Artur P. Toshev and Jonas A. Erbesdobler and Nikolaus A. Adams and Johannes Brandstetter},
  year   = {2024},
  url    = {https://api.semanticscholar.org/CorpusID:267616845}
}

@article{Li2023MPMNetAD,
  title   = {MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction},
  author  = {Jin Li and Yang Gao and Ju Dai and Shuai Li and Aimin Hao and Hong Qin},
  journal = {IEEE transactions on visualization and computer graphics},
  year    = {2023},
  volume  = {PP},
  url     = {https://api.semanticscholar.org/CorpusID:258437596}
}
@inproceedings{li2023pacnerf,
  title     = {{PAC}-Ne{RF}: Physics Augmented Continuum Neural Radiance Fields for Geometry-Agnostic System Identification},
  author    = {Xuan Li and Yi-Ling Qiao and Peter Yichen Chen and Krishna Murthy Jatavallabhula and Ming Lin and Chenfanfu Jiang and Chuang Gan},
  booktitle = {The Eleventh International Conference on Learning Representations },
  year      = {2023},
  url       = {https://openreview.net/forum?id=tVkrbkz42vc}
}

@inproceedings{xu2021accelerated,
  title     = {Accelerated Policy Learning with Parallel Differentiable Simulation},
  author    = {Xu, Jie and Makoviychuk, Viktor and Narang, Yashraj and Ramos, Fabio and Matusik, Wojciech and Garg, Animesh and Macklin, Miles},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}

@article{BommasaniFoundationModels2021,
	title={On the Opportunities and Risks of Foundation Models},
	author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
	journal={arXiv:2108.07258},
	year={2021},
	url={https://crfm.stanford.edu/assets/report.pdf}
}
@techreport{Salak-2008,
  author      = {R. Salakhutdinov},
  title       = {Learning and Evaluating {B}oltzmann Machines},
  institution = {Department of Computer Science, University of Toronto},
  year        = {2008},
  month       = {June},
  number      = {UTML TR 2008-002}
}

@inproceedings{Schulz-2010,
  title     = {Exploiting Local Structure in Stacked {B}oltzmann Machines},
  author    = {Hannes Schulz and Andreas C. M\"{u}ller and Sven Behnke},
  booktitle = {ESANN},
  year      = {2010}
}

@misc{2023hafnermastering,
  title         = {Mastering Diverse Domains through World Models},
  author        = {Danijar Hafner and Jurgis Pasukonis and Jimmy Ba and Timothy Lillicrap},
  year          = {2023},
  eprint        = {2301.04104},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}

@misc{2023sukhijagradientbased,
  title         = {Gradient-Based Trajectory Optimization With Learned Dynamics},
  author        = {Bhavya Sukhija and Nathanael Köhler and Miguel Zamora and Simon Zimmermann and Sebastian Curi and Andreas Krause and Stelian Coros},
  year          = {2023},
  eprint        = {2204.04558},
  archiveprefix = {arXiv},
  primaryclass  = {cs.RO}
}

@article{2018LowreyPlanOL,
  title   = {Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control},
  author  = {Kendall Lowrey and Aravind Rajeswaran and Sham M. Kakade and Emanuel Todorov and Igor Mordatch},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1811.01848},
  url     = {https://api.semanticscholar.org/CorpusID:53216818}
}
@inproceedings{2019NagabandiDeepDM,
  title     = {Deep Dynamics Models for Learning Dexterous Manipulation},
  author    = {Anusha Nagabandi and Kurt Konolige and Sergey Levine and Vikash Kumar},
  booktitle = {Conference on Robot Learning},
  year      = {2019},
  url       = {https://api.semanticscholar.org/CorpusID:202750286}
}

@article{2023HuPlanningGF,
  title   = {Planning Goals for Exploration},
  author  = {Edward S. Hu and Richard Chang and Oleh Rybkin and Dinesh Jayaraman},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2303.13002},
  url     = {https://api.semanticscholar.org/CorpusID:257687184}
}

@article{2022PascalRLExcavation,
  year      = {2022},
  volume    = {7},
  type      = {Journal Article},
  journal   = {IEEE Robotics and Automation Letters},
  author    = {Egli, Pascal and Gaschen, Dominique and Kerscher, Simon and Jud, Dominic and Hutter, Marco},
  address   = {Piscataway, NJ},
  publisher = {IEEE},
  number    = {4},
  title     = {Soil-Adaptive Excavation Using {R}einforcement {L}earning},
  pages     = {9778 - 9785}
}

@inproceedings{2023CendikiaRLPickAndPlace,
  author    = {Ishmatuka, Cendikia and Soesanti, Indah and Ataka, Ahmad},
  booktitle = {2023 15th International Conference on Information Technology and Electrical Engineering (ICITEE)},
  title     = {Autonomous Pick-and-Place Using Excavator Based on Deep Reinforcement Learning},
  year      = {2023},
  pages     = {19-24},
  keywords  = {Electrical engineering;Process control;Reinforcement learning;Kinematics;Excavation;Trajectory;Task analysis;Excavator;Reinforcement Learning;Proximal Policy Optimization;Control},
  doi       = {10.1109/ICITEE59582.2023.10317662}
}

@article{2023TakayukiExcavationSampling,
  author   = {Osa, Takayuki and Osajima, Naoto and Aizawa, Masanori and Harada, Tatsuya},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {Learning Adaptive Policies for Autonomous Excavation Under Various Soil Conditions by Adversarial Domain Sampling},
  year     = {2023},
  volume   = {8},
  number   = {9},
  pages    = {5536-5543},
  keywords = {Task analysis;Excavation;Training;Soil;Metalearning;Trajectory;Reinforcement learning;Robotics and automation in construction;reinforcement learning;deep learning methods},
  doi      = {10.1109/LRA.2023.3296933}
}

@article{2024choithreedimensional,
  title   = {Graph Neural Network-based surrogate model for granular flows},
  journal = {Computers and Geotechnics},
  volume  = {166},
  pages   = {106015},
  year    = {2024},
  issn    = {0266-352X},
  doi     = {https://doi.org/10.1016/j.compgeo.2023.106015},
  url     = {https://www.sciencedirect.com/science/article/pii/S0266352X23007723},
  author  = {Yongjin Choi and Krishna Kumar}
}

@article{2023WhitneyLearning3P,
  title   = {Learning 3D Particle-based Simulators from RGB-D Videos},
  author  = {William F. Whitney and Tatiana Lopez-Guevara and Tobias Pfaff and Yulia Rubanova and Thomas Kipf and Kimberly L. Stachenfeld and Kelsey R. Allen},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2312.05359},
  url     = {https://api.semanticscholar.org/CorpusID:266163316}
}

@misc{2018sanchezgonzalezgraph,
  title         = {Graph networks as learnable physics engines for inference and control},
  author        = {Alvaro Sanchez-Gonzalez and Nicolas Heess and Jost Tobias Springenberg and Josh Merel and Martin Riedmiller and Raia Hadsell and Peter Battaglia},
  year          = {2018},
  eprint        = {1806.01242},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{2022ShiRoboCraftLT,
  title   = {RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks},
  author  = {Haochen Shi and Huazhe Xu and Zhiao Huang and Yunzhu Li and Jiajun Wu},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2205.02909},
  url     = {https://api.semanticscholar.org/CorpusID:248562698}
}

@article{2023KimBridgingAE,
  title   = {Bridging Active Exploration and Uncertainty-Aware Deployment Using Probabilistic Ensemble Neural Network Dynamics},
  author  = {Taekyung Kim and Jungwi Mun and Junwon Seo and Beomsu Kim and Seong II Hong},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2305.12240},
  url     = {https://api.semanticscholar.org/CorpusID:258833080}
}
@article{2018KurutachModelEnsembleTP,
  title   = {Model-Ensemble Trust-Region Policy Optimization},
  author  = {Thanard Kurutach and Ignasi Clavera and Yan Duan and Aviv Tamar and P. Abbeel},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1802.10592},
  url     = {https://api.semanticscholar.org/CorpusID:3536221}
}

@article{Salakhutdinov-AnnualReview-2015,
  author  = {Ruslan Salakhutdinov},
  title   = {Learning Deep Generative Models},
  journal = {Annual Review of Statistics and Its Application},
  volume  = {2},
  number  = {1},
  pages   = {361-385},
  year    = {2015},
  doi     = {10.1146/annurev-statistics-010814-020120},
  url     = {http://dx.doi.org/10.1146/annurev-statistics-010814-020120},
  eprint  = {http://dx.doi.org/10.1146/annurev-statistics-010814-020120}
}

@article{theano2016,
  author   = {\relax{Theano Development Team}},
  title    = {{Theano}: A {Python} framework for fast computation of mathematical expressions},
  journal  = {arXiv e-prints},
  volume   = {abs/1605.02688},
  keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Computer Science - Mathematical Software},
  year     = {2016},
  url      = {http://arxiv.org/abs/1605.02688}
}

@article{openAI-gym2016,
  author        = {Greg Brockman and
                   Vicki Cheung and
                   Ludwig Pettersson and
                   Jonas Schneider and
                   John Schulman and
                   Jie Tang and
                   Wojciech Zaremba},
  title         = {{OpenAI Gym}},
  journal       = {CoRR},
  volume        = {abs/1606.01540},
  year          = {2016},
  url           = {http://arxiv.org/abs/1606.01540},
  archiveprefix = {arXiv},
  eprint        = {1606.01540},
  timestamp     = {Mon, 13 Aug 2018 16:48:42 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/BrockmanCPSSTZ16},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{paszke2017PyTorch,
  title     = {Automatic differentiation in {PyTorch}},
  author    = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle = {NIPS 2017 Workshop Autodiff},
  year      = {2017},
  type      = {Conference Proceedings}
}

@article{paszke2019pytorch,
  title    = {Pytorch: An imperative style, high-performance deep learning library},
  author   = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal  = {Advances in neural information processing systems},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  url      = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  volume   = {32},
  year     = {2019}
}

@inproceedings{collobert2011torch7,
  title     = {Torch7: A matlab-like environment for machine learning},
  author    = {Collobert, Ronan and Kavukcuoglu, Koray and Farabet, Cl{\'e}ment},
  booktitle = {BigLearn, NIPS workshop},
  number    = {CONF},
  year      = {2011}
}

@article{kirkpatrickOptimizationAnnealing1983,
  title     = {Optimization by simulated annealing},
  author    = {Kirkpatrick, Scott and Gelatt, C Daniel and Vecchi, Mario P},
  journal   = {Science},
  volume    = {220},
  number    = {4598},
  pages     = {671--680},
  year      = {1983},
  publisher = {American association for the advancement of science}
}    

@article{bottouOptML2018,
  title     = {Optimization methods for large-scale machine learning},
  author    = {Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal   = {Siam Review},
  volume    = {60},
  number    = {2},
  pages     = {223--311},
  year      = {2018},
  publisher = {SIAM}
}

@incollection{bottou2010large,
  title     = {Large-scale machine learning with stochastic gradient descent},
  author    = {Bottou, L{\'e}on},
  booktitle = {Proceedings of COMPSTAT'2010},
  pages     = {177--186},
  year      = {2010},
  publisher = {Springer}
}

@article{dekkers1991global,
  title     = {Global optimization and simulated annealing},
  author    = {Dekkers, Anton and Aarts, Emile},
  journal   = {Mathematical programming},
  volume    = {50},
  number    = {1},
  pages     = {367--393},
  year      = {1991},
  publisher = {Springer}
}    
@book{bruntonML_in_control2019,
  title     = {Data-driven science and engineering: Machine learning, dynamical systems, and control},
  author    = {Brunton, Steven L and Kutz, J Nathan},
  year      = {2019},
  publisher = {Cambridge University Press}
}

@article{caffe2014,
  author  = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  journal = {arXiv preprint arXiv:1408.5093},
  title   = {{Caffe}: {C}onvolutional Architecture for Fast Feature Embedding},
  year    = {2014}
}

@article{Gao-Nature-2016,
  author    = {Gao, Jianxi
               and Barzel, Baruch
               and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  title     = {Universal Resilience Patterns in Complex Networks},
  journal   = {Nature},
  year      = {2016},
  month     = {Feb},
  day       = {18},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  volume    = {530},
  number    = {7590},
  pages     = {307-312},
  note      = {Letter},
  issn      = {0028-0836},
  url       = {http://dx.doi.org/10.1038/nature16948}
}

@article{Leemput-PNAS-2014,
  author   = {van de Leemput, Ingrid A. and Wichers, Marieke and Cramer, Angélique O. J. and Borsboom, Denny and Tuerlinckx, Francis and Kuppens, Peter and van Nes, Egbert H. and Viechtbauer, Wolfgang and Giltay, Erik J. and Aggen, Steven H. and Derom, Catherine and Jacobs, Nele and Kendler, Kenneth S. and van der Maas, Han L. J. and Neale, Michael C. and Peeters, Frenk and Thiery, Evert and Zachar, Peter and Scheffer, Marten},
  title    = {Critical Slowing Down as Early Warning for the Onset and Termination of Depression},
  volume   = {111},
  number   = {1},
  pages    = {87-92},
  year     = {2014},
  doi      = {10.1073/pnas.1312114110},
  abstract = {About 17% of humanity goes through an episode of major depression at some point in their lifetime. Despite the enormous societal costs of this incapacitating disorder, it is largely unknown how the likelihood of falling into a depressive episode can be assessed. Here, we show for a large group of healthy individuals and patients that the probability of an upcoming shift between a depressed and a normal state is related to elevated temporal autocorrelation, variance, and correlation between emotions in fluctuations of autorecorded emotions. These are indicators of the general phenomenon of critical slowing down, which is expected to occur when a system approaches a tipping point. Our results support the hypothesis that mood may have alternative stable states separated by tipping points, and suggest an approach for assessing the likelihood of transitions into and out of depression.},
  url      = {http://www.pnas.org/content/111/1/87.abstract},
  eprint   = {http://www.pnas.org/content/111/1/87.full.pdf},
  journal  = {Proceedings of the National Academy of Sciences}
}

@article{Dakos-PNAS-2014,
  author   = {Dakos, Vasilis and Bascompte, Jordi},
  title    = {Critical Slowing Down as Early Warning for the Onset of Collapse in Mutualistic Communities},
  volume   = {111},
  number   = {49},
  pages    = {17546-17551},
  year     = {2014},
  doi      = {10.1073/pnas.1406326111},
  abstract = {Tipping points are crossed when small changes in external conditions cause abrupt unexpected responses in the current state of a system. In the case of ecological communities under stress, the risk of approaching a tipping point is unknown, but its stakes are high. Here, we test recently developed critical slowing-down indicators as early-warning signals for detecting the proximity to a potential tipping point in structurally complex ecological communities. We use the structure of 79 empirical mutualistic networks to simulate a scenario of gradual environmental change that leads to an abrupt first extinction event followed by a sequence of species losses until the point of complete community collapse. We find that critical slowing-down indicators derived from time series of biomasses measured at the species and community level signal the proximity to the onset of community collapse. In particular, we identify specialist species as likely the best-indicator species for monitoring the proximity of a community to collapse. In addition, trends in slowing-down indicators are strongly correlated to the timing of species extinctions. This correlation offers a promising way for mapping species resilience and ranking species risk to extinction in a given community. Our findings pave the road for combining theory on tipping points with patterns of network structure that might prove useful for the management of a broad class of ecological networks under global environmental change.},
  url      = {http://www.pnas.org/content/111/49/17546.abstract},
  eprint   = {http://www.pnas.org/content/111/49/17546.full.pdf},
  journal  = {Proceedings of the National Academy of Sciences}
}

@article{Scheffer-Science-2012,
  author    = {Scheffer, Marten and Carpenter, Stephen R. and Lenton, Timothy M. and Bascompte, Jordi and Brock, William and Dakos, Vasilis and van de Koppel, Johan and van de Leemput, Ingrid A. and Levin, Simon A. and van Nes, Egbert H. and Pascual, Mercedes and Vandermeer, John},
  title     = {Anticipating Critical Transitions},
  volume    = {338},
  number    = {6105},
  pages     = {344--348},
  year      = {2012},
  doi       = {10.1126/science.1225244},
  publisher = {American Association for the Advancement of Science},
  abstract  = {Tipping points in complex systems may imply risks of unwanted collapse, but also opportunities for positive change. Our capacity to navigate such risks and opportunities can be boosted by combining emerging insights from two unconnected fields of research. One line of work is revealing fundamental architectural features that may cause ecological networks, financial markets, and other complex systems to have tipping points. Another field of research is uncovering generic empirical indicators of the proximity to such critical thresholds. Although sudden shifts in complex systems will inevitably continue to surprise us, work at the crossroads of these emerging fields offers new approaches for anticipating critical transitions.},
  issn      = {0036-8075},
  url       = {http://science.sciencemag.org/content/338/6105/344},
  eprint    = {http://science.sciencemag.org/content/338/6105/344.full.pdf},
  journal   = {Science}
}

@article{Scheffer-Nature-2009,
  author    = {Scheffer, Marten
               and Bascompte, Jordi
               and Brock, William A.
               and Brovkin, Victor
               and Carpenter, Stephen R.
               and Dakos, Vasilis
               and Held, Hermann
               and van Nes, Egbert H.
               and Rietkerk, Max
               and Sugihara, George},
  title     = {Early-Warning Signals for Critical Transitions},
  journal   = {Nature},
  year      = {2009},
  month     = {Sep},
  day       = {03},
  publisher = {Macmillan Publishers Limited. All rights reserved},
  volume    = {461},
  number    = {7260},
  pages     = {53-59},
  issn      = {0028-0836},
  doi       = {10.1038/nature08227},
  url       = {http://dx.doi.org/10.1038/nature08227}
}

@techreport{Bian-Ising-2010,
  title       = {The {I}sing {M}odel: Teaching an Old Problem New Tricks},
  author      = {Bian, Z. and Chudak, F. and Macready, W.G. and Rose, G.},
  year        = {2010},
  institution = {D-Wave Systems}
}

@article{Amin-arXiv-2016,
  author  = {Mohammad H. Amin and Evgeny Andriyash and Jason Rolfe and Bohdan Kulchytskyy and Roger Melko},
  title   = {Quantum {B}oltzmann Machine},
  journal = {arXiv:1601.02036},
  year    = {2016}
}

@article{Adachi-arXiv-2015,
  title   = {Application of Quantum Annealing to Training of Deep Neural Networks},
  author  = {Steven H. Adachi and Maxwell P. Henderson},
  journal = {arXiv:1510.06356},
  year    = {2015}
}

@article{Amin-PRA-2015,
  title     = {Searching for Quantum Speedup in Quasistatic Quantum Annealers},
  author    = {Amin, Mohammad H.},
  journal   = {Phys. Rev. A},
  volume    = {92},
  issue     = {5},
  pages     = {052323},
  numpages  = {5},
  year      = {2015},
  month     = {Nov},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevA.92.052323},
  url       = {http://link.aps.org/doi/10.1103/PhysRevA.92.052323}
}

@article{Caticha-AIP-2006,
  author  = {Caticha, Ariel and Giffin, Adom},
  title   = {Updating Probabilities},
  journal = {{AIP} Conference Proceedings},
  year    = {2006},
  volume  = {872},
  number  = {1},
  pages   = {31-42},
  url     = {http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.2423258},
  doi     = {http://dx.doi.org/10.1063/1.2423258}
}

@article{Cavagna-PRE-2014,
  title     = {Dynamical Maximum Entropy Approach to Flocking},
  author    = {Cavagna, Andrea and Giardina, Irene and Ginelli, Francesco and Mora, Thierry and Piovani, Duccio and Tavarone, Raffaele and Walczak, Aleksandra M.},
  journal   = {Phys. Rev. E},
  volume    = {89},
  issue     = {4},
  pages     = {042707},
  numpages  = {10},
  year      = {2014},
  month     = {Apr},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevE.89.042707},
  url       = {http://link.aps.org/doi/10.1103/PhysRevE.89.042707}
}

@article{Mora-PRL-2015,
  title     = {Dynamical Criticality in the Collective Activity of a Population of Retinal Neurons},
  author    = {Mora, Thierry and Deny, St\'ephane and Marre, Olivier},
  journal   = {Phys. Rev. Lett.},
  volume    = {114},
  issue     = {7},
  pages     = {078105},
  numpages  = {5},
  year      = {2015},
  month     = {Feb},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.114.078105},
  url       = {http://link.aps.org/doi/10.1103/PhysRevLett.114.078105}
}

@article{Presse-RMP-2013,
  title     = {Principles of Maximum Entropy and Maximum Caliber in Statistical Physics},
  author    = {Press\'e, Steve and Ghosh, Kingshuk and Lee, Julian and Dill, Ken A.},
  journal   = {Rev. Mod. Phys.},
  volume    = {85},
  issue     = {3},
  pages     = {1115--1141},
  numpages  = {0},
  year      = {2013},
  month     = {Jul},
  publisher = {American Physical Society},
  doi       = {10.1103/RevModPhys.85.1115},
  url       = {http://link.aps.org/doi/10.1103/RevModPhys.85.1115}
}

@article{Chliamovitch-EPL-2015,
  author   = {G. Chliamovitch and A. Dupuis and A. Golub and B. Chopard},
  title    = {Improving Predictability of Time Series Using Maximum Entropy Methods},
  journal  = {EPL (Europhysics Letters)},
  volume   = {110},
  number   = {1},
  pages    = {10003},
  url      = {http://stacks.iop.org/0295-5075/110/i=1/a=10003},
  year     = {2015},
  abstract = {We discuss how maximum entropy methods may be applied to the reconstruction of Markov processes underlying empirical time series and compare this approach to usual frequency sampling. It is shown that, in low dimension, there exists a subset of the space of stochastic matrices for which the MaxEnt method is more efficient than sampling, in the sense that shorter historical samples have to be considered to reach the same accuracy. Considering short samples is of particular interest when modelling smoothly non-stationary processes, which provides, under some conditions, a powerful forecasting tool. The method is illustrated for a discretized empirical series of exchange rates.}
}

@article{Schneidman-Nature-2006,
  author  = {Schneidman, Elad
             and Berry, Michael J.
             and Segev, Ronen
             and Bialek, William},
  title   = {{Weak Pairwise Correlations Imply Strongly Correlated Network States in a Neural Population}},
  journal = {Nature},
  year    = {2006},
  month   = {Apr},
  day     = {20},
  volume  = {440},
  number  = {7087},
  pages   = {1007-1012},
  issn    = {0028-0836},
  doi     = {10.1038/nature04701},
  url     = {http://dx.doi.org/10.1038/nature04701}
}

@article{Altarelli-JSTAT-2009,
  author   = {Altarelli, F. and Braunstein, A. and Realpe-G\'omez, J. and Zecchina, R.},
  title    = {Statistical Mechanics of Budget-Constrained Auctions},
  journal  = {Journal of Statistical Mechanics: Theory and Experiment},
  volume   = {2009},
  number   = {07},
  pages    = {P07002},
  url      = {http://stacks.iop.org/1742-5468/2009/i=07/a=P07002},
  year     = {2009},
  abstract = {Finding the optimal assignment in budget-constrained auctions is a combinatorial optimization problem with many important applications, a notable example being in the sale of advertisement space by search engines (in this context the problem is often referred to as the off-line AdWords problem). On the basis of the cavity method of statistical mechanics, we introduce a message-passing algorithm that is capable of solving efficiently random instances of the problem extracted from a natural distribution, and we derive from its properties the phase diagram of the problem. As the control parameter (average value of the budgets) is varied, we find two phase transitions delimiting a region in which long-range correlations arise.}
}

@article{Ramezanpour-EPJB-2011,
  year      = {2011},
  issn      = {1434-6028},
  journal   = {The European Physical Journal B},
  volume    = {81},
  number    = {3},
  doi       = {10.1140/epjb/e2011-10963-x},
  title     = {Statistical Physics Approach to Graphical Games: Local and Global Interactions},
  url       = {http://dx.doi.org/10.1140/epjb/e2011-10963-x},
  publisher = {Springer-Verlag},
  author    = {Ramezanpour, A. and Realpe-G\'omez, J. and Zecchina, R.},
  pages     = {327-339},
  language  = {English}
}

@article{Benedetti2018,
  author  = {Benedetti, M. and Realpe G\'omez, J. and Perdomo-Ortiz, A.},
  title   = {Quantum-Assisted {H}elmholtz Machines: A Quantum-Classical Deep Learning Framework for Industrial Datasets in Near-term Devices},
  journal = {Quantum Science and Technology},
  volume  = {3},
  number  = {3},
  pages   = {034007},
  url     = {http://stacks.iop.org/2058-9565/3/i=3/a=034007},
  year    = {2018}
}

@article{Lloyd-2013,
  title   = {Quantum Algorithms for Supervised and Unsupervised Machine Learning},
  author  = {Lloyd, S. and Mohseni,  M. and Rebentrost, P. },
  journal = {arXiv:1307.0411},
  year    = {2013}
}

@article{Decelle-PRB-2014,
  title     = {Belief-Propagation-Guided {M}onte-{C}arlo Sampling},
  author    = {Decelle, Aur\'elien and Krzakala, Florent},
  journal   = {Phys. Rev. B},
  volume    = {89},
  issue     = {21},
  pages     = {214421},
  numpages  = {5},
  year      = {2014},
  month     = {Jun},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevB.89.214421},
  url       = {http://link.aps.org/doi/10.1103/PhysRevB.89.214421}
}

@article{Decelle-arXiv-2015,
  title   = {Solving the Inverse Ising Problem by Mean-field Methods in a Clustered Phase Space with Many States},
  author  = {Decelle, Aur{\'e}lien and Ricci-Tersenghi, Federico},
  journal = {arXiv preprint arXiv:1501.03034},
  year    = {2015}
}

@article{Szeliski-2008,
  author   = {Szeliski, R. and Zabih, R. and Scharstein, D. and Veksler, O. and Kolmogorov, V. and Agarwala, Aseem and Tappen, M. and Rother, C.},
  journal  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  title    = {A Comparative Study of Energy Minimization Methods for {M}arkov Random Fields with Smoothness-Based Priors},
  year     = {2008},
  volume   = {30},
  number   = {6},
  pages    = {1068-1080},
  keywords = {Markov processes;belief networks;energy consumption;image denoising;image segmentation;image texture;iterative methods;message passing;random processes;stereo image processing;trees (mathematics);Markov random fields;depth computation;early vision;energy minimization methods;graph cuts;image denoising;image stitching;interactive segmentation;iterated conditional mode algorithm;loopy belief propagation;optimization methods;pixel-labeling tasks;smoothness-based priors;software interface;stereo methods;texture computation;tree-reweighted message passing;Belief propagation;Global optimization;Graph cuts;Markov random fields;Performance evaluation of algorithms and systems;Algorithms;Artificial Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Markov Chains;Models, Statistical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity},
  doi      = {10.1109/TPAMI.2007.70844},
  issn     = {0162-8828},
  month    = {June}
}

@article{Felzenszwalb-2006,
  year      = {2006},
  issn      = {0920-5691},
  journal   = {International Journal of Computer Vision},
  volume    = {70},
  number    = {1},
  doi       = {10.1007/s11263-006-7899-4},
  title     = {Efficient Belief Propagation for Early Vision},
  url       = {http://dx.doi.org/10.1007/s11263-006-7899-4},
  publisher = {Kluwer Academic Publishers},
  keywords  = {belief propagation; Markov random fields; stereo; image restoration; efficient algorithms},
  author    = {Felzenszwalb, PedroF. and Huttenlocher, DanielP.},
  pages     = {41-54},
  language  = {English}
}

@article{Mastromatteo-PhD-2013,
  author        = {{Mastromatteo}, I.},
  title         = {On the Typical Properties of Inverse Problems in Statistical Mechanics},
  journal       = {ArXiv e-prints},
  archiveprefix = {arXiv},
  eprint        = {1311.0190},
  primaryclass  = {cond-mat.stat-mech},
  keywords      = {Condensed Matter - Statistical Mechanics},
  year          = 2013,
  month         = nov,
  adsurl        = {http://adsabs.harvard.edu/abs/2013arXiv1311.0190M},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Grosse-2015,
  publisher = {JMLR Workshop and Conference Proceedings},
  title     = {Scaling up Natural Gradient by Sparsely Factorizing the Inverse {F}isher Matrix},
  author    = {Grosse, Roger and Salakhudinov, Ruslan},
  year      = {2015},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  editor    = {David Blei and Francis Bach},
  pages     = {2304--2313},
  url       = {http://jmlr.org/proceedings/papers/v37/grosse15.pdf}
}

@article{Desjardins-2013,
  author        = {{Desjardins}, G. and {Pascanu}, R. and {Courville}, A. and {Bengio}, Y.
                   },
  title         = {Metric-Free Natural Gradient for Joint-Training of {B}oltzmann Machines},
  journal       = {ArXiv e-prints},
  archiveprefix = {arXiv},
  eprint        = {1301.3545},
  primaryclass  = {cs.LG},
  keywords      = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  year          = 2013,
  month         = jan,
  adsurl        = {http://adsabs.harvard.edu/abs/2013arXiv1301.3545D},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Zoubin-Nature-2015,
  author  = {Ghahramani, Zoubin},
  title   = {Probabilistic Machine Learning and Artificial Intelligence},
  journal = {Nature},
  volume  = {521},
  pages   = {452 - 459},
  year    = {2015}
}

@article{LeCun-Nature-2015,
  author  = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title   = {Deep Learning},
  journal = {Nature},
  volume  = {521},
  pages   = {436 - 444},
  year    = {2015}
}

@article{OGorman-EPJST-2015,
  year      = {2015},
  issn      = {1951-6355},
  journal   = {The European Physical Journal Special Topics},
  volume    = {224},
  number    = {1},
  doi       = {10.1140/epjst/e2015-02349-9},
  title     = {Bayesian Network Structure Learning Using Quantum Annealing},
  url       = {http://dx.doi.org/10.1140/epjst/e2015-02349-9},
  publisher = {Springer Berlin Heidelberg},
  author    = {O’Gorman, B. and Babbush, R. and Perdomo-Ortiz, A. and Aspuru-Guzik, A. and Smelyanskiy, V.},
  pages     = {163-188},
  language  = {English}
}

@article{Eslami-IJCV-2014,
  year      = {2014},
  issn      = {0920-5691},
  journal   = {International Journal of Computer Vision},
  volume    = {107},
  number    = {2},
  doi       = {10.1007/s11263-013-0669-1},
  title     = {The {S}hape {B}oltzmann {M}achine: A Strong Model of Object Shape},
  url       = {http://dx.doi.org/10.1007/s11263-013-0669-1},
  publisher = {Springer US},
  keywords  = {Shape; Generative; Deep Boltzmann machine; Sampling},
  author    = {Eslami, S.M. Ali and Heess, Nicolas and Williams, Christopher K.I. and Winn, John},
  pages     = {155-176},
  language  = {English}
}

@article{Mezard-Science-2002,
  author   = {M\'ezard, M. and Parisi, G. and Zecchina, R.},
  title    = {Analytic and Algorithmic Solution of Random Satisfiability Problems},
  volume   = {297},
  number   = {5582},
  pages    = {812-815},
  year     = {2002},
  doi      = {10.1126/science.1073287},
  abstract = {We study the satisfiability of random Boolean expressions built from many clauses with K variables per clause (K-satisfiability). Expressions with a ratio α of clauses to variables less than a threshold αc are almost always satisfiable, whereas those with a ratio above this threshold are almost always unsatisfiable. We show the existence of an intermediate phase below αc, where the proliferation of metastable states is responsible for the onset of complexity in search algorithms. We introduce a class of optimization algorithms that can deal with these metastable states; one such algorithm has been tested successfully on the largest existing benchmark of K-satisfiability.},
  url      = {http://www.sciencemag.org/content/297/5582/812.abstract},
  eprint   = {http://www.sciencemag.org/content/297/5582/812.full.pdf},
  journal  = {Science}
}

@proceedings{LesHouses-2016,
  title     = {Statistical Physics, Optimization, Inference and Message-Passing Algorithms},
  editor    = {Florent Krzakala and Federico Ricci-Tersenghi and Lenka Zdeborova and Riccardo Zecchina and Eric W. Tramel and Leticia F. Cugliandolo},
  series    = {Lecture Notes of the Les Houches School of Physics: Special Issue, October 2013},
  publisher = {Oxford University Press},
  year      = {2016}
}

    
@inproceedings{goodfellow2014generative,
  title     = {Generative adversarial nets},
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in neural information processing systems},
  pages     = {2672--2680},
  year      = {2014}
}

@inproceedings{Dumolin-2014,
  author    = {V. Dumoulin and
               I. J. Goodfellow and
               A. C. Courville and
               Y. Bengio},
  title     = {On the Challenges of Physical Implementations of {RBMs}},
  booktitle = {Proceedings of the Twenty-Eighth {AAAI} Conference on Artificial Intelligence,
               July 27 -31, 2014, Qu{\'{e}}bec City, Qu{\'{e}}bec, Canada.},
  pages     = {1199--1205},
  year      = {2014},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8608},
  timestamp = {Thu, 31 Jul 2014 09:00:19 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/aaai/DumoulinGCB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Denil-2011,
  title   = {{Toward the Implementation of a Quantum {RBM}}},
  author  = {Denil, Misha and De Freitas, Nando},
  journal = {NIPS Deep Learning and Unsupervised Feature Learning Workshop},
  year    = {2011}
}

@article{Farhi2001,
  title    = {A Quantum Adiabatic Evolution Algorithm Applied to Random Instances of an {NP-Complete} Problem},
  volume   = {292},
  doi      = {10.1126/science.1057726},
  number   = {5516},
  journal  = {Science},
  author   = {Edward Farhi and Jeffrey Goldstone and Sam Gutmann and Joshua Lapan and Andrew Lundgren and Daniel Preda},
  month    = apr,
  year     = {2001},
  keywords = {aqc,zapo9},
  pages    = {472--475}
}

@article{Gaitan2012,
  title     = {Ramsey Numbers and Adiabatic Quantum Computing},
  author    = {Gaitan, Frank and Clark, Lane},
  journal   = {Phys. Rev. Lett.},
  volume    = {108},
  issue     = {1},
  pages     = {010501},
  numpages  = {4},
  year      = {2012},
  month     = {Jan},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.108.010501}
}

@article{PerdomoOrtiz2012_LPF,
  title   = {Finding Low-Energy Conformations of Lattice Protein Models by Quantum Annealing},
  author  = {A. Perdomo-Ortiz and N. Dickson and M. Drew-Brook and G. Rose and A. Aspuru-Guzik},
  journal = {Sci. Rep.},
  volume  = {2},
  pages   = {571},
  year    = {2012},
  month   = {Aug},
  doi     = {10.1038/srep00571}
}

@article{RieffelQIP2015,
  year      = {2015},
  issn      = {1570-0755},
  journal   = {Quantum Information Processing},
  volume    = {14},
  number    = {1},
  doi       = {10.1007/s11128-014-0892-x},
  title     = {{A Case Study in Programming a Quantum Annealer for Hard Operational Planning Problems}},
  publisher = {Springer US},
  keywords  = {Quantum computation; Quantum annealing keyword; Operational planning},
  author    = {Rieffel, E.G. and Venturelli, D. and O'Gorman, B. and Do, M.B. and Prystay, E.M. and Smelyanskiy, V.N.},
  pages     = {1-36},
  language  = {English}
}

@article{hinton1995wake,
  title     = {The Wake-Sleep Algorithm for Unsupervised Neural Networks},
  author    = {Hinton, G.E. and Dayan, P. and Frey, B.J. and Neal, R.M.},
  journal   = {Science},
  volume    = {268},
  number    = {5214},
  pages     = {1158},
  year      = {1995},
  publisher = {The American Association for the Advancement of Science}
}


@article{Benedetti2017,
  title     = {Quantum-Assisted Learning of Hardware-Embedded Probabilistic Graphical Models},
  author    = {Benedetti, M. and Realpe-G\'omez, J. and Biswas, R. and Perdomo-Ortiz, A.},
  journal   = {Phys. Rev. X},
  volume    = {7},
  issue     = {4},
  pages     = {041052},
  numpages  = {17},
  year      = {2017},
  month     = {Nov},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevX.7.041052}
}

@article{younes1999convergence,
  title     = {On the Convergence of Markovian Stochastic Algorithms with Rapidly Decreasing Ergodicity Rates},
  author    = {Younes, L.},
  journal   = {Stochastics: An International Journal of Probability and Stochastic Processes},
  volume    = {65},
  number    = {3-4},
  pages     = {177--228},
  year      = {1999},
  publisher = {Taylor \& Francis}
}

%------------------------------------------------------------
@misc{DWave,
  author       = {{D-Wave Systems Inc.}},
  title        = {{D-Wave 2000Q Technology Overview}},
  howpublished = {\url{https://www.dwavesys.com/resources/publications}},
  note         = {Accessed: 2018-05-17}
}


@techreport{QC_TR2018,
  author      = {Serban, R. and Wilson, M. and Benedetti, M. and Realpe-G\'omez, J. and Perdomo-Ortiz, A.},
  title       = {Quantum Annealing for Mobility Studies: Generation of {GO/NO-GO} Maps via Quantum-Assisted Machine Learning},
  institution = {Simulation-Based Engineering Laboratory, University of Wisconsin-Madison},
  year        = {2018},
  number      = {TR-2018-03},
  url         = {{http://sbel.wisc.edu/documents/TR-2018-03.pdf}}
}


%----Perception algorithms-----------------------------------
@article{ren2016faster,
  title     = {Faster {R-CNN}: Towards real-time object detection with region proposal networks},
  author    = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal   = {{IEEE} transactions on pattern analysis and machine intelligence},
  volume    = {39},
  number    = {6},
  pages     = {1137--1149},
  year      = {2016},
  publisher = {IEEE}
}

@article{redmon2018yolov3,
  title   = {{Yolov3}: An incremental improvement},
  author  = {Redmon, Joseph and Farhadi, Ali},
  journal = {arXiv preprint arXiv:1804.02767},
  year    = {2018}
}

@inproceedings{redmon2016yolo,
  title     = {You only look once: Unified, real-time object detection},
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {779--788},
  year      = {2016}
}

@article{chen2017deeplab,
  title     = {{Deeplab}: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs},
  author    = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  journal   = {{IEEE} transactions on pattern analysis and machine intelligence},
  volume    = {40},
  number    = {4},
  pages     = {834--848},
  year      = {2017},
  publisher = {IEEE}
}

@article{liu2019recent,
  title     = {Recent progress in semantic image segmentation},
  author    = {Liu, Xiaolong and Deng, Zhidong and Yang, Yuhan},
  journal   = {Artificial Intelligence Review},
  volume    = {52},
  number    = {2},
  pages     = {1089--1106},
  year      = {2019},
  publisher = {Springer}
}

@inproceedings{bdd100k,
  author    = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen,
               Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  title     = {{BDD100K}: A Diverse Driving Dataset for Heterogeneous Multitask Learning},
  booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  month     = {June},
  year      = {2020}
}

@inproceedings{liu2016ssd,
  title        = {Ssd: Single shot multibox detector},
  author       = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  booktitle    = {European conference on computer vision},
  pages        = {21--37},
  year         = {2016},
  organization = {Springer}
}

@inproceedings{he2017mask,
  title     = {Mask r-cnn},
  author    = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {2961--2969},
  year      = {2017}
}


@article{Levine2016,
  author  = {Sergey Levine and
             Peter Pastor and
             Alex Krizhevsky and
             Deirdre Quillen},
  title   = {Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning
             and Large-Scale Data Collection},
  journal = {CoRR},
  volume  = {abs/1603.02199},
  year    = {2016},
  url     = {http://arxiv.org/abs/1603.02199}
}

@article{inHandManipulation2020,
  author  = {Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremb, Wojciech},
  title   = {Learning dexterous in-hand manipulation},
  journal = {The International Journal of Robotics Research},
  volume  = {39},
  number  = {1},
  pages   = {3-20},
  doi     = {10.1177/0278364919887447},
  url     = {https://doi.org/10.1177/0278364919887447},
  year    = {2020}
}

@article{andrychowicz2020learning,
	author = {Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Józefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  title = {Learning Dexterous In-Hand Manipulation},
	journal = {International Journal of Robotics Research},
  volume = {39},
	number = {1},
  pages = {3--20},
  doi = {10.1177/0278364919887447},
	year = {2020},
}

@misc{AminiRKR19,
  author        = {Alexander Amini and
                   Guy Rosman and
                   Sertac Karaman and
                   Daniela Rus},
  title         = {Variational End-to-End Navigation and Localization},
  journal       = {arXiv preprint arXiv:1811.10119},
  url           = {http://arxiv.org/abs/1811.1011},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1811.10119},
  timestamp     = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1811-10119.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{BengioCurriculumLearning09,
  author    = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
  title     = {Curriculum Learning},
  year      = {2009},
  isbn      = {9781605585161},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1553374.1553380},
  doi       = {10.1145/1553374.1553380},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  pages     = {41–48},
  numpages  = {8},
  location  = {Montreal, Quebec, Canada},
  series    = {ICML '09}
}

@article{kingma2017adam,
  title         = {{Adam}: A Method for Stochastic Optimization},
  author        = {Diederik P. Kingma and Jimmy Ba},
  year          = {2017},
  journal       = {CoRR},
  volume        = {abs/1412.6980},
  eprint        = {1412.6980},
  archiveprefix = {arXiv},
  url           = {https://arxiv.org/abs/1412.6980},
  primaryclass  = {cs.LG}
}

@misc{baselines,
  author       = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title        = {{OpenAI Baselines}},
  howpublished = {\url{https://github.com/openai/baselines}}
}

@article{raffin2021stable,
  title     = {Stable-baselines3: Reliable reinforcement learning implementations},
  author    = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
  journal   = {The Journal of Machine Learning Research},
  volume    = {22},
  number    = {1},
  pages     = {12348--12355},
  year      = {2021},
  publisher = {JMLRORG}
}

@article{schulman2017proximal,
  title   = {Proximal policy optimization algorithms},
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  year    = {2017}
}

@article{yu2022surprising,
  title   = {The surprising effectiveness of ppo in cooperative multi-agent games},
  author  = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {24611--24624},
  year    = {2022}
}

@inproceedings{matignon2006reward,
  title        = {Reward function and initial values: Better choices for accelerated goal-directed reinforcement learning},
  author       = {Matignon, La{\"e}titia and Laurent, Guillaume J and Le Fort-Piat, Nadine},
  booktitle    = {Artificial Neural Networks--ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16},
  pages        = {840--849},
  year         = {2006},
  organization = {Springer}
}

@article{plappert2018multi,
  title   = {Multi-goal reinforcement learning: Challenging robotics environments and request for research},
  author  = {Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and others},
  journal = {arXiv preprint arXiv:1802.09464},
  year    = {2018}
}

@article{lillicrap2015continuous,
  title   = {Continuous control with deep reinforcement learning},
  author  = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal = {arXiv preprint arXiv:1509.02971},
  year    = {2015}
}

@article{mnih2013playing,
  title   = {Playing atari with deep reinforcement learning},
  author  = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal = {arXiv preprint arXiv:1312.5602},
  year    = {2013}
}

@inproceedings{fujimoto2018addressing,
  title        = {Addressing function approximation error in actor-critic methods},
  author       = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle    = {International conference on machine learning},
  pages        = {1587--1596},
  year         = {2018},
  organization = {PMLR}
}

@inproceedings{PatelCKK2017,
  author    = {Patel, Naman and Choromańska, Anna and Krishnamurthy, Prashanth and Khorrami, Farshad},
  booktitle = {{IEEE/RSJ} International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2017},
  pages     = {1531-1536},
  title     = {Sensor modality fusion with {CNNs} for {UGV} autonomous driving in indoor environments}
}

@misc{matas2018simtoreal,
  title         = {Sim-to-Real Reinforcement Learning for Deformable Object Manipulation},
  author        = {Jan Matas and Stephen James and Andrew J. Davison},
  year          = {2018},
  journal       = {arXiv preprint arXiv:1806.07851},
  url           = {https://arxiv.org/abs/1806.07851},
  eprint        = {1806.07851},
  archiveprefix = {arXiv},
  primaryclass  = {cs.RO}
}
@misc{Pan2017OffRoadAV,
  author        = {Yunpeng Pan and
                   Ching{-}An Cheng and
                   Kamil Saigol and
                   Keuntaek Lee and
                   Xinyan Yan and
                   Evangelos A. Theodorou and
                   Byron Boots},
  title         = {Agile Autonomous Driving Using End-to-End Deep Imitation
                   Learning},
  journal       = {CoRR},
  volume        = {abs/1709.07174},
  year          = {2017},
  url           = {http://arxiv.org/abs/1709.07174},
  archiveprefix = {arXiv},
  eprint        = {1709.07174},
  timestamp     = {Thu, 18 Jul 2019 16:08:56 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1709-07174.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@misc{BohezVCVSD17,
  author        = {Steven Bohez and
                   Tim Verbelen and
                   Elias De Coninck and
                   Bert Vankeirsbilck and
                   Pieter Simoens and
                   Bart Dhoedt},
  title         = {Sensor Fusion for Robot Control through Deep Reinforcement Learning},
  journal       = {CoRR},
  volume        = {abs/1703.04550},
  year          = {2017},
  url           = {http://arxiv.org/abs/1703.04550},
  archiveprefix = {arXiv},
  eprint        = {1703.04550},
  timestamp     = {Mon, 13 Aug 2018 16:48:52 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/BohezVCVSD17.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{richter2021enhancing,
  title={Enhancing Photorealism Enhancement},
  volume = {45},
  number = {2},
  author={Richter, Stephan R. and AlHaija, Hassan Abu and Koltun, Vladlen},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1700--1715},
  year = {2023},
  doi = {10.1109/TPAMI.2022.3166687},
}

@book{neal1996bayesianNN,
  author    = {Neal, Radford M.},
  title     = {Bayesian Learning for Neural Networks},
  year      = {1996},
  isbn      = {0387947248},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg}
}

@article{denton2015deep,
  title   = {Deep generative image models using a laplacian pyramid of adversarial networks},
  author  = {Denton, Emily L and Chintala, Soumith and Fergus, Rob and others},
  journal = {Advances in neural information processing systems},
  volume  = {28},
  year    = {2015}
}

@inproceedings{heusel2017gans,
  title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Advances in Neural Information Processing Systems ({NeurIPS})},
  volume={30},
  year={2017},
  address = {Long Beach, USA},
}

@inproceedings{binkowski2018demystifying,
  title = {Demystifying MMD GANs},
  author = {Bi{\'n}kowski, Miko{\l}aj and Sutherland, Danica J. and Arbel, Michael and Gretton, Arthur},
  booktitle = {International Conference on Learning Representations ({ICLR})},
  year = {2018},
  address = {Vancouver, Canada},
}

@article{wang2020deep,
  title     = {Deep high-resolution representation learning for visual recognition},
  author    = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and others},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {43},
  number    = {10},
  pages     = {3349--3364},
  year      = {2020},
  publisher = {IEEE}
}

@inproceedings{lambert2020mseg,
  title     = {MSeg: A composite dataset for multi-domain semantic segmentation},
  author    = {Lambert, John and Liu, Zhuang and Sener, Ozan and Hays, James and Koltun, Vladlen},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {2879--2888},
  year      = {2020}
}

@misc{jocher2020yolov5,
  author    = {Glenn Jocher and
               Alex Stoken and
               Jirka Borovec and
               NanoCode012 and
               ChristopherSTAN and
               Liu Changyu and
               Laughing and
               tkianai and
               Adam Hogan and
               lorenzomammana and
               yxNONG and
               AlexWang1900 and
               Laurentiu Diaconu and
               Marc and
               wanghaoyang0106 and
               ml5ah and
               Doug and
               Francisco Ingham and
               Frederik and
               Guilhen and
               Hatovix and Jake Poznanski and
               Jiacong Fang and
               Lijun Yu and
               changyu98 and
               Mingyu Wang and
               Naman Gupta and
               Osama Akhtar and
               PetrDvoracek and
               Prashant Rai},
  title     = {{ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements}},
  month     = oct,
  year      = 2020,
  publisher = {Zenodo},
  version   = {v3.1},
  doi       = {10.5281/zenodo.4154370},
  url       = {https://doi.org/10.5281/zenodo.4154370}
}

@misc{jocher_yolov5_2020,
	title = {{YOLOv5} by {Ultralytics}},
	url = {https://github.com/ultralytics/yolov5},
	author = {Jocher, Glenn},
	year = {2020},
	doi = {10.5281/zenodo.3908559},
}

@inproceedings{park2020contrastive,
  title={Contrastive Learning for Unpaired Image-to-Image Translation},
  author={Park, Taesung and Efros, Alexei A. and Zhang, Richard and Zhu, Jun-Yan},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={319--345},
  month = aug,
  year={2020},
  organization={Springer},
  address = {Online},
  doi = {10.1007/978-3-030-58545-7_19},
}

@misc{tao2020semanticsegmentation,
  doi       = {10.48550/ARXIV.2005.10821},
  url       = {https://arxiv.org/abs/2005.10821},
  author    = {Tao, Andrew and Sapra, Karan and Catanzaro, Bryan},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Hierarchical Multi-Scale Attention for Semantic Segmentation},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{neuhold2017mapillary,
  title     = {The mapillary vistas dataset for semantic understanding of street scenes},
  author    = {Neuhold, Gerhard and Ollmann, Tobias and Rota Bulo, Samuel and Kontschieder, Peter},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {4990--4999},
  year      = {2017}
}

@inproceedings{salimans2016improved,
  title={Improved Techniques for Training GANs},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  booktitle={Advances in Neural Information Processing Systems ({NeurIPS})},
  volume={29},
  year={2016},
  address = {Barcelona, Spain},
}

@inproceedings{hoffman2018cycada,
  title={CyCADA: Cycle-Consistent Adversarial Domain Adaptation},
  author={Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1989--1998},
  year={2018},
  month = jul,
}

@inproceedings{huang2018munit,
  title={Multimodal Unsupervised Image-to-Image Translation},
  author={Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={172--189},
  year={2018},
  month = sep,
  address = {Munich, Germany},
}

@inproceedings{huang2018multimodal,
  title={Multimodal Unsupervised Image-to-Image Translation},
  author={Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={172--189},
  year={2018},
  month = sep,
  address = {Munich, Germany},
}

@inproceedings{jiang2020tsit,
  title        = {Tsit: A simple and versatile framework for image-to-image translation},
  author       = {Jiang, Liming and Zhang, Changxu and Huang, Mingyang and Liu, Chunxiao and Shi, Jianping and Loy, Chen Change},
  booktitle    = {European Conference on Computer Vision},
  pages        = {206--222},
  year         = {2020},
  organization = {Springer}
}

@inproceedings{park2019semantic,
  title     = {Semantic image synthesis with spatially-adaptive normalization},
  author    = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {2337--2346},
  year      = {2019}
}

@inproceedings{wang2018high,
  title     = {High-resolution image synthesis and semantic manipulation with conditional gans},
  author    = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {8798--8807},
  year      = {2018}
}


@inproceedings{ronneberger2015u,
  title        = {U-net: Convolutional networks for biomedical image segmentation},
  author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle    = {International Conference on Medical image computing and computer-assisted intervention},
  pages        = {234--241},
  year         = {2015},
  organization = {Springer}
}

@misc{open_x_embodiment_rt_x_2023,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {Open X-Embodiment Collaboration and Abby O'Neill and Abdul Rehman and Abhiram Maddukuri and Abhishek Gupta and Abhishek Padalkar and Abraham Lee and Acorn Pooley and Agrim Gupta and Ajay Mandlekar and Ajinkya Jain and Albert Tung and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anchit Gupta and Andrew Wang and Andrey Kolobov and Anikait Singh and Animesh Garg and Aniruddha Kembhavi and Annie Xie and Anthony Brohan and Antonin Raffin and Archit Sharma and Arefeh Yavary and Arhan Jain and Ashwin Balakrishna and Ayzaan Wahid and Ben Burgess-Limerick and Beomjoon Kim and Bernhard Schölkopf and Blake Wulfe and Brian Ichter and Cewu Lu and Charles Xu and Charlotte Le and Chelsea Finn and Chen Wang and Chenfeng Xu and Cheng Chi and Chenguang Huang and Christine Chan and Christopher Agia and Chuer Pan and Chuyuan Fu and Coline Devin and Danfei Xu and Daniel Morton and Danny Driess and Daphne Chen and Deepak Pathak and Dhruv Shah and Dieter Büchler and Dinesh Jayaraman and Dmitry Kalashnikov and Dorsa Sadigh and Edward Johns and Ethan Foster and Fangchen Liu and Federico Ceola and Fei Xia and Feiyu Zhao and Felipe Vieira Frujeri and Freek Stulp and Gaoyue Zhou and Gaurav S. Sukhatme and Gautam Salhotra and Ge Yan and Gilbert Feng and Giulio Schiavi and Glen Berseth and Gregory Kahn and Guangwen Yang and Guanzhi Wang and Hao Su and Hao-Shu Fang and Haochen Shi and Henghui Bao and Heni Ben Amor and Henrik I Christensen and Hiroki Furuta and Homer Walke and Hongjie Fang and Huy Ha and Igor Mordatch and Ilija Radosavovic and Isabel Leal and Jacky Liang and Jad Abou-Chakra and Jaehyung Kim and Jaimyn Drake and Jan Peters and Jan Schneider and Jasmine Hsu and Jeannette Bohg and Jeffrey Bingham and Jeffrey Wu and Jensen Gao and Jiaheng Hu and Jiajun Wu and Jialin Wu and Jiankai Sun and Jianlan Luo and Jiayuan Gu and Jie Tan and Jihoon Oh and Jimmy Wu and Jingpei Lu and Jingyun Yang and Jitendra Malik and João Silvério and Joey Hejna and Jonathan Booher and Jonathan Tompson and Jonathan Yang and Jordi Salvador and Joseph J. Lim and Junhyek Han and Kaiyuan Wang and Kanishka Rao and Karl Pertsch and Karol Hausman and Keegan Go and Keerthana Gopalakrishnan and Ken Goldberg and Kendra Byrne and Kenneth Oslund and Kento Kawaharazuka and Kevin Black and Kevin Lin and Kevin Zhang and Kiana Ehsani and Kiran Lekkala and Kirsty Ellis and Krishan Rana and Krishnan Srinivasan and Kuan Fang and Kunal Pratap Singh and Kuo-Hao Zeng and Kyle Hatch and Kyle Hsu and Laurent Itti and Lawrence Yunliang Chen and Lerrel Pinto and Li Fei-Fei and Liam Tan and Linxi "Jim" Fan and Lionel Ott and Lisa Lee and Luca Weihs and Magnum Chen and Marion Lepert and Marius Memmel and Masayoshi Tomizuka and Masha Itkina and Mateo Guaman Castro and Max Spero and Maximilian Du and Michael Ahn and Michael C. Yip and Mingtong Zhang and Mingyu Ding and Minho Heo and Mohan Kumar Srirama and Mohit Sharma and Moo Jin Kim and Naoaki Kanazawa and Nicklas Hansen and Nicolas Heess and Nikhil J Joshi and Niko Suenderhauf and Ning Liu and Norman Di Palo and Nur Muhammad Mahi Shafiullah and Oier Mees and Oliver Kroemer and Osbert Bastani and Pannag R Sanketi and Patrick "Tree" Miller and Patrick Yin and Paul Wohlhart and Peng Xu and Peter David Fagan and Peter Mitrano and Pierre Sermanet and Pieter Abbeel and Priya Sundaresan and Qiuyu Chen and Quan Vuong and Rafael Rafailov and Ran Tian and Ria Doshi and Roberto Mart{'i}n-Mart{'i}n and Rohan Baijal and Rosario Scalise and Rose Hendrix and Roy Lin and Runjia Qian and Ruohan Zhang and Russell Mendonca and Rutav Shah and Ryan Hoque and Ryan Julian and Samuel Bustamante and Sean Kirmani and Sergey Levine and Shan Lin and Sherry Moore and Shikhar Bahl and Shivin Dass and Shubham Sonawani and Shuran Song and Sichun Xu and Siddhant Haldar and Siddharth Karamcheti and Simeon Adebola and Simon Guist and Soroush Nasiriany and Stefan Schaal and Stefan Welker and Stephen Tian and Subramanian Ramamoorthy and Sudeep Dasari and Suneel Belkhale and Sungjae Park and Suraj Nair and Suvir Mirchandani and Takayuki Osa and Tanmay Gupta and Tatsuya Harada and Tatsuya Matsushima and Ted Xiao and Thomas Kollar and Tianhe Yu and Tianli Ding and Todor Davchev and Tony Z. Zhao and Travis Armstrong and Trevor Darrell and Trinity Chung and Vidhi Jain and Vincent Vanhoucke and Wei Zhan and Wenxuan Zhou and Wolfram Burgard and Xi Chen and Xiangyu Chen and Xiaolong Wang and Xinghao Zhu and Xinyang Geng and Xiyuan Liu and Xu Liangwei and Xuanlin Li and Yansong Pang and Yao Lu and Yecheng Jason Ma and Yejin Kim and Yevgen Chebotar and Yifan Zhou and Yifeng Zhu and Yilin Wu and Ying Xu and Yixuan Wang and Yonatan Bisk and Yongqiang Dou and Yoonyoung Cho and Youngwoon Lee and Yuchen Cui and Yue Cao and Yueh-Hua Wu and Yujin Tang and Yuke Zhu and Yunchu Zhang and Yunfan Jiang and Yunshuang Li and Yunzhu Li and Yusuke Iwasawa and Yutaka Matsuo and Zehan Ma and Zhuo Xu and Zichen Jeff Cui and Zichen Zhang and Zipeng Fu and Zipeng Lin},
howpublished  = {\url{https://arxiv.org/abs/2310.08864}},
year = {2023},
}

@inproceedings{he2016deep,
  title     = {Deep residual learning for image recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {770--778},
  year      = {2016}
}

@inproceedings{karras2019style,
  title     = {A style-based generator architecture for generative adversarial networks},
  author    = {Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {4401--4410},
  year      = {2019}
}

@article{bowles2018gan,
  title   = {Gan augmentation: Augmenting training data using generative adversarial networks},
  author  = {Bowles, Christopher and Chen, Liang and Guerrero, Ricardo and Bentley, Paul and Gunn, Roger and Hammers, Alexander and Dickie, David Alexander and Hern{\'a}ndez, Maria Vald{\'e}s and Wardlaw, Joanna and Rueckert, Daniel},
  journal = {arXiv preprint arXiv:1810.10863},
  year    = {2018}
}

@article{zhao2020fine,
  title     = {Fine-grained facial image-to-image translation with an attention based pipeline generative adversarial framework},
  author    = {Zhao, Yan and Zheng, Ziqiang and Wang, Chao and Gu, Zhaorui and Fu, Min and Yu, Zhibin and Zheng, Haiyong and Wang, Nan and Zheng, Bing},
  journal   = {Multimedia Tools and Applications},
  pages     = {1--20},
  year      = {2020},
  publisher = {Springer}
}

@inproceedings{yoo2019photorealistic,
  title     = {Photorealistic Style Transfer via Wavelet Transforms},
  author    = {Yoo, Jaejun and Uh, Youngjung and Chun, Sanghyuk and Kang, Byeongkyu and Ha, Jung-Woo},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages     = {9035--9044},
  year      = {2019},
  address = {Seoul, South Korea},
  doi = {10.1109/ICCV.2019.00913},
}

@article{pitie2007automated,
  title     = {Automated colour grading using colour distribution transfer},
  author    = {Piti{\'e}, Fran{\c{c}}ois and Kokaram, Anil C and Dahyot, Rozenn},
  journal   = {Computer Vision and Image Understanding},
  volume    = {107},
  number    = {1-2},
  pages     = {123--137},
  year      = {2007},
  publisher = {Elsevier}
}

@article{reinhard2001color,
  title     = {Color transfer between images},
  author    = {Reinhard, Erik and Adhikhmin, Michael and Gooch, Bruce and Shirley, Peter},
  journal   = {IEEE Computer graphics and applications},
  volume    = {21},
  number    = {5},
  pages     = {34--41},
  year      = {2001},
  publisher = {IEEE}
}

@inproceedings{ho2021retinagan,
  title        = {Retinagan: An object-aware approach to sim-to-real transfer},
  author       = {Ho, Daniel and Rao, Kanishka and Xu, Zhuo and Jang, Eric and Khansari, Mohi and Bai, Yunfei},
  booktitle    = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {10920--10926},
  year         = {2021},
  organization = {IEEE}
}

@inproceedings{du2022bayesian,
  title        = {Bayesian Imitation Learning for End-to-End Mobile Manipulation},
  author       = {Du, Yuqing and Ho, Daniel and Alemi, Alex and Jang, Eric and Khansari, Mohi},
  booktitle    = {International Conference on Machine Learning},
  pages        = {5531--5546},
  year         = {2022},
  organization = {PMLR}
}


@misc{nvidiaSemanticSegmentation,
  author       = {{NVIDIA}},
  title        = {semantic-segmentation},
  howpublished = {\url{https://github.com/NVIDIA/semantic-segmentation}},
  note         = {Accessed: 2022-09-14}
}

@article{li2017universal,
  title   = {Universal style transfer via feature transforms},
  author  = {Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{zhao2019object,
  title     = {Object detection with deep learning: A review},
  author    = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-tao and Wu, Xindong},
  journal   = {IEEE transactions on neural networks and learning systems},
  volume    = {30},
  number    = {11},
  pages     = {3212--3232},
  year      = {2019},
  publisher = {IEEE}
}

@article{xiao2020review,
  title     = {A review of object detection based on deep learning},
  author    = {Xiao, Youzi and Tian, Zhiqiang and Yu, Jiachen and Zhang, Yinshu and Liu, Shuai and Du, Shaoyi and Lan, Xuguang},
  journal   = {Multimedia Tools and Applications},
  volume    = {79},
  number    = {33},
  pages     = {23729--23791},
  year      = {2020},
  publisher = {Springer}
}

@article{mo2022review,
  title     = {Review the state-of-the-art technologies of semantic segmentation based on deep learning},
  author    = {Mo, Yujian and Wu, Yan and Yang, Xinneng and Liu, Feilin and Liao, Yujun},
  journal   = {Neurocomputing},
  volume    = {493},
  pages     = {626--646},
  year      = {2022},
  publisher = {Elsevier}
}

@article{guo2018review,
  title     = {A review of semantic segmentation using deep neural networks},
  author    = {Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S},
  journal   = {International journal of multimedia information retrieval},
  volume    = {7},
  number    = {2},
  pages     = {87--93},
  year      = {2018},
  publisher = {Springer}
}

@article{yu2018methods,
  title     = {Methods and datasets on semantic segmentation: A review},
  author    = {Yu, Hongshan and Yang, Zhengeng and Tan, Lei and Wang, Yaonan and Sun, Wei and Sun, Mingui and Tang, Yandong},
  journal   = {Neurocomputing},
  volume    = {304},
  pages     = {82--103},
  year      = {2018},
  publisher = {Elsevier}
}

@inproceedings{ganeshan2021warp,
  title     = {Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency},
  author    = {Ganeshan, Aditya and Vallet, Alexis and Kudo, Yasunori and Maeda, Shin-ichi and Kerola, Tommi and Ambrus, Rares and Park, Dennis and Gaidon, Adrien},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {15499--15509},
  year      = {2021}
}

@article{alhaija2018IJCV,
  author  = {Hassan Alhaija and Siva Mustikovela and Lars Mescheder and Andreas Geiger and Carsten Rother},
  title   = {Augmented Reality Meets Computer Vision: Efficient Data Generation for Urban Driving Scenes},
  journal = {International Journal of Computer Vision (IJCV)},
  year    = {2018}
}

@inproceedings{vanholder2016efficient,
  title     = {Efficient inference with tensorrt},
  author    = {Vanholder, Han},
  booktitle = {GPU Technology Conference},
  volume    = {1},
  pages     = {2},
  year      = {2016}
}

@article{goldstein2015peeking,
  title   = {Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation},
  author  = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  journal = {Journal of Computational and Graphical Statistics},
  volume  = {24},
  number  = {1},
  pages   = {44--65},
  year    = {2015}
}
@article{Shu2019aircraft,
  author  = {Shu, Dule and Cunningham, James and Stump, Gary and Miller, Simon W. and Yukish, Michael A. and Simpson, Timothy W. and Tucker, Conrad S.},
  title   = {{3D Design Using Generative Adversarial Networks and Physics-Based Validation}},
  journal = {Journal of Mechanical Design},
  volume  = {142},
  number  = {7},
  pages   = {071701},
  year    = {2019},
  month   = {11},
  issn    = {1050-0472},
  doi     = {10.1115/1.4045419},
  url     = {https://doi.org/10.1115/1.4045419},
  eprint  = {https://asmedigitalcollection.asme.org/mechanicaldesign/article-pdf/142/7/071701/6577896/md\_142\_7\_071701.pdf}
}
@article{Oh_2019,
  doi       = {10.1115/1.4044229},
  url       = {https://doi.org/10.1115%2F1.4044229},
  year      = 2019,
  month     = {sep},
  publisher = {{ASME} International},
  volume    = {141},
  number    = {11},
  author    = {Sangeun Oh and Yongsu Jung and Seongsin Kim and Ikjin Lee and Namwoo Kang},
  title     = {Deep Generative Design: Integration of Topology Optimization and Generative Models},
  journal   = {Journal of Mechanical Design}
}
@article{quinonero2005,
  author     = {Qui\~{n}onero-Candela, Joaquin and Rasmussen, Carl Edward},
  title      = {A Unifying View of Sparse Approximate Gaussian Process Regression},
  year       = {2005},
  issue_date = {12/1/2005},
  publisher  = {JMLR.org},
  volume     = {6},
  issn       = {1532-4435},
  journal    = {J. Mach. Learn. Res.},
  month      = {dec},
  pages      = {1939–1959},
  numpages   = {21}
}

@misc{chatgpt,
	author       = {OpenAI},
	title        = {{ChatGPT: Language Model}},
	year         = {2023},
	howpublished = {\url{https://chat.openai.com/}},
	note         = {Accessed: 2024-07-03}
}

@book{bentley1999,
  author    = {Bentley, P.J.},
  title     = {Evolutionary Design by Computers},
  publisher = {Morgan Kaufmann Publishers},
  year      = {1999}
}
@misc{OPENAI2020Scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{NIPS2017Transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
@article{NIPS2022HumanFeedback,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{NIPS2023LessIsbetter,
 author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and YU, LILI and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {55006--55021},
 publisher = {Curran Associates, Inc.},
 title = {LIMA: Less Is More for Alignment},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{Google2023Finetune,
title	= {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
author	= {Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
year	= {2023},
URL	= {https://proceedings.mlr.press/v202/longpre23a/longpre23a.pdf},
booktitle	= {Proceedings of the 40th International Conference on Machine Learning},
pages	= {22631--22648}}
@article{meta2022Opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{hu2022lora,
	title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
	author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{
   zhang2023adalora,
   title={Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning },
   author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
   booktitle={The Eleventh International Conference on Learning Representations },
   year={2023},
   url={https://openreview.net/forum?id=lq62uWRJjiY}
}
@article{gerstmayr2024multibodyllm,
  title={Multibody Models Generated from Natural Language},
  author={Gerstmayr, Johannes and Manzl, Peter and Pieber, Michael},
  journal={Multibody System Dynamics},
  pages={1--23},
  year={2024},
  publisher={Springer}
}
@article{kumar2023mycrunchgpt,
  title={Mycrunchgpt: A llm assisted framework for scientific machine learning},
  author={Kumar, Varun and Gleyzer, Leonard and Kahana, Adar and Shukla, Khemraj and Karniadakis, George Em},
  journal={Journal of Machine Learning for Modeling and Computing},
  volume={4},
  number={4},
  year={2023},
  publisher={Begel House Inc.}
}
@article{lin2024pegpt,
  title={PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design},
  author={Lin, Fanfan and Liu, Junhua and Li, Xinze and Zhao, Shuai and Zhao, Bohui and Ma, Hao and Zhang, Xin},
  journal={arXiv preprint arXiv:2403.14059},
  year={2024}
}
@article{aragones2024c4qgpt,
  title={C4Q: A Chatbot for Quantum},
  author={Aragon{\'e}s-Soria, Yaiza and Oriol, Manuel},
  journal={arXiv preprint arXiv:2402.01738},
  year={2024}
}

@article{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Yongqiang Ma},
  journal={arXiv preprint arXiv:2403.13372},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}

@article{gerstmayr2023exudyn,
  title={Exudyn--a C++-based Python package for flexible multibody systems},
  author={Gerstmayr, Johannes},
  journal={Multibody System Dynamics},
  pages={1--29},
  year={2023},
  publisher={Springer}
}
@article{RAISSI2019PINN,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{Aluga2023civilgpt, title={Application of CHATGPT in civil engineering}, volume={6}, url={https://journals.eanso.org/index.php/eaje/article/view/1272}, DOI={10.37284/eaje.6.1.1272}, number={1}, journal={East African Journal of Engineering}, author={Aluga, Martin}, year={2023}, month={Jun.}, pages={104-112} }

@article{king2023medicinegpt,
  title={The future of AI in medicine: a perspective from a Chatbot},
  author={King, Michael R},
  journal={Annals of Biomedical Engineering},
  volume={51},
  number={2},
  pages={291--295},
  year={2023},
  publisher={Springer}
}
@misc{touvron2023llama1,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Ollama,
 title        = {{Ollama's GitHub repository}},
  howpublished = {\url{https://github.com/ollama/ollama}},
  note         = {Accessed: 2024-04-28},
  year         = {2024}
}

@misc{vLLM,
      title={{Efficient Memory Management for Large Language Model Serving with PagedAttention}}, 
      author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
      year={2023},
      eprint={2309.06180},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.06180}, 
}

@misc{huggingface,
      title={{HuggingFace's Transformers: State-of-the-art Natural Language Processing}}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.03771}, 
}

@misc{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{gradio2019,
  title={Gradio: Hassle-free sharing and testing of ml models in the wild},
  author={Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},
  journal={arXiv preprint arXiv:1906.02569},
  year={2019}
}

@article{ahn2022google_robot,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}

@inproceedings{
sun2023adaplanner,
title={AdaPlanner: Adaptive Planning from Feedback with Language Models},
author={Haotian Sun and Yuchen Zhuang and Lingkai Kong and Bo Dai and Chao Zhang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=rnKgbKmelt}
}

@inproceedings{kwon2022rewardllm1,
  title={Reward Design with Language Models},
  author={Kwon, Minae and Xie, Sang Michael and Bullard, Kalesha and Sadigh, Dorsa},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{song2023rewardllm2,
  title={Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics},
  author={Song, Jiayang and Zhou, Zhehua and Liu, Jiawei and Fang, Chunrong and Shu, Zhan and Ma, Lei},
  journal={arXiv preprint arXiv:2309.06687},
  year={2023}
}

@inproceedings{ma2023rewardllm3,
  title={Eureka: Human-Level Reward Design via Coding Large Language Models},
  author={Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@INPROCEEDINGS{Singh2023ICRA_llmplan1,
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={ProgPrompt: Generating Situated Robot Task Plans using Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={11523-11530},
  doi={10.1109/ICRA48891.2023.10161317}
}

@article{chen2023ICRA_llmplan3,
  title={Scalable multi-robot collaboration with large language models: Centralized or decentralized systems?},
  author={Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
  journal={arXiv preprint arXiv:2309.15943},
  year={2023}
}
@article{hu2023llmplan4,
  title={Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning},
  author={Hu, Yingdong and Lin, Fanqi and Zhang, Tong and Yi, Li and Gao, Yang},
  journal={arXiv preprint arXiv:2311.17842},
  year={2023}
}
@article{wei2022NIPS_ChainOfThought,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{HAN_DNN2021113480,
title = {A DNN-based data-driven modeling employing coarse sample data for real-time flexible multibody dynamics simulations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {373},
pages = {113480},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113480},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520306654},
author = {Seongji Han and Hee-Sun Choi and Juhwan Choi and Jin Hwan Choi and Jin-Gyun Kim},
keywords = {Flexible multibody dynamics (FMBD), Data-driven modeling, Real-time simulations, Deep neural networks, Coarse sample data, Error correction},
abstract = {To achieve real-time simulations for flexible multibody dynamics (FMBD) systems, we suggest data-driven modeling based on deep neural networks (DNNs). While a DNN can represent system dynamics accurately, two main factors of FMBD systems require demanding computational costs for training a DNN. One is a fine discretization of flexible bodies, which produces a large number of training data. The other is the nonlinearity of FMBD, which requires train DNN models to have numerous weight and bias parameters. To overcome these difficulties, we propose a data-driven modeling algorithm for training a DNN efficiently that consists of two steps. First, sets of randomly chosen coarse data sequentially train a DNN model. This helps speed up the training process, even for highly parametrized DNNs. At some point, the model no longer improves, and introducing an error correction step increases the performance of the model. The proposed algorithm is easy to employ and utilizes an efficient size of training data while achieving high performance of the DNN as demonstrated by numerical examples.}
}

@article{YE_MBSNET2021107716,
title = {MBSNet: A deep learning model for multibody dynamics simulation and its application to a vehicle-track system},
journal = {Mechanical Systems and Signal Processing},
volume = {157},
pages = {107716},
year = {2021},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.107716},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021001114},
author = {Yunguang Ye and Ping Huang and Yu Sun and Dachuan Shi},
keywords = {MBSNet, Deep learning, Multibody dynamics simulation, Vehicle-track coupled dynamics simulation, Prediction, Challenge},
abstract = {In multibody dynamics simulation (MBS) analysis, researchers usually face three challenges: high modeling difficulty, large calculation amount, and restricted solver. Therefore, it is of interest to explore a new technology that can solve or avoid one or more of these challenges. This article first explores how to use deep learning (DL) to realize dynamics simulation of a multibody system and contributes to the modeling and analysis of MBS in two meaningful aspects: (1) Based on the 3D convolutional neural network (3DCNN), long short-term memory(LSTM), and fully connected network (FCNN), we develop a DL network called MBSNet, which considers the relationship between system variables and disturbance variables of a general multibody system, to realize MBS analysis. (2) Based on the short-term MBS results of a vehicle-track vertically coupled dynamics model, we train MBSNet and apply it to long-term system dynamic predictions. The comparison between the MBSNet result and the MBS result shows that MBSNet has high robustness in the face of different track irregularities and can accurately and quickly achieve long-term predictions of low-frequency dynamic responses. Finally, the evidence from the vehicle-track case shows that MBSNet has the potential to be used in MBS analysis, but it faces some challenges, including inaccurate prediction of high-frequency impact components caused by the coupled motion of bodies in a multi-degree-of-freedom system, improper training strategy, insufficient working conditions and samples considered during training, etc.}
}

@article{Choi_DDSdd57a6f7c2064450bc1f79231ef67414,
title = "Data-driven simulation for general-purpose multibody dynamics using Deep Neural Networks",
abstract = "In this paper, we introduce a machine learning-based simulation framework of general-purpose multibody dynamics (MBD). The aim of the framework is to construct a well-trained meta-model of MBD systems, based on a deep neural network (DNN). Since the main advantage of the meta-model is the enhancement of computational efficiency in returning solutions, the modeling would be beneficial for solving highly complex MBD problems in a short time. Furthermore, for dynamics problems, not only the accuracy but also the smoothness in time of motion solutions, such as displacement, velocity, and acceleration, are essential aspects to consider. We analyze and discuss the influence of training data structures on both aspects of solutions. As a result of the introduced approach, the meta-model provides motion estimation of system dynamics without solving an analytical equation of motion or a numerical solver. Numerical tests demonstrate the performance of the proposed meta-modeling for representing several MBD systems.",
keywords = "Data-driven simulation, Deep neural network, Feedforward network, Meta-model, Multibody dynamics",
author = "Choi, {Hee Sun} and Junmo An and Seongji Han and Kim, {Jin Gyun} and Jung, {Jae Yoon} and Juhwan Choi and Grzegorz Orzechowski and Aki Mikkola and Choi, {Jin Hwan}",
year = "2021",
month = apr,
doi = "10.1007/s11044-020-09772-8",
language = "English",
volume = "51",
pages = "419--454",
journal = "Multibody System Dynamics",
issn = "1384-5640",
publisher = "Springer Netherlands",
number = "4",
}

@article{Ye_FTI_2023,
author = {Go, Myeong-Seok and Han, Seongji and Lim, Jae Hyuk and Kim, Jin-Gyun},
year = {2023},
month = {02},
pages = {1-19},
title = {An efficient fixed-time increment-based data-driven simulation for general multibody dynamics using deep neural networks},
journal = {Engineering with Computers},
doi = {10.1007/s00366-023-01793-z}
}

@article{Chen_NODE_2018,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{OWO_CHEMNODE_2022,
title = {ChemNODE: A neural ordinary differential equations framework for efficient chemical kinetic solvers},
journal = {Energy and AI},
volume = {7},
pages = {100118},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100118},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000677},
author = {Opeoluwa Owoyele and Pinaki Pal},
keywords = {Machine learning, Neural ordinary differential equations, Artificial neural networks, Deep learning, Chemical kinetics, Artificial neural network, Chemistry solvers},
abstract = {Solving for detailed chemical kinetics remains one of the major bottlenecks for computational fluid dynamics simulations of reacting flows using a finite-rate-chemistry approach. This has motivated the use of neural networks to predict stiff chemical source terms as functions of the thermochemical state of the combustion system. However, due to the nonlinearities and multi-scale nature of combustion, the predicted solution often diverges from the true solution when these machine learning models are coupled with a computational fluid dynamics solver. This is because these approaches minimize the error during training without guaranteeing successful integration with ordinary differential equation solvers. In the present work, a novel neural ordinary differential equations approach to modeling chemical kinetics, termed as ChemNODE, is developed. In this machine learning framework, the chemical source terms predicted by the neural networks are integrated during training, and by computing the required derivatives, the neural network weights are adjusted accordingly to minimize the difference between the predicted and ground-truth solution. A proof-of-concept study is performed with ChemNODE for homogeneous autoignition of hydrogen-air mixture over a range of composition and thermodynamic conditions. It is shown that ChemNODE accurately captures the chemical kinetic behavior and reproduces the results obtained using the detailed kinetic mechanism at a fraction of the computational cost.}
}

@inproceedings{norcliffe2020secondNODE,
 author = {Norcliffe, Alexander and Bodnar, Cristian and Day, Ben and Simidjievski, Nikola and Li\'{o}, Pietro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5911--5921},
 publisher = {Curran Associates, Inc.},
 title = {On Second Order Behaviour in Augmented Neural ODEs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/418db2ea5d227a9ea8db8e5357ca2084-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{chen2020symplecticRNN,
      title={Symplectic Recurrent Neural Networks}, 
      author={Zhengdao Chen and Jianyu Zhang and Martin Arjovsky and Léon Bottou},
      year={2020},
      eprint={1909.13334},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{RNNMBD,
AUTHOR = {Koutsoupakis, Josef and Giagopoulos, Dimitrios},
TITLE = {Drivetrain Response Prediction Using AI-Based Surrogate and Multibody Dynamics Model},
JOURNAL = {Machines},
VOLUME = {11},
YEAR = {2023},
NUMBER = {5},
ARTICLE-NUMBER = {514},
URL = {https://www.mdpi.com/2075-1702/11/5/514},
ISSN = {2075-1702},
ABSTRACT = {Numerical models, such as multibody dynamics ones, are broadly used in various engineering applications, either as an integral part of the preliminary design of a product or simply to analyze its behavior. Aiming to increase the accuracy and potential of these models, complex mechanisms are constantly being added to existing methods of simulation, leading to powerful modelling frameworks that are able to simulate most mechanical systems. This increase in accuracy and flexibility, however, comes at a great computational cost. To mitigate the issue of high computation times, surrogates, such as reduced order models, have traditionally been used as cheaper alternatives, allowing for much faster simulations at the cost of introducing some error to the overall process. More recently, advancements in Artificial Intelligence have also allowed for the introduction of Artificial Intelligence-based models in the field of surrogates. While still undergoing development, these Artificial Intelligence based methodologies seem to be a potentially good alternative to the high-fidelity/burden models. To this end, an Artificial Intelligence-based surrogate comprised of Artificial Neural Networks as a means of predicting the response of dynamic mechanical systems is presented in this work, with application to a non-linear experimental gear drivetrain. The model utilizes Recurrent Neural Networks to accurately capture the system&rsquo;s response and is shown to yield accurate results, especially in the feature space. This methodology can provide an alternative to the traditional model surrogates and find application in multiple fields such as system optimization or data mining.},
DOI = {10.3390/machines11050514}
}

@misc{MNODE_supportData2024,
	author = {Wang, Jingquan and Wang, Shu and Unjhawala, Huzaifa and Wu, Jinlong and Negrut, Dan}, 
	title = {{Models, scripts, and meta-data: Physics-informed data-driven modeling and simulation of constrained multibody systems}},
	howpublished = {\url{https://github.com/uwsbel/sbel-reproducibility/tree/master/2024/MNODE-code}},
	year = {2024}
}

@Article{Wehage82,
	Title                    = {Generalized Coordinate Partitioning for Dimension Reduction in Analysis of Constrained Dynamic Systems},
	Author                   = {R.~A. Wehage and E.~J. Haug},
	Journal                  = {J. Mech. Design},
	Year                     = {1982},
	Pages                    = {247--255},
	Volume                   = {104}
}

@InProceedings{PINODE_constraints-djeumou22a,
  title = 	 {Neural Networks with Physics-Informed Architectures and Constraints for Dynamical Systems Modeling},
  author =       {Djeumou, Franck and Neary, Cyrus and Goubault, Eric and Putot, Sylvie and Topcu, Ufuk},
  booktitle = 	 {Proceedings of The 4th Annual Learning for Dynamics and Control Conference},
  pages = 	 {263--277},
  year = 	 {2022},
  editor = 	 {Firoozi, Roya and Mehr, Negar and Yel, Esen and Antonova, Rika and Bohg, Jeannette and Schwager, Mac and Kochenderfer, Mykel},
  volume = 	 {168},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v168/djeumou22a/djeumou22a.pdf},
  url = 	 {https://proceedings.mlr.press/v168/djeumou22a.html},
  abstract = 	 {Effective inclusion of physics-based knowledge into deep neural network models of dynamical systems can greatly improve data efficiency and generalization. Such a priori knowledge might arise from physical principles (e.g., conservation laws) or from the system’s design (e.g., the Jacobian matrix of a robot), even if large portions of the system dynamics remain unknown. We develop a framework to learn dynamics models from trajectory data while incorporating a priori system knowledge as inductive bias. More specifically, the proposed framework uses physics-based side information to inform the structure of the neural network itself, and to place constraints on the values of the outputs and the internal states of the model. It represents the system’s vector field as a composition of known and unknown functions, the latter of which are parametrized by neural networks. The physics-informed constraints are enforced via the augmented Lagrangian method during the model’s training. We experimentally demonstrate the benefits of the proposed approach on a variety of dynamical systems – including a benchmark suite of robotics environments featuring large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. By exploiting a priori system knowledge during training, the proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.}
}
@article{PINN_constraints,
author = {Lu, Lu and Pestourie, Rapha\"{e}l and Yao, Wenjie and Wang, Zhicheng and Verdugo, Francesc and Johnson, Steven G.},
title = {Physics-Informed Neural Networks with Hard Constraints for Inverse Design},
journal = {SIAM Journal on Scientific Computing},
volume = {43},
number = {6},
pages = {B1105-B1132},
year = {2021},
doi = {10.1137/21M1397908},

URL = { 
        https://doi.org/10.1137/21M1397908
}
}

@InProceedings{adap_adjoint_NODE,
  title = 	 {Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural {ODE}},
  author =       {Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Tatikonda, Sekhar and Papademetris, Xenophon and Duncan, James},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11639--11649},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhuang20a/zhuang20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zhuang20a.html},
  abstract = 	 {The empirical performance of neural ordinary differential equations (NODEs) is significantly inferior to discrete-layer models on benchmark tasks (e.g. image classification). We demonstrate an explanation is the inaccuracy of existing gradient estimation methods: the adjoint method has numerical errors in reverse-mode integration; the naive method suffers from a redundantly deep computation graph. We propose the Adaptive Checkpoint Adjoint (ACA) method: ACA applies a trajectory checkpoint strategy which records the forward- mode trajectory as the reverse-mode trajectory to guarantee accuracy; ACA deletes redundant components for shallow computation graphs; and ACA supports adaptive solvers. On image classification tasks, compared with the adjoint and naive method, ACA achieves half the error rate in half the training time; NODE trained with ACA outperforms ResNet in both accuracy and test-retest reliability. On time-series modeling, ACA outperforms competing methods. Furthermore, NODE with ACA can incorporate physical knowledge to achieve better accuracy.}
}
@inproceedings{adjoint_symplectic,
 author = {Matsubara, Takashi and Miyatake, Yuto and Yaguchi, Takaharu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {20772--20784},
 publisher = {Curran Associates, Inc.},
 title = {Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/adf8d7f8c53c8688e63a02bfb3055497-Paper.pdf},
 volume = {34},
 year = {2021}
}
@article{Lagrangian_model_based,
  author       = {Jayesh K. Gupta and
                  Kunal Menda and
                  Zachary Manchester and
                  Mykel J. Kochenderfer},
  title        = {A General Framework for Structured Learning of Mechanical Systems},
  journal      = {CoRR},
  volume       = {abs/1902.08705},
  year         = {2019},
  url          = {http://arxiv.org/abs/1902.08705},
  eprinttype    = {arXiv},
  eprint       = {1902.08705},
  timestamp    = {Tue, 21 May 2019 18:03:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1902-08705.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{efficient_PCA,
title = "An efficient fixed-time increment-based data-driven simulation for general multibody dynamics using deep neural networks",
abstract = "In this study, we propose an efficient fixed-time increment-based numerical scheme for data-driven analysis of general multibody dynamics (MBD) problems combining deep neural network (DNN) modeling and principal component analysis (PCA), avoiding local fluctuation of the time transient response that occurred in other works. Output results of the transient dynamics simulation can be expressed as general displacement, velocity, and acceleration, which can also be represented in a reduced dimension by the PCA. This data set is expressed in a fixed-time increment based format, leading to a large data set that is advantageous for training to construct an efficient DNN meta-model. In addition, the number of samples is also significantly reduced. As a result, the training cost is dramatically reduced compared to the simulation without PCA despite a smaller number of samples being used. To demonstrate the performance of the proposed scheme, we solve three benchmark problems: a double pendulum, damped spherical elastic pendulum, and vibrating transmission. From the results, it was found that, when the proposed scheme is used, the training time can be drastically reduced while maintaining high accuracy.",
keywords = "Deep neural networks (DNN), Machine learning (ML), Multibody dynamics (MBD), Principal component analysis (PCA), Real-time simulation",
author = "Go, {Myeong Seok} and Seongji Han and Lim, {Jae Hyuk} and Kim, {Jin Gyun}",
year = "2023",
doi = "10.1007/s00366-023-01793-z",
language = "English",
journal = "Engineering with Computers",
issn = "0177-0667",
publisher = "Springer London",
}
@article{
NODE_con_HNN,
title={Unifying physical systems{\textquoteright} inductive biases in neural {ODE} using dynamics constraints},
author={Yi Heng Lim and Muhammad Firmansyah Kasim},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=ZOAb497iaY},
note={}
}

@article{HNN_EOM,
  title = {Hamiltonian neural networks for solving equations of motion},
  author = {Mattheakis, Marios and Sondak, David and Dogra, Akshunna S. and Protopapas, Pavlos},
  journal = {Phys. Rev. E},
  volume = {105},
  issue = {6},
  pages = {065305},
  numpages = {11},
  year = {2022},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.105.065305},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.105.065305}
}
@article{HNN,
  title={Hamiltonian neural networks},
  author={Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{DissipativeHNN,
  title={Dissipative {H}amiltonian Neural Networks: Learning Dissipative and Conservative Dynamics Separately},
  author={Andrew Sosanya and Sam Greydanus},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.10085},
  url={https://api.semanticscholar.org/CorpusID:246275946}
}
@inproceedings{
zhong2019dissipativeHN,
title={Dissipative Sym{ODEN}: Encoding {H}amiltonian Dynamics with Dissipation and Control into Deep Learning},
author={Yaofeng Desmond Zhong and Biswadip Dey and Amit Chakraborty},
booktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},
year={2019},
url={https://openreview.net/forum?id=knjWFNx6CN}
}
@inproceedings{xiong2020nonseparable_HNN,
  title={Nonseparable Symplectic Neural Networks},
  author={Xiong, Shiying and Tong, Yunjin and He, Xingzhe and Yang, Shuqi and Yang, Cheng and Zhu, Bo},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{toth2019hamiltonian_gen,
  title={Hamiltonian Generative Networks},
  author={Toth, Peter and Rezende, Danilo J and Jaegle, Andrew and Racani{\`e}re, S{\'e}bastien and Botev, Aleksandar and Higgins, Irina},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{sanchez2019hamiltonian_graph,
  title={Hamiltonian graph networks with ode integrators},
  author={Sanchez-Gonzalez, Alvaro and Bapst, Victor and Cranmer, Kyle and Battaglia, Peter},
  journal={arXiv preprint arXiv:1909.12790},
  year={2019}
}
@incollection{ssinn2020_symplectic_HNN,
title = {Sparse Symplectically Integrated Neural Networks},
author = {DiPietro, Daniel M. and Xiong, Shiying and Zhu, Bo},
booktitle = {Advances in Neural Information Processing Systems 34},
year = {2020}
}
@article{DAVID_symplectic_HNN,
title = {Symplectic learning for {H}amiltonian neural networks},
journal = {Journal of Computational Physics},
volume = {494},
pages = {112495},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112495},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123005909},
author = {Marco David and Florian Méhats},
keywords = {Hamiltonian neural network, Ordinary differential equation, Hamiltonian system, Geometric numerical integration, Symplectic numerical method},
abstract = {Machine learning methods are widely used in the natural sciences to model and predict physical systems from observation data. Yet, they are often used as poorly understood “black boxes,” disregarding existing mathematical structure and invariants of the problem. Recently, the proposal of Hamiltonian Neural Networks (HNNs) took a first step towards a unified “gray box” approach, using physical insight to improve performance for Hamiltonian systems. In this paper, we explore a significantly improved training method for HNNs, exploiting the symplectic structure of Hamiltonian systems with a different loss function. This frees the loss from an artificial lower bound. We mathematically guarantee the existence of an exact Hamiltonian function which the HNN can learn. This allows us to prove and numerically analyze the errors made by HNNs which, in turn, renders them fully explainable. Finally, we present a novel post-training correction to obtain the true Hamiltonian only from discretized observation data, up to an arbitrary order.}
}
@article{finzi2020simplifying_HNNLNN_CON,
  title={Simplifying {H}amiltonian and {L}agrangian neural networks via explicit constraints},
  author={Finzi, Marc and Wang, Ke Alexander and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={13880--13889},
  year={2020}
}
@inproceedings{
NSF_HNN,
title={Neural Symplectic Form: Learning {H}amiltonian Equations on General Coordinate Systems},
author={Yuhan Chen and Takashi Matsubara and Takaharu Yaguchi},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=4h4oqp-ATxb}
}

@misc{HNN_IET,
      title={Learning Dynamical Systems from Noisy Data with Inverse-Explicit Integrators}, 
      author={Håkon Noren and Sølve Eidnes and Elena Celledoni},
      year={2023},
      eprint={2306.03548},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{LNN,
  author       = {Miles D. Cranmer and
                  Sam Greydanus and
                  Stephan Hoyer and
                  Peter W. Battaglia and
                  David N. Spergel and
                  Shirley Ho},
  title        = {Lagrangian Neural Networks},
  journal      = {CoRR},
  volume       = {abs/2003.04630},
  year         = {2020},
  url          = {https://arxiv.org/abs/2003.04630},
  eprinttype    = {arXiv},
  eprint       = {2003.04630},
  timestamp    = {Sat, 23 Jan 2021 01:12:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2003-04630.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
Constant_NODE,
title={Constants of motion network},
author={Muhammad Firmansyah Kasim and Yi Heng Lim},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=Lpla1jmJkW}
}

@article{Orignal_PINODE,
title = {Modeling System Dynamics with Physics-Informed Neural Networks Based on {L}agrangian Mechanics⁎⁎This work was sponsored by the German Federal Ministry of Education and Research (ID: 01 IS 18049 A).},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {9195-9200},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2182},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320328354},
author = {Manuel A. Roehrl and Thomas A. Runkler and Veronika Brandtstetter and Michel Tokic and Stefan Obermayer},
keywords = {neural network models, computer simulation, differential equations, semi-parametric identification, system identification},
abstract = {Identifying accurate dynamic models is required for the simulation and control of various technical systems. In many important real-world applications, however, the two main modeling approaches often fail to meet requirements: first principles methods suffer from high bias, whereas data-driven modeling tends to have high variance. Additionally, purely data-based models often require large amounts of data and are often difficult to interpret. In this paper, we present physics-informed neural ordinary differential equations (PINODE), a hybrid model that combines the two modeling techniques to overcome the aforementioned problems. This new approach directly incorporates the equations of motion originating from the Lagrange mechanics into a deep neural network structure. Thus, we can integrate prior physics knowledge where it is available and use function approximation—e.g., neural networks—where it is not. The method is tested with a forward model of a real-world physical system with large uncertainties. The resulting model is accurate and data-efficient while ensuring physical plausibility. With this, we demonstrate a method that beneficially merges physical insight with real data. Our findings are of interest for model-based control and system identification of mechanical systems.}
}
@article{review_MBD12023,
  title={Multibody dynamics and control using machine learning},
  author={Hashemi, Arash and Orzechowski, Grzegorz and Mikkola, Aki and McPhee, John},
  journal={Multibody System Dynamics},
  pages={1--35},
  year={2023},
  publisher={Springer}
}

@article{DL_minimalcorrdinates,
title = {Deep learning for model order reduction of multibody systems to minimal coordinates},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {373},
pages = {113517},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113517},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520307027},
author = {Andrea Angeli and Wim Desmet and Frank Naets},
keywords = {Multibody dynamics, Minimal coordinates, Deep learning, Model order reduction},
abstract = {Among the proposed formulations for rigid multibody dynamics, the minimal coordinates approach permits to parametrize the system motion with the minimal amount of degrees of freedom without the need of additional constraints equations. This leads to a system of ordinary differential equations to describe the motion which enables a straightforward combination of the model with control or estimation algorithms. However, an explicit relation between the model full coordinates and a minimal number of parameters is not always available or easily obtainable, especially for spatial closed-loop mechanisms. In this work, we therefore propose to deploy deep learning to find an approximation of such motion mappings. More specifically, an autoencoder neural network architecture is exploited for the nonlinear dimensionality reduction from full to minimal coordinates. A novel neural-network training scheme is introduced, which exploits the multibody model dynamics information to optimize the decoder-function derivatives so that they represent the tangent space and the curvature of the minimal coordinates manifold. This scheme leads to an effective description of the motion manifold which can be used to express the dynamics in minimal coordinates. The approach is validated on two reference rigid body mechanisms.}
}
@inproceedings{nwankpa2021activation,
  title={Activation functions: comparison of trends in practice and research for deep learning},
  author={Nwankpa, Chigozie Enyinna and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
  booktitle={2nd International Conference on Computational Sciences and Technology},
  pages={124--133},
  year={2021}
}
@inproceedings{CRNN_DYSC,
  title={Dynamic systems simulation and control using consecutive recurrent neural networks},
  author={Chandar, Srikanth and Sunder, Harsha},
  booktitle={Modeling, Machine Learning and Astronomy: First International Conference, MMLA 2019, Bangalore, India, November 22--23, 2019, Revised Selected Papers 1},
  pages={92--103},
  year={2020},
  organization={Springer}
}

@article{ActivationFC,
  title={Activation Functions: Comparison of trends in Practice and Research for Deep Learning},
  author={Chigozie Nwankpa and Winifred L. Ijomah and Anthony Gachagan and Stephen Marshall},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.03378},
  url={https://api.semanticscholar.org/CorpusID:53208763}
}

@article{RNN,
title = {Recurrent Neural Networks for Time Series Forecasting: Current status and future directions},
journal = {International Journal of Forecasting},
volume = {37},
number = {1},
pages = {388-427},
year = {2021},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169207020300996},
author = {Hansika Hewamalage and Christoph Bergmeir and Kasun Bandara},
keywords = {Big data, Forecasting, Best practices, Framework},
abstract = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.}
}
@inproceedings{invariant_NN_CNN,
  title={Incorporating Symmetry into Deep Dynamics Models for Improved Generalization.},
  author={Wang, R},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@conference{PINN_cheats,
title = "How PINNs cheat: Predicting chaotic motion of a double pendulum",
abstract = "Despite extensive research, physics-informed neural networks (PINNs) are still difficult to train, especially when the optimization relies heavily on the physics loss term. Convergence problems frequently occur when simulating dynamical systems with high-frequency components, chaotic or turbulent behavior. In this work, we discuss whether the traditional PINN framework is able to predict chaotic motion by conducting experiments on the undamped double pendulum. Our results demonstrate that PINNs do not exhibit any sensitivity to perturbations in the initial condition. Instead, the PINN optimization consistently converges to physically correct solutions that violate the initial condition only marginally, but diverge significantly from the desired solution due to the chaotic nature of the system. In fact, the PINN predictions primarily exhibit low-frequency components with a smaller magnitude of higher-order derivatives, which favors lower physics loss values compared to the desired solution. We thus hypothesize that the PINNs {"}cheat{"} by shifting the initial conditions to values that correspond to physically correct solutions that are easier to learn. Initial experiments suggest that domain decomposition combined with an appropriate loss weighting scheme mitigates this effect and allows convergence to the desired solution.",
keywords = "neural network, Partial differential equations, optimization",
author = "Sophie Steger and Rohrhofer, {Franz Martin} and Bernhard Geiger",
year = "2022",
language = "English",
note = "The Symbiosis of Deep Learning and Differential Equations II @ the 36th Neural Information Processing Systems (NeurIPS) Conference ; Conference date: 09-12-2022",
}

@book{MBD,
  title={Dynamics of multibody systems},
  author={Shabana, Ahmed A},
  year={2020},
  publisher={Cambridge university press}
}

@article{newmark1959method,
  title={A method of computation for structural dynamics},
  author={Newmark, Nathan M},
  journal={Journal of the engineering mechanics division},
  volume={85},
  number={3},
  pages={67--94},
  year={1959},
  publisher={American Society of Civil Engineers}
}

@article{SONODE_optimizer,
  title={Second-order neural ode optimizer},
  author={Liu, Guan-Horng and Chen, Tianrong and Theodorou, Evangelos},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25267--25279},
  year={2021}
}

@article{PNODE,
  title={Parameterized neural ordinary differential equations: Applications to computational physics problems},
  author={Lee, Kookjin and Parish, Eric J},
  journal={Proceedings of the Royal Society A},
  volume={477},
  number={2253},
  pages={20210162},
  year={2021},
  publisher={The Royal Society}
}

@book{fortin2000augmented,
  title={Augmented {L}agrangian methods: applications to the numerical solution of boundary-value problems},
  author={Fortin, Michel and Glowinski, Roland},
  year={2000},
  publisher={Elsevier}
}

@book{PDE_con,
  title={Numerical PDE-constrained optimization},
  author={De los Reyes, Juan Carlos},
  year={2015},
  publisher={Springer}
}

@article{beucler2021enforcing,
  title={Enforcing analytic constraints in neural networks emulating physical systems},
  author={Beucler, Tom and Pritchard, Michael and Rasp, Stephan and Ott, Jordan and Baldi, Pierre and Gentine, Pierre},
  journal={Physical Review Letters},
  volume={126},
  number={9},
  pages={098302},
  year={2021},
  publisher={APS}
}

@article{PhysRevLett.126.098302,
  title = {Enforcing Analytic Constraints in Neural Networks Emulating Physical Systems},
  author = {Beucler, Tom and Pritchard, Michael and Rasp, Stephan and Ott, Jordan and Baldi, Pierre and Gentine, Pierre},
  journal = {Phys. Rev. Lett.},
  volume = {126},
  issue = {9},
  pages = {098302},
  numpages = {7},
  year = {2021},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.126.098302},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.126.098302}
}
@inproceedings{kaiming_ini,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@InProceedings{xavier_ini,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}
@article{Kingma2014AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980},
  url={https://api.semanticscholar.org/CorpusID:6628106}
}
@article{Zhao_23_optical_design,
author = {Yun Zhao and Hang Chen and Min Lin and Haiou Zhang and Tao Yan and Ruqi Huang and Xing Lin and Qionghai Dai},
journal = {Opt. Lett.},
keywords = {Machine learning; Neural networks; Optical computing; Optical imaging; Optical neural systems; Optical signals},
number = {3},
pages = {628--631},
publisher = {Optica Publishing Group},
title = {Optical neural ordinary differential equations},
volume = {48},
month = {Feb},
year = {2023},
url = {https://opg.optica.org/ol/abstract.cfm?URI=ol-48-3-628},
doi = {10.1364/OL.477713},
abstract = {Increasing the layer number of on-chip photonic neural networks (PNNs) is essential to improve its model performance. However, the successive cascading of network hidden layers results in larger integrated photonic chip areas. To address this issue, we propose the optical neural ordinary differential equations (ON-ODEs) architecture that parameterizes the continuous dynamics of hidden layers with optical ODE solvers. The ON-ODE comprises the PNNs followed by the photonic integrator and optical feedback loop, which can be configured to represent residual neural networks (ResNets) and implement the function of recurrent neural networks with effectively reduced chip area occupancy. For the interference-based optoelectronic nonlinear hidden layer, the numerical experiments demonstrate that the single hidden layer ON-ODE can achieve approximately the same accuracy as the two-layer optical ResNets in image classification tasks. In addition, the ON-ODE improves the model classification accuracy for the diffraction-based all-optical linear hidden layer. The time-dependent dynamics property of ON-ODE is further applied for trajectory prediction with high accuracy.},
}
@inproceedings{video_node,
  title={Vid-ode: Continuous-time video generation with neural ordinary differential equation},
  author={Park, Sunghyun and Kim, Kangyeol and Lee, Junsoo and Choo, Jaegul and Lee, Joonseok and Kim, Sookyung and Choi, Edward},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={3},
  pages={2412--2422},
  year={2021}
}
@article{chem_node2,
  title={Physics-Enhanced Neural Ordinary Differential Equations: Application to Industrial Chemical Reaction Systems},
  author={Sorourifar, Farshud and Peng, You and Castillo, Ivan and Bui, Linh and Venegas, Juan and Paulson, Joel A},
  journal={Industrial \& Engineering Chemistry Research},
  volume={62},
  number={38},
  pages={15563--15577},
  year={2023},
  publisher={ACS Publications}
}
@inproceedings{lim2022unifying,
  title={Unifying physical systems’ inductive biases in neural ODE using dynamics constraints},
  author={Lim, Yi Heng and Kasim, Muhammad Firmansyah},
  booktitle={ICML 2022 2nd AI for Science Workshop},
  year={2022}
}
@article{daems2022keycld,
  title={KeyCLD: Learning Constrained {L}agrangian Dynamics in Keypoint Coordinates from Images},
  author={Daems, Rembert and Taets, Jeroen and Crevecoeur, Guillaume and others},
  journal={arXiv preprint arXiv:2206.11030},
  year={2022}
}

@InProceedings{Jiang_2021_CVPR,
    author    = {Jiang, Boyan and Zhang, Yinda and Wei, Xingkui and Xue, Xiangyang and Fu, Yanwei},
    title     = {Learning Compositional Representation for 4D Captures With Neural ODE},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {5340-5350}
}
@inproceedings{NEURIPS2021_pharm,
 author = {Qian, Zhaozhi and Zame, William and Fleuren, Lucas and Elbers, Paul and van der Schaar, Mihaela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {11364--11383},
 publisher = {Curran Associates, Inc.},
 title = {Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5ea1649a31336092c05438df996a3e59-Paper.pdf},
 volume = {34},
 year = {2021}
}
@inproceedings{lutter2019deeplnn,
  title={Deep {L}agrangian Networks: Using Physics as Model Prior for Deep Learning},
  author={Lutter, M and Ritter, C and Peters, Jan},
  booktitle={International Conference on Learning Representations (ICLR 2019)},
  year={2019},
  organization={OpenReview. net}
}
@article{bacsa2023symplectic_nature,
  title={Symplectic encoders for physics-constrained variational dynamics inference},
  author={Bacsa, Kiran and Lai, Zhilu and Liu, Wei and Todd, Michael and Chatzi, Eleni},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={2643},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{bhattoo2023lnn_graph,
  title={Learning the dynamics of particle-based systems with {L}agrangian graph neural networks},
  author={Bhattoo, Ravinder and Ranu, Sayan and Krishnan, NM Anoop},
  journal={Machine Learning: Science and Technology},
  volume={4},
  number={1},
  pages={015003},
  year={2023},
  publisher={IOP Publishing}
}

@inproceedings{lnn_hnn_contact,
 author = {Zhong, Yaofeng Desmond and Dey, Biswadip and Chakraborty, Amit},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {21910--21922},
 publisher = {Curran Associates, Inc.},
 title = {Extending {L}agrangian and {H}amiltonian Neural Networks with Differentiable Contact Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/b7a8486459730bea9569414ef76cf03f-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{pan2021vehicle_mbd,
  title={Data-driven vehicle modeling of longitudinal dynamics based on a multibody model and deep neural networks},
  author={Pan, Yongjun and Nie, Xiaobo and Li, Zhixiong and Gu, Shuitao},
  journal={Measurement},
  volume={180},
  pages={109541},
  year={2021},
  publisher={Elsevier}
}
@article{TANG2023_MBD_PPINN,
title = {Application of a parallel physics-informed neural network to solve the multi-body dynamic equations for full-scale train collisions},
journal = {Applied Soft Computing},
volume = {142},
pages = {110328},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110328},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623003460},
author = {Zhao Tang and Shaodi Dong and Xiaosong Yang and Jianjun Zhang},
keywords = {Machine learning, Parallel physics-informed neural network, Train collision dynamics},
abstract = {The prohibitive cost of acquiring data from full-scale train collision experiments limits the applicability of data-driven machine learning methods in train collision simulation. Physics-informed neural networks (PINN) attempt to address this challenge by incorporating physics equations as part of the loss function construction. However, the PINN approach is relatively time-consuming when it comes to solving a large number of physical equations. In this paper, a parallel physics-informed neural network (PPINN) methodology is developed for the solution of multibody dynamics equations to further reduce the computational cost. As well, a PPINN-based framework for engineering applications is proposed, investigating the dynamic responses, absorbed energy, collision forces, and wheel vertical rises of the full-scale train collision. Automatic differentiation and parallelization algorithms are applied to the multi-body dynamic equations including mass, damping, and stiffness matrices, as well as the. The residuals and the initial conditions are included in the loss function. The dynamic responses, absorbed energy, collision forces, and wheel vertical rise of the full-scale train collision are simulated and studied in detail. The results obtained from the PPINN method are in excellent agreement with those obtained from the finite element method (FEM), Newmark-β, and fourth-order Runge–Kutta methods for all four train collision scenarios. Additionally, the PPINN methodology keeps better stability even under large time steps which implies a big potential for computational cost reduction.}
}

@ARTICLE{2019RNN_railway,
       author = {{Kraft}, S{\"o}nke and {Causse}, Julien and {Martinez}, Aur{\'e}lie},
        title = "{Black-box modelling of nonlinear railway vehicle dynamics for track geometry assessment using neural networks}",
      journal = {Vehicle System Dynamics, International Journal of Vehicle Mechanics and Mobility},
     keywords = {Railway vehicle dynamics, black-box modelling, neural networks, model validation},
         year = 2019,
        month = sep,
       volume = {57},
       number = {9},
        pages = {1241-1270},
          doi = {10.1080/00423114.2018.1497186},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019VSD....57.1241K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{bauchau2008review,
	author = {Bauchau, Olivier A. and Laulusa, André},
	title = "{Review of Contemporary Approaches for Constraint Enforcement in Multibody Systems}",
	journal = {Journal of Computational and Nonlinear Dynamics},
	volume = {3},
	number = {1},
	pages = {011005},
	year = {2007},
	month = {11},
	issn = {1555-1415},
	doi = {10.1115/1.2803258},
	url = {https://doi.org/10.1115/1.2803258},
	eprint = {https://asmedigitalcollection.asme.org/computationalnonlinear/article-pdf/3/1/011005/5776226/011005\_1.pdf},
}

@article{hashemi2023multibody_review,
  title={Multibody dynamics and control using machine learning},
  author={Hashemi, Arash and Orzechowski, Grzegorz and Mikkola, Aki and McPhee, John},
  journal={Multibody System Dynamics},
  pages={1--35},
  year={2023},
  publisher={Springer}
}
@article{ding2022pino,
  title={PINO-MBD: Physics-Informed Neural Operator for Solving Coupled ODEs in Multi-Body Dynamics},
  author={Ding, Wenhao and He, Qing and Tong, Hanghang and Wang, Ping},
  journal={arXiv preprint arXiv:2205.12262},
  year={2022}
}

@article{hochreiter1997lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@article{kasim2022constants,
  title={Constants of motion network},
  author={Kasim, Muhammad Firmansyah and Lim, Yi Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25295--25305},
  year={2022}
}
@article{portwood2019turbulence,
  title={Turbulence forecasting via neural ode},
  author={Portwood, Gavin D and Mitra, Peetak P and Ribeiro, Mateus Dias and Nguyen, Tan Minh and Nadiga, Balasubramanya T and Saenz, Juan A and Chertkov, Michael and Garg, Animesh and Anandkumar, Anima and Dengel, Andreas and others},
  journal={arXiv preprint arXiv:1911.05180},
  year={2019}
}

@article{chen2022forecasting,
  title={Forecasting the outcome of spintronic experiments with neural ordinary differential equations},
  author={Chen, Xing and Araujo, Flavio Abreu and Riou, Mathieu and Torrejon, Jacob and Ravelosona, Dafin{\'e} and Kang, Wang and Zhao, Weisheng and Grollier, Julie and Querlioz, Damien},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={1016},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{quaglino2019snode,
  title={SNODE: Spectral Discretization of Neural ODEs for System Identification},
  author={Quaglino, Alessio and Gallieri, Marco and Masci, Jonathan and Koutn{\'\i}k, Jan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{Quantum_node,
  title = {Learning quantum dynamics with latent neural ordinary differential equations},
  author = {Choi, Matthew and Flam-Shepherd, Daniel and Kyaw, Thi Ha and Aspuru-Guzik, Al\'an},
  journal = {Phys. Rev. A},
  volume = {105},
  issue = {4},
  pages = {042403},
  numpages = {10},
  year = {2022},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.105.042403},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.105.042403}
}
@article{chaotic_rnn,
  title={New results for prediction of chaotic systems using deep recurrent neural networks},
  author={Serrano-P{\'e}rez, Jos{\'e} de Jes{\'u}s and Fern{\'a}ndez-Anaya, Guillermo and Carrillo-Moreno, Salvador and Yu, Wen},
  journal={Neural Processing Letters},
  volume={53},
  pages={1579--1596},
  year={2021},
  publisher={Springer}
}
@article{chaotic_rnn2,
title = {Forecasting of noisy chaotic systems with deep neural networks},
journal = {Chaos, Solitons \& Fractals},
volume = {153},
pages = {111570},
year = {2021},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2021.111570},
url = {https://www.sciencedirect.com/science/article/pii/S0960077921009243},
author = {Matteo Sangiorgio and Fabio Dercole and Giorgio Guariso},
keywords = {Recurrent neural networks, LSTM cell, Teacher forcing, Multi-step prediction, Deterministic chaos, Non-stationary processes},
abstract = {Recurrent neural networks have recently proved the state-of-the-art approach in forecasting complex oscillatory time series on a multi-step horizon. Researchers in the field investigated different machine learning techniques and training approaches on dynamical systems with different degrees of complexity. Still, these analyses are usually limited to noise-free chaotic time series. This paper extends the analysis from a deterministic to a noisy environment, by considering both observation and structural noise. Observation noise is evaluated by adding different levels of artificially-generated random values on deterministic processes obtained from the simulation of four archetypal chaotic systems. A case of structural noise is implemented through a time-varying version of the logistic map, which exhibits a slow structural change of the system’s dynamic that makes the system non-stationary. Finally, a time series of ozone concentration in Northern Italy is considered to test the theoretical findings on a real-world case study in which both forms of noise play a significant role. Recurrent neural networks formed by LSTM cells are compared with two benchmark feed-forward architectures. LSTM trained without the standard teacher forcing approach, i.e., with training that replicates the setting used in inference mode, proved to have the best performance in compensating the stochasticity generated by the observation noise and reproducing the structural non-stationarity of the process.}
}
@article{chaotic_rnn3,
  title = {Model scale versus domain knowledge in statistical forecasting of chaotic systems},
  author = {Gilpin, William},
  journal = {Phys. Rev. Res.},
  volume = {5},
  issue = {4},
  pages = {043252},
  numpages = {18},
  year = {2023},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.5.043252},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.5.043252}
}
@article{FCNN_iid,
title = {Generalization in fully-connected neural networks for time series forecasting},
journal = {Journal of Computational Science},
volume = {36},
pages = {101020},
year = {2019},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2019.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877750319301838},
author = {Anastasia Borovykh and Cornelis W. Oosterlee and Sander M. Bohté},
keywords = {Neural networks, Deep learning, Generalization, Time series, Forecasting},
abstract = {In this paper we study the generalization capabilities of fully-connected neural networks trained in the context of time series forecasting. Time series do not satisfy the typical assumption in statistical learning theory of the data being i.i.d. samples from some data-generating distribution. We use the input and weight Hessians, that is the smoothness of the learned function with respect to the input and the width of the minimum in weight space, to quantify a network's ability to generalize to unseen data. While such generalization metrics have been studied extensively in the i.i.d. setting of for example image recognition, here we empirically validate their use in the task of time series forecasting. Furthermore we discuss how one can control the generalization capability of the network by means of the training process using the learning rate, batch size and the number of training iterations as controls. Using these hyperparameters one can efficiently control the complexity of the output function without imposing explicit constraints.}
}
@article{hNODE,
   title={Neural modal ordinary differential equations: Integrating physics-based modeling with neural ordinary differential equations for modeling high-dimensional monitored structures},
   volume={3},
   ISSN={2632-6736},
   url={http://dx.doi.org/10.1017/dce.2022.35},
   DOI={10.1017/dce.2022.35},
   journal={Data-Centric Engineering},
   publisher={Cambridge University Press (CUP)},
   author={Lai, Zhilu and Liu, Wei and Jian, Xudong and Bacsa, Kiran and Sun, Limin and Chatzi, Eleni},
   year={2022} }

@article{node_general1,
  title={Enhancing the inductive biases of graph neural ode for modeling dynamical systems},
  author={Bishnoi, Suresh and Bhattoo, Ravinder and Ranu, Sayan and Krishnan, NM},
  journal={arXiv preprint arXiv:2209.10740},
  year={2022}
}

@InProceedings{node_general2,
  title = 	 {Approximation Capabilities of Neural {ODE}s and Invertible Residual Networks},
  author =       {Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11086--11095},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhang20h/zhang20h.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zhang20h.html},
  abstract = 	 {Recent interest in invertible models and normalizing flows has resulted in new architectures that ensure invertibility of the network model. Neural ODEs and i-ResNets are two recent techniques for constructing models that are invertible, but it is unclear if they can be used to approximate any continuous invertible mapping. Here, we show that out of the box, both of these architectures are limited in their approximation capabilities. We then show how to overcome this limitation: we prove that any homeomorphism on a $p$-dimensional Euclidean space can be approximated by a Neural ODE or an i-ResNet operating on a $2p$-dimensional Euclidean space. We conclude by showing that capping a Neural ODE or an i-ResNet with a single linear layer is sufficient to turn the model into a universal approximator for non-invertible continuous functions.}
}

@article{node_general3,
  title={Deconstructing the inductive biases of {H}amiltonian neural networks},
  author={Gruver, Nate and Finzi, Marc and Stanton, Samuel and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2202.04836},
  year={2022}
}


@InProceedings{node_convergence,
  title = 	 {How to Train Your Neural {ODE}: the World of {J}acobian and Kinetic Regularization},
  author =       {Finlay, Chris and Jacobsen, Joern-Henrik and Nurbekyan, Levon and Oberman, Adam},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3154--3164},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/finlay20a/finlay20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/finlay20a.html},
  abstract = 	 {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.}
}

@inproceedings{node_robust,
  title={On the verification of neural odes with stochastic guarantees},
  author={Grunbacher, Sophie and Hasani, Ramin and Lechner, Mathias and Cyranka, Jacek and Smolka, Scott A and Grosu, Radu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={13},
  pages={11525--11535},
  year={2021}
}
@article{kouvaritakis2016MPC,
  title={Model predictive control},
  author={Kouvaritakis, Basil and Cannon, Mark},
  journal={Switzerland: Springer International Publishing},
  volume={38},
  pages={13--56},
  year={2016},
  publisher={Springer}
}

@misc{OPENAI2020Scaling,
	title={Scaling Laws for Neural Language Models}, 
	author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
	year={2020},
	eprint={2001.08361},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@inproceedings{NIPS2017Transformer,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Attention is All you Need},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	volume = {30},
	year = {2017}
}
@inproceedings{NIPS2020_FewShotLearner,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1877--1901},
	publisher = {Curran Associates, Inc.},
	title = {Language Models are Few-Shot Learners},
	url = {https://proceedings.neurips.cc/paper\_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	volume = {33},
	year = {2020}
}
@article{NIPS2022HumanFeedback,
	title={Training language models to follow instructions with human feedback},
	author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
	journal={Advances in neural information processing systems},
	volume={35},
	pages={27730--27744},
	year={2022}
}

@inproceedings{NIPS2023LessIsbetter,
	author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and YU, LILI and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
	pages = {55006--55021},
	publisher = {Curran Associates, Inc.},
	title = {LIMA: Less Is More for Alignment},
	volume = {36},
	year = {2023}
}
@misc{OPENAI2024gpt4,
	title={GPT-4 Technical Report}, 
	author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
	year={2024},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{Google2023Finetune,
	title	= {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
	author	= {Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
	year	= {2023},
	URL	= {https://proceedings.mlr.press/v202/longpre23a/longpre23a.pdf},
	booktitle	= {Proceedings of the 40th International Conference on Machine Learning},
	pages	= {22631--22648}}
@article{meta2022Opt,
	title={Opt: Open pre-trained transformer language models},
	author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
	journal={arXiv preprint arXiv:2205.01068},
	year={2022}
}
@misc{meta2023llama,
	title={LLaMA: Open and Efficient Foundation Language Models}, 
	author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year={2023},
	eprint={2302.13971},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{
	zhang2023adalora,
	title={Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning },
	author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=lq62uWRJjiY}
}

@article{lin2024pegpt,
	title={PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design},
	author={Lin, Fanfan and Liu, Junhua and Li, Xinze and Zhao, Shuai and Zhao, Bohui and Ma, Hao and Zhang, Xin},
	journal={arXiv preprint arXiv:2403.14059},
	year={2024}
}

@misc{acikgoz2024healthcarellm,
	title={Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare}, 
	author={Emre Can Acikgoz and Osman Batur İnce and Rayene Bench and Arda Anıl Boz and İlker Kesen and Aykut Erdem and Erkut Erdem},
	year={2024},
	eprint={2404.16621},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@article{KIM2024geogpt,
	title = {A ChatGPT-MATLAB framework for numerical modeling in geotechnical engineering applications},
	journal = {Computers and Geotechnics},
	volume = {169},
	pages = {106237},
	year = {2024},
	issn = {0266-352X},
	doi = {https://doi.org/10.1016/j.compgeo.2024.106237},
	url = {https://www.sciencedirect.com/science/article/pii/S0266352X24001733},
	author = {Daehyun Kim and Taegu Kim and Yejin Kim and Yong-Hoon Byun and Tae Sup Yun},
	keywords = {ChatGPT, Numerical modeling, Automated Programming, Artificial Intelligence (AI), Large Language Model (LLM)},
	abstract = {ChatGPT has recently emerged as a representative of Large Language Models (LLMs) that have brought evolutionary changes to our society, and the effectiveness of ChatGPT in various applications has been increasingly reported. This study aimed to explore the potential of employing programming performance driven by ChatGPT responses to conversational prompts in the field of geotechnical engineering. The tested examples included the analysis of seepage flow and slope stability, and the image processing of X-ray computed tomographic image for partially saturated sand. For each case, the prompt was initially fed by a narrative explanation of the problem attributes such as geometry, initial conditions, and boundary conditions to generate the MATLAB code that was in turn executed to evaluate the correctness and functionality. Any errors and unanticipated results were further refined by additional prompts until the correct outcome was achieved. ChatGPT was able to generate the numerical code at a considerable level, demonstrating creditable awareness of the refining process, when meticulous prompts were provided based on a comprehensive understanding of given problems. While ChatGPT may not be able to replace the entire process of programming, it can help minimize sloppy syntax errors and assist in designing a basic framework for logical programming.}
}
@article{zheng2024llamafactory,
	title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
	author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Yongqiang Ma},
	journal={arXiv preprint arXiv:2403.13372},
	year={2024},
	url={http://arxiv.org/abs/2403.13372}
}
@article{gu2023anomalyagpt,
	title={AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models},
	author={Gu, Zhaopeng and Zhu, Bingke and Zhu, Guibo and Chen, Yingying and Tang, Ming and Wang, Jinqiao},
	journal={arXiv preprint arXiv:2308.15366},
	year={2023}
}

@article{Aluga2023civilgpt, 
	title={Application of CHATGPT in civil engineering}, 
	volume={6}, 
	url={https://journals.eanso.org/index.php/eaje/article/view/1272}, 
	DOI={10.37284/eaje.6.1.1272}, 
	number={1}, 
	journal={East African Journal of Engineering}, 
	author={Aluga, Martin}, 
	year={2023}, 
	month={Jun.}, 
	pages={104-112} 
}

@article{king2023medicinegpt,
	title={The future of AI in medicine: a perspective from a Chatbot},
	author={King, Michael R},
	journal={Annals of Biomedical Engineering},
	volume={51},
	number={2},
	pages={291--295},
	year={2023},
	publisher={Springer}
}

@misc{Claudemodelcard,
	title={Model Card and Evaluations for Claude Models},
	author={Anthropic},
	year={2023},
	url = {https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf}
}

@article{ahn2022google_robot,
	title={Do as i can, not as i say: Grounding language in robotic affordances},
	author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and others},
	journal={arXiv preprint arXiv:2204.01691},
	year={2022}
}

@inproceedings{
	sun2023adaplanner,
	title={AdaPlanner: Adaptive Planning from Feedback with Language Models},
	author={Haotian Sun and Yuchen Zhuang and Lingkai Kong and Bo Dai and Chao Zhang},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023},
	url={https://openreview.net/forum?id=rnKgbKmelt}
}

@inproceedings{kwon2022rewardllm1,
	title={Reward Design with Language Models},
	author={Kwon, Minae and Xie, Sang Michael and Bullard, Kalesha and Sadigh, Dorsa},
	booktitle={The Eleventh International Conference on Learning Representations},
	year={2022}
}
@article{song2023rewardllm2,
	title={Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics},
	author={Song, Jiayang and Zhou, Zhehua and Liu, Jiawei and Fang, Chunrong and Shu, Zhan and Ma, Lei},
	journal={arXiv preprint arXiv:2309.06687},
	year={2023}
}

@inproceedings{ma2023rewardllm3,
	title={Eureka: Human-Level Reward Design via Coding Large Language Models},
	author={Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2023}
}
@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{team2023gemini,
	title={Gemini: a family of highly capable multimodal models},
	author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
	journal={arXiv preprint arXiv:2312.11805},
	year={2023}
}
@INPROCEEDINGS{Singh2023ICRA_llmplan1,
	author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
	booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={ProgPrompt: Generating Situated Robot Task Plans using Large Language Models}, 
	year={2023},
	volume={},
	number={},
	pages={11523-11530},
	doi={10.1109/ICRA48891.2023.10161317}}

@article{chen2023ICRA_llmplan2,
	title={Autotamp: Autoregressive task and motion planning with llms as translators and checkers},
	author={Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
	journal={arXiv preprint arXiv:2306.06531},
	year={2023}
}
@article{chen2023ICRA_llmplan3,
	title={Scalable multi-robot collaboration with large language models: Centralized or decentralized systems?},
	author={Chen, Yongchao and Arkin, Jacob and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
	journal={arXiv preprint arXiv:2309.15943},
	year={2023}
}
@article{hu2023llmplan4,
	title={Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning},
	author={Hu, Yingdong and Lin, Fanqi and Zhang, Tong and Yi, Li and Gao, Yang},
	journal={arXiv preprint arXiv:2311.17842},
	year={2023}
}

@misc{schneider2024mpirigen,
	title={MPIrigen: MPI Code Generation through Domain-Specific Language Models}, 
	author={Nadav Schneider and Niranjan Hasabnis and Vy A. Vo and Tal Kadosh and Neva Krien and Mihai Capotă and Guy Tamir and Ted Willke and Nesreen Ahmed and Yuval Pinter and Timothy Mattson and Gal Oren},
	year={2024},
	eprint={2402.09126},
	archivePrefix={arXiv},
	primaryClass={cs.DC}
}

@article{chen2024ompgpt,
	title={OMPGPT: A Generative Pre-trained Transformer Model for OpenMP},
	author={Chen, Le and Bhattacharjee, Arijit and Ahmed, Nesreen and Hasabnis, Niranjan and Oren, Gal and Vo, Vy and Jannesari, Ali},
	journal={arXiv preprint arXiv:2401.16445},
	year={2024}
}
@article{kadosh2023domain,
	title={Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks},
	author={Kadosh, Tal and Hasabnis, Niranjan and Vo, Vy A and Schneider, Nadav and Krien, Neva and Capota, Mihai and Wasay, Abdul and Ahmed, Nesreen and Willke, Ted and Tamir, Guy and others},
	journal={arXiv preprint arXiv:2312.13322},
	year={2023}
}

@misc{zhao2024loraland,
	title={LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report}, 
	author={Justin Zhao and Timothy Wang and Wael Abid and Geoffrey Angus and Arnav Garg and Jeffery Kinnison and Alex Sherstinsky and Piero Molino and Travis Addair and Devvret Rishi},
	year={2024},
	eprint={2405.00732},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{rafailov2024dpo,
	title={Direct preference optimization: Your language model is secretly a reward model},
	author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{chen2023maybe0005,
	title={Maybe only 0.5\% data is needed: A preliminary exploration of low training data instruction tuning},
	author={Chen, Hao and Zhang, Yiming and Zhang, Qi and Yang, Hantao and Hu, Xiaomeng and Ma, Xuetao and Yanggong, Yifan and Zhao, Junbo},
	journal={arXiv preprint arXiv:2305.09246},
	year={2023}
}
@article{he2021towards,
	title={Towards a unified view of parameter-efficient transfer learning},
	author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
	journal={arXiv preprint arXiv:2110.04366},
	year={2021}
}

@article{chen2021evaluating1,
	title={Evaluating large language models trained on code},
	author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
	journal={arXiv preprint arXiv:2107.03374},
	year={2021}
}
@inproceedings{xu2022evaluating2,
	title={A systematic evaluation of large language models of code},
	author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
	booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
	pages={1--10},
	year={2022}
}
@article{liu2024evaluating3,
	title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
	author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{liu2024dora,
	title={DoRA: Weight-Decomposed Low-Rank Adaptation},
	author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
	journal={arXiv preprint arXiv:2402.09353},
	year={2024}
}
@misc{zhao2024galore,
	title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, 
	author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
	year={2024},
	eprint={2403.03507},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
@article{google2021scrathpad,
	title={Show your work: Scratchpads for intermediate computation with language models},
	author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
	journal={arXiv preprint arXiv:2112.00114},
	year={2021}
}

@article{kaplan2020scalinglaw1,
	title={Scaling laws for neural language models},
	author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	journal={arXiv preprint arXiv:2001.08361},
	year={2020}
}
@article{hoffmann2022scalinglaw2,
	title={Training compute-optimal large language models},
	author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
	journal={arXiv preprint arXiv:2203.15556},
	year={2022}
}

@article{wei2022emergent1,
	title={Emergent abilities of large language models},
	author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
	journal={arXiv preprint arXiv:2206.07682},
	year={2022}
}
@article{schaeffer2024emergent2,
	title={Are emergent abilities of large language models a mirage?},
	author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@misc{lialin2023relora,
	title={Stack More Layers Differently: High-Rank Training Through Low-Rank Updates},
	author={Vladislav Lialin and Namrata Shivagunde and Sherin Muckatira and Anna Rumshisky},
	year={2023},
	eprint={2307.05695},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{google2023data1,
	title={The flan collection: Designing data and methods for effective instruction tuning},
	author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
	booktitle={International Conference on Machine Learning},
	pages={22631--22648},
	year={2023},
	organization={PMLR}
}

@article{puri2022dataaug,
	title={How many data samples is an additional instruction worth?},
	author={Puri, Ravsehaj Singh and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
	journal={arXiv preprint arXiv:2203.09161},
	year={2022}
}

@article{gupta2023continualwarm,
	title={Continual Pre-Training of Large Language Models: How to (re) warm your model?},
	author={Gupta, Kshitij and Th{\'e}rien, Benjamin and Ibrahim, Adam and Richter, Mats L and Anthony, Quentin and Belilovsky, Eugene and Rish, Irina and Lesort, Timoth{\'e}e},
	journal={arXiv preprint arXiv:2308.04014},
	year={2023}
}
@article{li2023structuredCOT,
	title={Structured chain-of-thought prompting for code generation},
	author={Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
	journal={arXiv preprint arXiv:2305.06599},
	year={2023}
}
@article{chung2024dataaug2,
	title={Scaling instruction-finetuned language models},
	author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
	journal={Journal of Machine Learning Research},
	volume={25},
	number={70},
	pages={1--53},
	year={2024}
}
@article{cobbe2021scalinglawfinetune,
	title={Training verifiers to solve math word problems},
	author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
	journal={arXiv preprint arXiv:2110.14168},
	year={2021}
}

@misc{abdin2024phi3,
	title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
	author={Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Qin Cai and Martin Cai and Caio César Teodoro Mendes and Weizhu Chen and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Yen-Chun Chen and Yi-Ling Chen and Parul Chopra and Xiyang Dai and Allie Del Giorno and Gustavo de Rosa and Matthew Dixon and Ronen Eldan and Victor Fragoso and Dan Iter and Mei Gao and Min Gao and Jianfeng Gao and Amit Garg and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Jamie Huynh and Mojan Javaheripi and Xin Jin and Piero Kauffmann and Nikos Karampatziakis and Dongwoo Kim and Mahoud Khademi and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Ce Liu and Mengchen Liu and Weishung Liu and Eric Lin and Zeqi Lin and Chong Luo and Piyush Madan and Matt Mazzola and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Xin Wang and Lijuan Wang and Chunyu Wang and Yu Wang and Rachel Ward and Guanhua Wang and Philipp Witte and Haiping Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Sonali Yadav and Fan Yang and Jianwei Yang and Ziyi Yang and Yifan Yang and Donghan Yu and Lu Yuan and Chengruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
	year={2024},
	eprint={2404.14219},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{papineni2002bleu1,
	title={Bleu: a method for automatic evaluation of machine translation},
	author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
	pages={311--318},
	year={2002}
}
@article{EVTIKHIEV2023bleu2,
	title = {Out of the BLEU: How should we assess quality of the Code Generation models?},
	journal = {Journal of Systems and Software},
	volume = {203},
	pages = {111741},
	year = {2023},
	issn = {0164-1212},
	doi = {https://doi.org/10.1016/j.jss.2023.111741},
	url = {https://www.sciencedirect.com/science/article/pii/S016412122300136X},
	author = {Mikhail Evtikhiev and Egor Bogomolov and Yaroslav Sokolov and Timofey Bryksin},
	keywords = {Code generation, Metrics, Neural networks, Code similarity},
	abstract = {In recent years, researchers have created and introduced a significant number of various code generation models. As human evaluation of every new model version is unfeasible, the community adopted automatic evaluation metrics such as BLEU to approximate the results of human judgement. These metrics originate from the machine translation domain and it is unclear whether they are applicable for the code generation tasks and how well they agree with the human evaluation on this task. There are also other metrics, CodeBLEU and RUBY, developed to estimate the similarity of code, that take into account the properties of source code. However, for these metrics there are hardly any studies on their agreement with the human evaluation. Despite all that, minimal differences in the metric scores have been used in recent papers to claim superiority of some code generation models over the others. In this paper, we present a study on the applicability of six metrics—BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, and RUBY—for evaluation of code generation models. We conduct a study on two different code generation datasets and use human annotators to assess the quality of all models run on these datasets. The results indicate that for the CoNaLa dataset of Python one-liners, none of the metrics can correctly emulate human judgement on which model is better with >95% certainty if the difference in model scores is less than 5 points. For the HearthStone dataset, which consists of classes of a particular structure, a difference in model scores of at least 2 points is enough to claim the superiority of one model over the other. Our findings suggest that the ChrF metric is a better fit for the evaluation of code generation models than the commonly used BLEU and CodeBLEU. Yet, finding a metric for code generation that closely agrees with humans requires additional work.}
}
@article{ganesan2018rouge,
	title={Rouge 2.0: Updated and improved measures for evaluation of summarization tasks},
	author={Ganesan, Kavita},
	journal={arXiv preprint arXiv:1803.01937},
	year={2018}
}
@article{chang2024LLMevaluation,
	title={A survey on evaluation of large language models},
	author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
	journal={ACM Transactions on Intelligent Systems and Technology},
	volume={15},
	number={3},
	pages={1--45},
	year={2024},
	publisher={ACM New York, NY}
}
@article{liu2024LLMcodeevaluation,
	title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
	author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{roziere2023codellama,
	title={Code llama: Open foundation models for code},
	author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
	journal={arXiv preprint arXiv:2308.12950},
	year={2023}
}
@article{google2024gemma,
	title={Gemma: Open models based on gemini research and technology},
	author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
	journal={arXiv preprint arXiv:2403.08295},
	year={2024}
}

@misc{codegemma_2024,
	title={CodeGemma: Open Code Models Based on Gemma},
	url={https://goo.gle/codegemma},
	author={Hartman, Ale Jakse and Hu, Andrea and Choquette-Choo, Christopher and Zhao, Heri and Fine, Jane and Hui,
	Jeffrey and Shen, Jingyue and Kelley, Joe and Howland, Joshua and Bansal, Kshitij and Vilnis, Luke and Wirth, Mateo and Nguyen, Nam and Michel, Paul and Choy, Peter and Joshi, Pratik and Kumar, Ravin and Hashmi, Sarmad and Agrawal, Shubham and Zuo, Siqi and Warkentin, Tris and Gong, Zhitao et al.},
	year={2024}
}
@article{bi2023oceangpt,
	title={Oceangpt: A large language model for ocean science tasks},
	author={Bi, Zhen and Zhang, Ningyu and Xue, Yida and Ou, Yixin and Ji, Daxiong and Zheng, Guozhou and Chen, Huajun},
	journal={arXiv preprint arXiv:2310.02031},
	year={2023}
}
@article{liu2022finetunevsicl,
	title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
	author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={1950--1965},
	year={2022}
}
@article{mosbach2023finetunevsicl2,
	title={Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation},
	author={Mosbach, Marius and Pimentel, Tiago and Ravfogel, Shauli and Klakow, Dietrich and Elazar, Yanai},
	journal={arXiv preprint arXiv:2305.16938},
	year={2023}
}
@article{bertGoogle2018,
	author    = {Jacob Devlin and
	Ming{-}Wei Chang and
	Kenton Lee and
	Kristina Toutanova},
	title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
	journal   = {CoRR},
	volume    = {abs/1810.04805},
	year      = {2018},
	url       = {http://arxiv.org/abs/1810.04805},
	archivePrefix = {arXiv},
	eprint    = {1810.04805},
	timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{kadosh2023domainspecificcode,
	title={Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks}, 
	author={Tal Kadosh and Niranjan Hasabnis and Vy A. Vo and Nadav Schneider and Neva Krien and Mihai Capota and Abdul Wasay and Nesreen Ahmed and Ted Willke and Guy Tamir and Yuval Pinter and Timothy Mattson and Gal Oren},
	year={2023},
	eprint={2312.13322},
	archivePrefix={arXiv},
	primaryClass={cs.PL}
}
@article{li2023starcoder_LLM,
	title={Starcoder: may the source be with you!},
	author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
	journal={arXiv preprint arXiv:2305.06161},
	year={2023}
}

@misc{ren2024cfpeft,
	title={Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning}, 
	author={Weijieying Ren and Xinlong Li and Lei Wang and Tianxiang Zhao and Wei Qin},
	year={2024},
	eprint={2402.18865},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{peng2023instructionGPT4,
	title={Instruction tuning with gpt-4},
	author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	journal={arXiv preprint arXiv:2304.03277},
	year={2023}
}
@article{dettmers2024qlora,
	title={Qlora: Efficient finetuning of quantized llms},
	author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{hendrycks2020mmlu,
	title={Measuring massive multitask language understanding},
	author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2009.03300},
	year={2020}
}

@article{liu2024visualinstructiontuning,
	title={Visual instruction tuning},
	author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	journal={Advances in neural information processing systems},
	volume={36},
	year={2024}
}
@article{luccioni2023estimatingcarbon,
	title={Estimating the carbon footprint of bloom, a 176b parameter language model},
	author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
	journal={Journal of Machine Learning Research},
	volume={24},
	number={253},
	pages={1--15},
	year={2023}
}
@article{cossu2022continualpretrain1,
	title={Continual pre-training mitigates forgetting in language and vision},
	author={Cossu, Andrea and Tuytelaars, Tinne and Carta, Antonio and Passaro, Lucia and Lomonaco, Vincenzo and Bacciu, Davide},
	journal={arXiv preprint arXiv:2205.09357},
	year={2022}
}
@article{jin2021continualpretrain,
	title={Lifelong pretraining: Continually adapting language models to emerging corpora},
	author={Jin, Xisen and Zhang, Dejiao and Zhu, Henghui and Xiao, Wei and Li, Shang-Wen and Wei, Xiaokai and Arnold, Andrew and Ren, Xiang},
	journal={arXiv preprint arXiv:2110.08534},
	year={2021}
}
@article{rawte2023surveyhallucinations,
	title={A survey of hallucination in large foundation models},
	author={Rawte, Vipula and Sheth, Amit and Das, Amitava},
	journal={arXiv preprint arXiv:2309.05922},
	year={2023}
}
@inproceedings{tasora2016chrono,
	title={Chrono: An open source multi-physics dynamics engine},
	author={Tasora, Alessandro and Serban, Radu and Mazhar, Hammad and Pazouki, Arman and Melanz, Daniel and Fleischmann, Jonathan and Taylor, Michael and Sugiyama, Hiroyuki and Negrut, Dan},
	booktitle={High Performance Computing in Science and Engineering: Second International Conference, HPCSE 2015, Sol{\'a}{\v{n}}, Czech Republic, May 25-28, 2015, Revised Selected Papers 2},
	pages={19--49},
	year={2016},
	organization={Springer}
}
@article{li2021prefix,
	title={Prefix-tuning: Optimizing continuous prompts for generation},
	author={Li, Xiang Lisa and Liang, Percy},
	journal={arXiv preprint arXiv:2101.00190},
	year={2021}
}
@article{liu2021ptun,
	title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
	author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
	journal={arXiv preprint arXiv:2110.07602},
	year={2021}
}
@article{gu2021ppt,
	title={Ppt: Pre-trained prompt tuning for few-shot learning},
	author={Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
	journal={arXiv preprint arXiv:2109.04332},
	year={2021}
}

@article{taylor2023clinical,
	title={Clinical prompt learning with frozen language models},
	author={Taylor, Niall and Zhang, Yi and Joyce, Dan W and Gao, Ziming and Kormilitzin, Andrey and Nevado-Holgado, Alejo},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	year={2023},
	publisher={IEEE}
}
@article{wang2020fslearning,
	title={Generalizing from a few examples: A survey on few-shot learning},
	author={Wang, Yaqing and Yao, Quanming and Kwok, James T and Ni, Lionel M},
	journal={ACM computing surveys (csur)},
	volume={53},
	number={3},
	pages={1--34},
	year={2020},
	publisher={ACM New York, NY, USA}
}
@inproceedings{hu2023adapter,
	title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
	author={Hu, Zhiqiang and Wang, Lei and Lan, Yihuai and Xu, Wanyu and Lim, Ee-Peng and Bing, Lidong and Xu, Xing and Poria, Soujanya and Lee, Roy Ka-Wei},
	booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
	year={2023}
}
@article{bai2022rlhf,
	title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
	author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
	journal={arXiv preprint arXiv:2204.05862},
	year={2022}
}
@article{meng2024simpo,
	title={SimPO: Simple Preference Optimization with a Reference-Free Reward},
	author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
	journal={arXiv preprint arXiv:2405.14734},
	year={2024}
}
@article{du2024understandemergent,
	title={Understanding emergent abilities of language models from the loss perspective},
	author={Du, Zhengxiao and Zeng, Aohan and Dong, Yuxiao and Tang, Jie},
	journal={arXiv preprint arXiv:2403.15796},
	year={2024}
}
@article{jiang2024mixtral,
	title={Mixtral of experts},
	author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
	journal={arXiv preprint arXiv:2401.04088},
	year={2024}
}
@article{hinton2015distilling,
	title={Distilling the knowledge in a neural network},
	author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	journal={arXiv preprint arXiv:1503.02531},
	year={2015}
}
@article{lopez2017gradient,
	title={Gradient episodic memory for continual learning},
	author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{kirkpatrick2017overcoming,
	title={Overcoming catastrophic forgetting in neural networks},
	author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
	journal={Proceedings of the national academy of sciences},
	volume={114},
	number={13},
	pages={3521--3526},
	year={2017},
	publisher={National Acad Sciences}
}
@article{french1999catastrophic,
	title={Catastrophic forgetting in connectionist networks},
	author={French, Robert M},
	journal={Trends in cognitive sciences},
	volume={3},
	number={4},
	pages={128--135},
	year={1999},
	publisher={Elsevier}
}
@inproceedings{wu2022pretrained,
	title={Pretrained language model in continual learning: A comparative study},
	author={Wu, Tongtong and Caccia, Massimo and Li, Zhuang and Li, Yuan Fang and Qi, Guilin and Haffari, Gholamreza},
	booktitle={International Conference on Learning Representations 2022},
	year={2022},
	organization={OpenReview}
}
@article{hayou2024lora+,
	title={LoRA+: Efficient Low Rank Adaptation of Large Models},
	author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
	journal={arXiv preprint arXiv:2402.12354},
	year={2024}
}
@article{lialin2023peftreview,
	title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
	author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
	journal={arXiv preprint arXiv:2303.15647},
	year={2023}
}

@article{lester2021softprompt,
	title={The power of scale for parameter-efficient prompt tuning},
	author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	journal={arXiv preprint arXiv:2104.08691},
	year={2021}
}

@inproceedings{zhang2023adapter2,
	title={LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention},
	author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Zhou, Aojun and Lu, Pan and Qiao, Yu and Li, Hongsheng and Gao, Peng},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2023}
}
@inproceedings{zaken2022bitfit,
	title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
	author={Zaken, Elad Ben and Goldberg, Yoav and Ravfogel, Shauli},
	booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	pages={1--9},
	year={2022}
}
@article{sung2021sparseselect,
	title={Training neural networks with fixed sparse masks},
	author={Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	pages={24193--24205},
	year={2021}
}
@inproceedings{sievert2014lda,
	title={LDAvis: A method for visualizing and interpreting topics},
	author={Sievert, Carson and Shirley, Kenneth},
	booktitle={Proceedings of the workshop on interactive language learning, visualization, and interfaces},
	pages={63--70},
	year={2014}
}
@misc{yao2024LLMunlearning,
	title={Large Language Model Unlearning}, 
	author={Yuanshun Yao and Xiaojun Xu and Yang Liu},
	year={2024},
	eprint={2310.10683},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2310.10683}, 
}
@misc{jiang2024mixtralexperts,
	title={Mixtral of Experts}, 
	author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
	year={2024},
	eprint={2401.04088},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2401.04088}, 
}
@misc{deepseekai2024deepseekcoderv2breakingbarrierclosedsource,
	title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence}, 
	author={DeepSeek-AI and Qihao Zhu and Daya Guo and Zhihong Shao and Dejian Yang and Peiyi Wang and Runxin Xu and Y. Wu and Yukun Li and Huazuo Gao and Shirong Ma and Wangding Zeng and Xiao Bi and Zihui Gu and Hanwei Xu and Damai Dai and Kai Dong and Liyue Zhang and Yishi Piao and Zhibin Gou and Zhenda Xie and Zhewen Hao and Bingxuan Wang and Junxiao Song and Deli Chen and Xin Xie and Kang Guan and Yuxiang You and Aixin Liu and Qiushi Du and Wenjun Gao and Xuan Lu and Qinyu Chen and Yaohui Wang and Chengqi Deng and Jiashi Li and Chenggang Zhao and Chong Ruan and Fuli Luo and Wenfeng Liang},
	year={2024},
	eprint={2406.11931},
	archivePrefix={arXiv},
	primaryClass={cs.SE},
	url={https://arxiv.org/abs/2406.11931}, 
}
@misc{stiennon2022ppo,
	title={Learning to summarize from human feedback}, 
	author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
	year={2022},
	eprint={2009.01325},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2009.01325}, 
}
@inproceedings{macal2005tutorial,
	title={Tutorial on agent-based modeling and simulation},
	author={Macal, Charles M and North, Michael J},
	booktitle={Proceedings of the Winter Simulation Conference, 2005.},
	pages={14--pp},
	year={2005},
	organization={IEEE}
}
@misc{weng2023agent,
  title   = {LLM-powered Autonomous Agents},
  author  = {Weng, Lilian},
  year    = {2023},
  month   = {Jun},
  url     = {https://lilianweng.github.io/posts/2023-06-23-agent/},
  note    = {Accessed: 2024-06-23}
}
@misc{autoGPT,
	author       = {\relax {AutoGPT Team}},
	title        = {Auto{GPT}: the heart of the open-source agent ecosystem},
	howpublished = {\url{https://github.com/Significant-Gravitas/AutoGPT}},
}
@article{Wang_agent2024,
	title={A survey on large language model based autonomous agents},
	volume={18},
	ISSN={2095-2236},
	url={http://dx.doi.org/10.1007/s11704-024-40231-1},
	DOI={10.1007/s11704-024-40231-1},
	number={6},
	journal={Frontiers of Computer Science},
	publisher={Springer Science and Business Media LLC},
	author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	year={2024},
	month=mar }
@misc{schick2023toolformerlanguagemodelsteach,
	title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
	author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
	year={2023},
	eprint={2302.04761},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2302.04761}, 
}
@misc{park2023generativeagentsinteractivesimulacra,
	title={Generative Agents: Interactive Simulacra of Human Behavior}, 
	author={Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
	year={2023},
	eprint={2304.03442},
	archivePrefix={arXiv},
	primaryClass={cs.HC},
	url={https://arxiv.org/abs/2304.03442}, 
}
@inproceedings{park2022social,
	title={Social simulacra: Creating populated prototypes for social computing systems},
	author={Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
	booktitle={Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
	pages={1--18},
	year={2022}
}
@article{li2023you,
	title={Are you in a masquerade? exploring the behavior and impact of large language model driven social bots in online social networks},
	author={Li, Siyu and Yang, Jin and Zhao, Kui},
	journal={arXiv preprint arXiv:2307.10337},
	year={2023}
}
@article{m2024augmenting,
	title={Augmenting large language models with chemistry tools},
	author={M. Bran, Andres and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D and Schwaller, Philippe},
	journal={Nature Machine Intelligence},
	pages={1--11},
	year={2024},
	publisher={Nature Publishing Group UK London}
}
@article{jin2023surrealdriver,
	title={Surrealdriver: Designing generative driver agent simulation framework in urban contexts based on large language model},
	author={Jin, Ye and Shen, Xiaoxi and Peng, Huiling and Liu, Xiaoan and Qin, Jingli and Li, Jiayang and Xie, Jintao and Gao, Peizhong and Zhou, Guyue and Gong, Jiangtao},
	journal={arXiv preprint arXiv:2309.13193},
	year={2023}
}
@misc{gur2024realworldwebagentplanninglong,
	title={A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis}, 
	author={Izzeddin Gur and Hiroki Furuta and Austin Huang and Mustafa Safdari and Yutaka Matsuo and Douglas Eck and Aleksandra Faust},
	year={2024},
	eprint={2307.12856},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2307.12856}, 
}
@misc{zhang2024generativeagentsrecommendation,
	title={On Generative Agents in Recommendation}, 
	author={An Zhang and Yuxin Chen and Leheng Sheng and Xiang Wang and Tat-Seng Chua},
	year={2024},
	eprint={2310.10108},
	archivePrefix={arXiv},
	primaryClass={cs.IR},
	url={https://arxiv.org/abs/2310.10108}, 
}
@article{williams2023epidemic,
	title={Epidemic modeling with generative agents},
	author={Williams, Ross and Hosseinichimeh, Niyousha and Majumdar, Aritra and Ghaffarzadegan, Navid},
	journal={arXiv preprint arXiv:2307.04986},
	year={2023}
}
@article{li2023large,
	title={Large language model-empowered agents for simulating macroeconomic activities},
	author={Li, Nian and Gao, Chen and Li, Yong and Liao, Qingmin},
	journal={arXiv preprint arXiv:2310.10436},
	year={2023}
}
@article{qian2023communicative,
	title={Communicative agents for software development},
	author={Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong},
	journal={arXiv preprint arXiv:2307.07924},
	year={2023}
}

@article{yao2024tree,
	title={Tree of thoughts: Deliberate problem solving with large language models},
	author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{joublin2023copal,
	title={CoPAL: corrective planning of robot actions with large language models},
	author={Joublin, Frank and Ceravola, Antonello and Smirnov, Pavel and Ocker, Felix and Deigmoeller, Joerg and Belardinelli, Anna and Wang, Chao and Hasler, Stephan and Tanneberg, Daniel and Gienger, Michael},
	journal={arXiv preprint arXiv:2310.07263},
	year={2023}
}
@article{gong2023multilevel,
	title={Multilevel large language models for everyone},
	author={Gong, Yuanhao},
	journal={arXiv preprint arXiv:2307.13221},
	year={2023}
}
@misc{huang2024agentcodermultiagentbasedcodegeneration,
      title={AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation}, 
      author={Dong Huang and Jie M. Zhang and Michael Luck and Qingwen Bu and Yuhao Qing and Heming Cui},
      year={2024},
      eprint={2312.13010},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.13010}, 
}
@misc{han2024llmmultiagentsystemschallenges,
      title={LLM Multi-Agent Systems: Challenges and Open Problems}, 
      author={Shanshan Han and Qifan Zhang and Yuhang Yao and Weizhao Jin and Zhaozhuo Xu and Chaoyang He},
      year={2024},
      eprint={2402.03578},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2402.03578}, 
}
@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}
@misc{hu2024minicpmunveilingpotentialsmall,
      title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}, 
      author={Shengding Hu and Yuge Tu and Xu Han and Chaoqun He and Ganqu Cui and Xiang Long and Zhi Zheng and Yewei Fang and Yuxiang Huang and Weilin Zhao and Xinrong Zhang and Zheng Leng Thai and Kaihuo Zhang and Chongyi Wang and Yuan Yao and Chenyang Zhao and Jie Zhou and Jie Cai and Zhongwu Zhai and Ning Ding and Chao Jia and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2404.06395},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06395}, 
}

@inproceedings{rao2020rl-cyclegan,
	title = {RL-CycleGAN: Reinforcement Learning Aware Simulation-to-Real},
	booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Rao, Kanishka and Harris, Chris and Irpan, Alex and Levine, Sergey and Ibarz, Julian and Khansari, Mohi},
	year = {2020},
  address = {Seattle, USA},
  doi = {10.1109/CVPR42600.2020.01117},
}

@article{dong2023shipgan,
	title = {ShipGAN: Generative Adversarial Network Based Simulation-to-Real Image Translation for Ships},
	volume = {131},
	doi = {10.1016/j.apor.2022.103456},
	journal = {Applied Ocean Research},
	author = {Dong, Yuxuan and Wu, Peng and Wang, Sen and Liu, Yuanchang},
	year = {2023},
	pages = {103456},
}

@inproceedings{ani2021quantifying,
	title = {Quantifying the Use of Domain Randomization},
	doi = {10.1109/ICPR48806.2021.9412118},
	booktitle = {International Conference on Pattern Recognition (ICPR)},
	author = {Ani, Mohammad and Basevi, Hector and Leonardis, Aleš},
	year = {2021},
  address = {Milan, Italy},
	pages = {6128--6135},
}

@inproceedings{guizilini2021geometric,
	title = {Geometric Unsupervised Domain Adaptation for Semantic Segmentation},
	booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
	author = {Guizilini, Vitor and Li, Jie and Ambruş, Rareş and Gaidon, Adrien},
	year = {2021},
  address = {Montreal, QC, Canada},
	pages = {8517--8527},
  doi = {10.1109/ICCV48922.2021.00842},
}

@inproceedings{shyam2022infra,
	title = {Infra Sim-to-Real: An Efficient Baseline and Dataset for Infrastructure Based Online Object Detection and Tracking using Domain Adaptation},
	booktitle = {IEE Intelligent Vehicles Symposium (IV)},
	author = {Shyam, Pranjay and Mishra, Sumit and Yoon, Kuk-Jin and Kim, Kyung-Soo},
	year = {2022},
	address = {Aachen, Germany},
	pages = {1393--1399},
  doi = {10.1109/IV51971.2022.9827395},
}

@inproceedings{zakharov2022photo-realistic,
	address = {Tel Aviv, Israel},
	title = {Photo-realistic Neural Domain Randomization},
	booktitle = {European Conference on Computer Vision (ECCV)},
	author = {Zakharov, Sergey and Ambruș, Rareș and Guizilini, Vitor and Kehl, Wadim and Gaidon, Adrien},
	year = {2022},
	pages = {310--327},
  doi = {10.1007/978-3-031-19806-9},
}

@inproceedings{bousmalis2017unsupervised,
	title = {Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
	month = jul,
	year = {2017},
}

@inproceedings{chen2017no,
	address = {Venice, Italy},
	title = {No More Discrimination: Cross City Adaptation of Road Scene Segmenters},
	doi = {10.1109/ICCV.2017.220},
	booktitle = {IEEE International Conference on Computer Vision (ICCV)},
	author = {Chen, Yi-Hsin and Chen, Wei-Yu and Chen, Yu-Ting and Tsai, Bo-Cheng and Wang, Yu-Chiang Frank and Sun, Min},
	year = {2017},
	pages = {2011--2020},
}

@inproceedings{hinterstoisser2019annotation,
	address = {Seoul, South Korea},
	title = {An Annotation Saved is an Annotation Earned: Using Fully Synthetic Training for Object Detection},
	doi = {10.1109/ICCVW.2019.00340},
	booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV) Workshop},
	author = {Hinterstoisser, Stefan and Pauly, Olivier and Heibel, Hauke and Martina, Marek and Bokeloh, Martin},
	month = oct,
	year = {2019},
	pages = {2787--2796},
}

@inproceedings{weinmann2014material,
	address = {Zurich, Switzerland},
	title = {Material Classification Based on Training Data Synthesized Using a BTF Database},
	doi = {10.1007/978-3-319-10578-9_11},
	booktitle = {European Conference on Computer Vision (ECCV)},
	publisher = {Springer International Publishing},
	author = {Weinmann, Michael and Gall, Juergen and Klein, Reinhard},
	month = sep,
	year = {2014},
	pages = {156--171},
}

@inproceedings{lee2021deep,
	address = {Montreal, QC, Canada},
	title = {Deep Hough Voting for Robust Global Registration},
	doi = {10.1109/ICCV48922.2021.01569},
	booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
	author = {Lee, Junha and Kim, Seungwook and Cho, Minsu and Park, Jaesik},
	month = oct,
	year = {2021},
	pages = {15974--15983},
}

@article{wagstaff2018deep,
	title = {Deep Mars: CNN Classification of Mars Imagery for the PDS Imaging Atlas},
	volume = {32},
	doi = {10.1609/aaai.v32i1.11404},
	number = {1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wagstaff, Kiri and Lu, You and Stanboli, Alice and Grimes, Kevin and Gowda, Thamme and Padams, Jordan},
	year = {2018},
}



@misc{sqlite3mysql,
    title={sqlite3mysql},
    url={https://github.com/techouse/sqlite3-to-mysql},
    author={Tusar, Klemen},
    year={2018}}

@misc{clement2020pymt5,
      title={PyMT5: multi-mode translation of natural language and Python code with transformers}, 
      author={Colin B. Clement and Dawn Drain and Jonathan Timcheck and Alexey Svyatkovskiy and Neel Sundaresan},
      year={2020},
      eprint={2010.03150},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{husain2020codesearchnet,
      title={CodeSearchNet Challenge: Evaluating the State of Semantic Code Search}, 
      author={Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
      year={2020},
      eprint={1909.09436},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2021codet5,
      title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, 
      author={Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
      year={2021},
      eprint={2109.00859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2020codebert,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      eprint={2002.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{puri2021codenet,
      title={CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks}, 
      author={Ruchir Puri and David S. Kung and Geert Janssen and Wei Zhang and Giacomo Domeniconi and Vladimir Zolotov and Julian Dolby and Jie Chen and Mihir Choudhury and Lindsey Decker and Veronika Thost and Luca Buratti and Saurabh Pujar and Shyam Ramji and Ulrich Finkler and Susan Malaika and Frederick Reiss},
      year={2021},
      eprint={2105.12655},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{lu2021codexglue,
  author = {Shuai Lu and
    Daya Guo and
    Shuo Ren and
    Junjie Huang and
    Alexey Svyatkovskiy and
    Ambrosio Blanco and
    Colin B. Clement and
    Dawn Drain and
    Daxin Jiang and
    Duyu Tang and
    Ge Li and
    Lidong Zhou and
    Linjun Shou and
    Long Zhou and
    Michele Tufano and
    Ming Gong and
    Ming Zhou and
    Nan Duan and
    Neel Sundaresan and
    Shao Kun Deng and
    Shengyu Fu and
    Shujie Liu},
  title = {CodeXGLUE: {A} Machine Learning Benchmark Dataset for Code Understanding
    and Generation},
  journal = {CoRR},
  volume = {abs/2102.04664},
  year = {2021}
}

@misc{dingsoyr2012decade,
  title={A decade of agile methodologies: Towards explaining agile software development},
  author={Dings{\o}yr, Torgeir and Nerur, Sridhar and Balijepally, VenuGopal and Moe, Nils Brede},
  journal={Journal of systems and software},
  volume={85},
  number={6},
  pages={1213--1221},
  year={2012},
  publisher={Elsevier}
}

@article{williams2000strengthening,
  title={Strengthening the case for pair programming},
  author={Williams, Laurie and Kessler, Robert R and Cunningham, Ward and Jeffries, Ron},
  journal={IEEE software},
  volume={17},
  number={4},
  pages={19--25},
  year={2000},
  publisher={IEEE}
}

@INPROCEEDINGS{cowan2003ctf,
  author={Cowan, C. and Arnold, S. and Beattie, S. and Wright, C. and Viega, J.},
  booktitle={Proceedings DARPA Information Survivability Conference and Exposition}, 
  title={Defcon Capture the Flag: defending vulnerable code from intense attack}, 
  year={2003},
  volume={1},
  number={},
  pages={120-129 vol.1},
  doi={10.1109/DISCEX.2003.1194878}}

@misc{havrylov2017emergence,
      title={Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols}, 
      author={Serhii Havrylov and Ivan Titov},
      year={2017},
      eprint={1705.11192},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wong2022deep,
      title={Deep Multiagent Reinforcement Learning: Challenges and Directions}, 
      author={Annie Wong and Thomas Bäck and Anna V. Kononova and Aske Plaat},
      year={2022},
      eprint={2106.15691},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{merkel2014docker,
  title={Docker: lightweight linux containers for consistent development and deployment},
  author={Merkel, Dirk},
  journal={Linux journal},
  volume={2014},
  number={239},
  pages={2},
  year={2014}
}

@misc{zeng2022nbest,
      title={N-Best Hypotheses Reranking for Text-To-SQL Systems}, 
      author={Lu Zeng and Sree Hari Krishnan Parthasarathi and Dilek Hakkani-Tur},
      year={2022},
      eprint={2210.10668},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yin-neubig-2019-reranking,
    title = "Reranking for Neural Semantic Parsing",
    author = "Yin, Pengcheng  and
      Neubig, Graham",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1447",
    doi = "10.18653/v1/P19-1447",
    pages = "4553--4559",
}

@misc{ni2023lever,
      title={LEVER: Learning to Verify Language-to-Code Generation with Execution}, 
      author={Ansong Ni and Srini Iyer and Dragomir Radev and Ves Stoyanov and Wen-tau Yih and Sida I. Wang and Xi Victoria Lin},
      year={2023},
      eprint={2302.08468},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shi2022natural,
      title={Natural Language to Code Translation with Execution}, 
      author={Freda Shi and Daniel Fried and Marjan Ghazvininejad and Luke Zettlemoyer and Sida I. Wang},
      year={2022},
      eprint={2204.11454},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2022coder,
      title={Coder Reviewer Reranking for Code Generation}, 
      author={Tianyi Zhang and Tao Yu and Tatsunori B. Hashimoto and Mike Lewis and Wen-tau Yih and Daniel Fried and Sida I. Wang},
      year={2022},
      eprint={2211.16490},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bunel2018leveraging,
      title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis}, 
      author={Rudy Bunel and Matthew Hausknecht and Jacob Devlin and Rishabh Singh and Pushmeet Kohli},
      year={2018},
      eprint={1805.04276},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Chen2018ExecutionGuidedNP,
  title={Execution-Guided Neural Program Synthesis},
  author={Xinyun Chen and Chang Liu and Dawn Xiaodong Song},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@misc{ellis2019write,
      title={Write, Execute, Assess: Program Synthesis with a REPL}, 
      author={Kevin Ellis and Maxwell Nye and Yewen Pu and Felix Sosa and Josh Tenenbaum and Armando Solar-Lezama},
      year={2019},
      eprint={1906.04604},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{zhang2023planning,
      title={Planning with Large Language Models for Code Generation}, 
      author={Shun Zhang and Zhenfang Chen and Yikang Shen and Mingyu Ding and Joshua B. Tenenbaum and Chuang Gan},
      year={2023},
      eprint={2303.05510},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yao2023tree,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhong2017seq2sql,
      title={Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning}, 
      author={Victor Zhong and Caiming Xiong and Richard Socher},
      year={2017},
      eprint={1709.00103},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dong2016language,
      title={Language to Logical Form with Neural Attention}, 
      author={Li Dong and Mirella Lapata},
      year={2016},
      eprint={1601.01280},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{chen-etal-2021-plotcoder,
    title = "{P}lot{C}oder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context",
    author = "Chen, Xinyun  and
      Gong, Linyuan  and
      Cheung, Alvin  and
      Song, Dawn",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.169",
    doi = "10.18653/v1/2021.acl-long.169",
    pages = "2169--2181",
}

@misc{wang2023executionbased,
      title={Execution-Based Evaluation for Open-Domain Code Generation}, 
      author={Zhiruo Wang and Shuyan Zhou and Daniel Fried and Graham Neubig},
      year={2023},
      eprint={2212.10481},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{zhou2023codebertscore,
      title={CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code}, 
      author={Shuyan Zhou and Uri Alon and Sumit Agarwal and Graham Neubig},
      year={2023},
      eprint={2302.05527},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@misc{ren2020codebleu,
      title={CodeBLEU: a Method for Automatic Evaluation of Code Synthesis}, 
      author={Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
      year={2020},
      eprint={2009.10297},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@inproceedings{Papineni2002BleuAM,
  title={Bleu: a Method for Automatic Evaluation of Machine Translation},
  author={Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2002}
}

@inproceedings{yao2020calm,
    title={Keep CALM and Explore: Language Models for Action Generation in Text-based Games},
    author={Yao, Shunyu and Rao, Rohan and Hausknecht, Matthew and Narasimhan, Karthik},
    booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
    year={2020}
}

@misc{openai_gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}


@article{chen2022codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2207.10397},
  year={2022}
}

@article{le2022coderl,
  title={Coderl: Mastering code generation through pretrained models and deep reinforcement learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21314--21328},
  year={2022}
}


@article{chen2023teaching,
  title={Teaching Large Language Models to Self-Debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@misc{yao2023react,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023planandsolve,
      title={Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models}, 
      author={Lei Wang and Wanyu Xu and Yihuai Lan and Zhiqiang Hu and Yunshi Lan and Roy Ka-Wei Lee and Ee-Peng Lim},
      year={2023},
      eprint={2305.04091},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{alphacode,
    doi = {10.1126/science.abq1158},
    url = {https://doi.org/10.1126\%2Fscience.abq1158},
    year = 2022,
    month = {dec},
    publisher = {American Association for the Advancement of Science ({AAAS})},
    volume = {378},
    number = {6624},
    pages = {1092--1097},
    author = {Yujia Li and David Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser and R{\'{e}
    }mi Leblond and Tom Eccles and James Keeling and Felix Gimeno and Agustin Dal Lago and Thomas Hubert and Peter Choy and Cyprien de Masson d'Autume and Igor Babuschkin and Xinyun Chen and Po-Sen Huang and Johannes Welbl and Sven Gowal and Alexey Cherepanov and James Molloy and Daniel J. Mankowitz and Esme Sutherland Robson and Pushmeet Kohli and Nando de Freitas and Koray Kavukcuoglu and Oriol Vinyals},
    title = {Competition-level code generation with {AlphaCode}},
    journal = {Science}
}

@misc{shinn2023reflexion,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{yu-etal-2018-spider,
    title = "{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Yang, Kai  and
      Yasunaga, Michihiro  and
      Wang, Dongxu  and
      Li, Zifan  and
      Ma, James  and
      Li, Irene  and
      Yao, Qingning  and
      Roman, Shanelle  and
      Zhang, Zilin  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1425",
    doi = "10.18653/v1/D18-1425",
    pages = "3911--3921"
}

@inproceedings{lin-etal-2018-nl2bash,
    title = "{NL}2{B}ash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System",
    author = "Lin, Xi Victoria  and
      Wang, Chenglong  and
      Zettlemoyer, Luke  and
      Ernst, Michael D.",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1491",
}

@article{nijkamp2022codegen,
  title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={ICLR},
  year={2023}
}

@misc{lahiri2022interactive,
      title={Interactive Code Generation via Test-Driven User-Intent Formalization}, 
      author={Shuvendu K. Lahiri and Aaditya Naik and Georgios Sakkas and Piali Choudhury and Curtis von Veh and Madanlal Musuvathi and Jeevana Priya Inala and Chenglong Wang and Jianfeng Gao},
      year={2022},
      eprint={2208.05950},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{Le2022CodeRLMC,
  title={CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},
  author={Hung Le and Yue Wang and Akhilesh Deepak Gotmare and Silvio Savarese and Steven C. H. Hoi},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.01780}
}

@misc{wang2023leti,
      title={LeTI: Learning to Generate from Textual Interactions}, 
      author={Xingyao Wang and Hao Peng and Reyhaneh Jabbarvand and Heng Ji},
      year={2023},
      eprint={2305.10314},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Lai2022DS1000,
  title={DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  author={Yuhang Lai and Chengxi Li and Yiming Wang and Tianyi Zhang and Ruiqi Zhong and Luke Zettlemoyer and Scott Wen-tau Yih and Daniel Fried and Sida Wang and Tao Yu},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.11501}
}

@inproceedings{huang-etal-2022-execution,
    title = "Execution-based Evaluation for Data Science Code Generation Models",
    author = "Huang, Junjie  and
      Wang, Chenglong  and
      Zhang, Jipeng  and
      Yan, Cong  and
      Cui, Haotian  and
      Inala, Jeevana Priya  and
      Clement, Colin  and
      Duan, Nan",
    booktitle = "Proceedings of the Fourth Workshop on Data Science with Human-in-the-Loop (Language Advances)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dash-1.5",
    pages = "28--36"
}

@misc{yin2022natural,
      title={Natural Language to Code Generation in Interactive Data Science Notebooks}, 
      author={Pengcheng Yin and Wen-Ding Li and Kefan Xiao and Abhishek Rao and Yeming Wen and Kensen Shi and Joshua Howland and Paige Bailey and Michele Catasta and Henryk Michalewski and Alex Polozov and Charles Sutton},
      year={2022},
      eprint={2212.09248},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2022compilable,
      title={Compilable Neural Code Generation with Compiler Feedback}, 
      author={Xin Wang and Yasheng Wang and Yao Wan and Fei Mi and Yitong Li and Pingyi Zhou and Jin Liu and Hao Wu and Xin Jiang and Qun Liu},
      year={2022},
      eprint={2203.05132},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023improving,
      title={Improving Code Generation by Training with Natural Language Feedback}, 
      author={Angelica Chen and Jérémy Scheurer and Tomasz Korbak and Jon Ander Campos and Jun Shern Chan and Samuel R. Bowman and Kyunghyun Cho and Ethan Perez},
      year={2023},
      eprint={2303.16749},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{mehta2023improving,
      title={Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback}, 
      author={Nikhil Mehta and Milagro Teruel and Patricio Figueroa Sanz and Xin Deng and Ahmed Hassan Awadallah and Julia Kiseleva},
      year={2023},
      eprint={2304.10750},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lee-etal-2021-kaggledbqa,
    title = "{K}aggle{DBQA}: Realistic Evaluation of Text-to-{SQL} Parsers",
    author = "Lee, Chia-Hsuan  and
      Polozov, Oleksandr  and
      Richardson, Matthew",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.176",
    doi = "10.18653/v1/2021.acl-long.176",
    pages = "2261--2273"
}


@InProceedings{pmlr-v133-agarwal21b,
  title = 	 {NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands},
  author =       {Agarwal, Mayank and Chakraborti, Tathagata and Fu, Quchen and Gros, David and Lin, Xi Victoria and Maene, Jaron and Talamadupula, Kartik and Teng, Zhongwei and White, Jules},
  booktitle = 	 {Proceedings of the NeurIPS 2020 Competition and Demonstration Track},
  pages = 	 {302--324},
  year = 	 {2021},
  editor = 	 {Escalante, Hugo Jair and Hofmann, Katja},
  volume = 	 {133},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--12 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v133/agarwal21b/agarwal21b.pdf},
  url = 	 {https://proceedings.mlr.press/v133/agarwal21b.html}
}

@inproceedings{yu-etal-2019-sparc,
    title = "{SP}ar{C}: Cross-Domain Semantic Parsing in Context",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Yasunaga, Michihiro  and
      Tan, Yi Chern  and
      Lin, Xi Victoria  and
      Li, Suyi  and
      Er, Heyang  and
      Li, Irene  and
      Pang, Bo  and
      Chen, Tao  and
      Ji, Emily  and
      Dixit, Shreya  and
      Proctor, David  and
      Shim, Sungrok  and
      Kraft, Jonathan  and
      Zhang, Vincent  and
      Xiong, Caiming  and
      Socher, Richard  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1443",
    doi = "10.18653/v1/P19-1443",
    pages = "4511--4523"
}

@misc{yu2019cosql,
      title={CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases}, 
      author={Tao Yu and Rui Zhang and He Yang Er and Suyi Li and Eric Xue and Bo Pang and Xi Victoria Lin and Yi Chern Tan and Tianze Shi and Zihan Li and Youxuan Jiang and Michihiro Yasunaga and Sungrok Shim and Tao Chen and Alexander Fabbri and Zifan Li and Luyao Chen and Yuwen Zhang and Shreya Dixit and Vincent Zhang and Caiming Xiong and Richard Socher and Walter S Lasecki and Dragomir Radev},
      year={2019},
      eprint={1909.05378},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hendrycksapps2021,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@misc{wang2023interactive,
      title={Interactive Natural Language Processing}, 
      author={Zekun Wang and Ge Zhang and Kexin Yang and Ning Shi and Wangchunshu Zhou and Shaochun Hao and Guangzheng Xiong and Yizhi Li and Mong Yuan Sim and Xiuying Chen and Qingqing Zhu and Zhenzhu Yang and Adam Nik and Qi Liu and Chenghua Lin and Shi Wang and Ruibo Liu and Wenhu Chen and Ke Xu and Dayiheng Liu and Yike Guo and Jie Fu},
      year={2023},
      eprint={2305.13246},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{li2023starcoder,
      title={StarCoder: may the source be with you!}, 
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and et al.},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Agashe2019JuICeAL,
  title={JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation},
  author={Rajas Agashe and Srini Iyer and Luke Zettlemoyer},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.02216}
}

@misc{anil2023palm,
      title={PaLM 2 Technical Report}, 
      author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and et al.},
      year={2023},
      eprint={2305.10403},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{picoctf,
    title={{picoCTF}},
    url={https://picoctf.org/},
    author={Carnegie Mellon University},
    year={2013}
}

@misc{zhang2023selfedit,
      title={Self-Edit: Fault-Aware Code Editor for Code Generation}, 
      author={Kechi Zhang and Zhuo Li and Jia Li and Ge Li and Zhi Jin},
      year={2023},
      eprint={2305.04087},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{chen2023program,
      title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}, 
      author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
      year={2023},
      eprint={2211.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yao2022webshop,
  title = {WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents},
  author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  booktitle = {ArXiv},
  year = {preprint},
  html = {https://arxiv.org/abs/2207.01206},
  tag = {NLP}
}

@misc{liang2023code,
      title={Code as Policies: Language Model Programs for Embodied Control}, 
      author={Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng},
      year={2023},
      eprint={2209.07753},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{li2023llm,
  title={Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs},
  author={Jinyang Li and Binyuan Hui and Ge Qu and Binhua Li and Jiaxi Yang and Bowen Li and Bailin Wang and Bowen Qin and Rongyu Cao and Ruiying Geng and Nan Huo and Chenhao Ma and Kevin C. C. Chang and Fei Huang and Reynold Cheng and Yongbin Li},
  year={2023},
  eprint={2305.03111},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}